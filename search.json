[
  {
    "objectID": "proyectos.html",
    "href": "proyectos.html",
    "title": "Mis Proyectos",
    "section": "",
    "text": "Aquí encontrarás una selección de los talleres y análisis que he desarrollado. Cada proyecto explora diferentes técnicas y conceptos de econometría y ciencia de datos. Haz clic en cualquiera de ellos para ver el análisis completo.\n\n\nSerie de Talleres: Monitoria de Econometría 2\nEstos talleres fueron creados como material de apoyo para el curso de Econometría 2, cubriendo desde los fundamentos de las series de tiempo hasta modelos multivariados.\n\nTaller 1: Conceptos sobre Series de Tiempo\n\nDescripción: Una introducción a los componentes fundamentales de las series de tiempo, incluyendo tendencia, estacionalidad, ciclo y estacionariedad.\n\nTaller 2: Metodología Box-Jenkins para Modelos ARIMA\n\nDescripción: Una guía práctica completa para la construcción de un modelo ARIMA, aplicada al análisis y pronóstico del PIB de Argentina.\n\nTaller 3: Modelo de Ecuaciones Simultáneas\n\nDescripción: Un análisis teórico de los modelos de ecuaciones simultáneas, el problema de la endogeneidad, la identificación y los métodos de estimación como MC2E.\n\nTaller 4: Pruebas de Cointegración\n\nDescripción: Exploración de las relaciones de largo plazo entre series no estacionarias, aplicando y comparando las pruebas de Engle-Granger y Johansen.\n\nTaller 5: Modelo de Corrección de Errores (MCE)\n\nDescripción: Un taller práctico sobre cómo modelar la dinámica de corto plazo y la velocidad de ajuste hacia el equilibrio de largo plazo en series cointegradas.\n\nTaller 6: Modelos de Vectores Autorregresivos (VAR)\n\nDescripción: Introducción al análisis de sistemas multivariados, utilizando datos simulados para interpretar funciones de impulso-respuesta y causalidad de Granger.\n\nTaller 7: Modelos ARCH y GARCH\n\nDescripción: Se desarrolla de forma pedagogica los temas referentes a modelos para series de tiempo con alta volatilidad.\n\nTaller 8: Modelos de Panel de Datos\n\nDescripción: Trabajamos un taller paractico para demsotar de forma matématica y por medio de códigos en R todo lo conveniente sobre modelos de cortes longitudinal.\n\nTaller 9: Repaso Tercer Parcial\n\nDescripción: Para finalizar en curso se lleva a cabo un taller final que permite recopilar toda vista durante el curso de econometria 2."
  },
  {
    "objectID": "Proyectos/Pruebas de Cointegración.html",
    "href": "Proyectos/Pruebas de Cointegración.html",
    "title": "Taller 4: Pruebas de Cointegración",
    "section": "",
    "text": "En el análisis de series de tiempo, a menudo nos encontramos con variables no estacionarias. Si realizamos una regresión entre dos variables no estacionarias, corremos el riesgo de encontrar una regresión espuria: una relación estadísticamente significativa que en realidad no existe, producto de que ambas variables comparten una tendencia común.\nLa cointegración es una herramienta econométrica que nos permite analizar si existe una relación de equilibrio de largo plazo entre dos o más series temporales que son, individualmente, no estacionarias.\n\nDefinición de Cointegración: “Si \\(y_t\\) y \\(x_t\\) son dos series no estacionarias I(1), por lo general se encontrará que cualquier combinación lineal de ellas también es I(1). Sin embargo, si existe una constante \\(\\beta\\) tal que \\(z_t = y_t - \\beta x_t\\) es una serie estacionaria I(0), se dice que \\(y_t\\) y \\(x_t\\) están cointegradas. La constante \\(\\beta\\) se denomina parámetro de cointegración.”\n— Pindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.).\n\nLa prueba de cointegración generalmente implica los siguientes pasos:\n\nVerificar que todas las series involucradas sean no estacionarias y tengan el mismo orden de integración (normalmente I(1)).\nEstimar la regresión de largo plazo entre las variables.\nAnalizar los residuos de esta regresión para verificar si son estacionarios. Si lo son, las variables están cointegradas."
  },
  {
    "objectID": "Proyectos/Pruebas de Cointegración.html#introducción-a-la-cointegración",
    "href": "Proyectos/Pruebas de Cointegración.html#introducción-a-la-cointegración",
    "title": "Taller 4: Pruebas de Cointegración",
    "section": "",
    "text": "En el análisis de series de tiempo, a menudo nos encontramos con variables no estacionarias. Si realizamos una regresión entre dos variables no estacionarias, corremos el riesgo de encontrar una regresión espuria: una relación estadísticamente significativa que en realidad no existe, producto de que ambas variables comparten una tendencia común.\nLa cointegración es una herramienta econométrica que nos permite analizar si existe una relación de equilibrio de largo plazo entre dos o más series temporales que son, individualmente, no estacionarias.\n\nDefinición de Cointegración: “Si \\(y_t\\) y \\(x_t\\) son dos series no estacionarias I(1), por lo general se encontrará que cualquier combinación lineal de ellas también es I(1). Sin embargo, si existe una constante \\(\\beta\\) tal que \\(z_t = y_t - \\beta x_t\\) es una serie estacionaria I(0), se dice que \\(y_t\\) y \\(x_t\\) están cointegradas. La constante \\(\\beta\\) se denomina parámetro de cointegración.”\n— Pindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.).\n\nLa prueba de cointegración generalmente implica los siguientes pasos:\n\nVerificar que todas las series involucradas sean no estacionarias y tengan el mismo orden de integración (normalmente I(1)).\nEstimar la regresión de largo plazo entre las variables.\nAnalizar los residuos de esta regresión para verificar si son estacionarios. Si lo son, las variables están cointegradas."
  },
  {
    "objectID": "Proyectos/Pruebas de Cointegración.html#ejemplo-1-pib-y-población-de-colombia",
    "href": "Proyectos/Pruebas de Cointegración.html#ejemplo-1-pib-y-población-de-colombia",
    "title": "Taller 4: Pruebas de Cointegración",
    "section": "2 Ejemplo 1: PIB y Población de Colombia",
    "text": "2 Ejemplo 1: PIB y Población de Colombia\nEn este ejemplo, analizaremos la posible relación de cointegración entre el PIB y la población de Colombia para el periodo 1960-2022.\n\n2.1 Carga y Preparación de los Datos\n\n\nCode\n# Crear un tibble (data frame) con los datos directamente en el código\ndatos_col &lt;- tibble(\n  PIB = c(4031152977, 4540447761, 4955538219, 4836166667, 5973366667, 5760761905, 5428518519, 5825170438, 5960212869, 6450175214, 7198360460, 7820380971, 8671358733, 10315760000, 12370029584, 13098633902, 15341403660, 19470960619, 23263511958, 27940411250, 33400735644, 36388366869, 38968039722, 43477319602, 50669166667, 52191500000, 56312100000, 62651600000, 71295200000, 80497500000, 96683800000, 98453400000, 103000000000, 115000000000, 126000000000, 146000000000, 165000000000, 183000000000, 166000000000, 163000000000, 180000000000, 201000000000, 230000000000, 243000000000, 270000000000, 300000000000, 334000000000, 379000000000, 382000000000, 401000000000, 427000000000, 451000000000, 469000000000, 491000000000, 494000000000, 485000000000, 485000000000, 459000000000, 425000000000, 461000000000, 497000000000, 525000000000, 53422800),\n  poblacion = c(15687688, 16182414, 16691282, 17210956, 17739754, 18275814, 18811407, 19343961, 19872503, 20392264, 20905254, 21405413, 21898055, 22396318, 22897871, 23403731, 23913002, 24443926, 25003608, 25579323, 26176195, 26785982, 27402803, 28022425, 28641979, 29261273, 29880295, 30499092, 31118742, 31739343, 32361000, 32983800, 33607900, 34233500, 34860700, 35489700, 36120500, 36753300, 37388200, 38025300, 38664700, 39306500, 39950700, 40597400, 41246700, 41898600, 42553200, 43210500, 43870600, 44533500, 45199300, 45868000, 46539600, 47214200, 47891800, 48572400, 49256100, 49942800, 50632600, 51325500, 52021500, 52720600, 53422800)\n)\n\n# Crear los objetos de serie de tiempo\nPIB_col &lt;- ts(datos_col$PIB, start = c(1960, 1), frequency = 1)\npoblacion_col &lt;- ts(datos_col$poblacion, start = c(1960, 1), frequency = 1)\n\n\n\n\n2.2 1. Verificar la Estacionariedad de las Variables\nPrimero, graficamos las series para observar su comportamiento.\n\n\nCode\n# Crear un data frame combinado para ggplot\nplot_data_col &lt;- data.frame(\n  Año = time(PIB_col),\n  PIB = as.numeric(PIB_col),\n  Poblacion = as.numeric(poblacion_col)\n) %&gt;% pivot_longer(-Año, names_to = \"Variable\", values_to = \"Valor\")\n\n# Graficar ambas series con ggplot2\nggplot(plot_data_col, aes(x = Año, y = Valor, color = Variable)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(~Variable, scales = \"free_y\", ncol = 1) +\n  labs(title = \"PIB y Población de Colombia\", x = \"Año\", y = \"Valor\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nSeries de PIB y Población de Colombia (1960-2022).\n\n\n\n\nAmbas series muestran una clara tendencia ascendente, lo que sugiere que son no estacionarias. Procedemos con la prueba formal de Dickey-Fuller Aumentada (DFA).\n\n\nCode\n# Función para resumir los resultados del test de Dickey-Fuller\nsummarize_ur_df &lt;- function(test_object, variable_name) {\n  tibble(\n    Variable = variable_name,\n    `Estadístico de Prueba` = test_object@teststat[1,1],\n    `Valor Crítico 1%` = test_object@cval[1,1],\n    `Valor Crítico 5%` = test_object@cval[1,2],\n    `Valor Crítico 10%` = test_object@cval[1,3]\n  )\n}\n\n# Realizar y resumir las pruebas\ndf_pib &lt;- ur.df(PIB_col, type = \"trend\", selectlags = \"AIC\")\ndf_pob &lt;- ur.df(poblacion_col, type = \"trend\", selectlags = \"AIC\")\n\nbind_rows(\n  summarize_ur_df(df_pib, \"PIB\"),\n  summarize_ur_df(df_pob, \"Población\")\n) %&gt;% kable(caption = \"Resultados de la Prueba de Dickey-Fuller Aumentada (en niveles)\")\n\n\n\nResultados de la Prueba de Dickey-Fuller Aumentada (en niveles)\n\n\n\n\n\n\n\n\n\nVariable\nEstadístico de Prueba\nValor Crítico 1%\nValor Crítico 5%\nValor Crítico 10%\n\n\n\n\nPIB\n-1.306836\n-4.04\n-3.45\n-3.15\n\n\nPoblación\n-1.403532\n-4.04\n-3.45\n-3.15\n\n\n\n\n\nInterpretación: En ambos casos, el estadístico de prueba es menor en valor absoluto que los valores críticos, por lo que no podemos rechazar la hipótesis nula de que las series tienen una raíz unitaria. Concluimos que ambas son no estacionarias (I(1)).\n\n\n2.3 2. Estimar la Regresión de Largo Plazo\nDado que la teoría económica sugiere que la población puede ser un determinante del PIB, estimamos el siguiente modelo:\n\n\nCode\n# Ajustar el modelo de regresión y mostrar una tabla limpia\nmodelo_col &lt;- lm(PIB_col ~ poblacion_col)\ntidy(modelo_col) %&gt;% kable(caption = \"Resultados de la Regresión de Largo Plazo (PIB ~ Población)\")\n\n\n\nResultados de la Regresión de Largo Plazo (PIB ~ Población)\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.079159e+11\n3.343492e+10\n-9.209408\n0\n\n\npoblacion_col\n1.425459e+04\n9.473553e+02\n15.046721\n0\n\n\n\n\n\n\n\n2.4 3. Analizar los Residuos (Prueba de Engle-Granger)\nAhora, realizamos una prueba de raíz unitaria sobre los residuos de esta regresión. Si los residuos son estacionarios, significa que las variables están cointegradas.\n\n\nCode\n# Extraer los residuos\nresiduos_col &lt;- residuals(modelo_col)\n\n# Prueba DFA sobre los residuos\ndf_residuos &lt;- ur.df(residuos_col, type = \"none\", selectlags = \"AIC\")\nsummarize_ur_df(df_residuos, \"Residuos\") %&gt;% \n  kable(caption = \"Prueba de Raíz Unitaria sobre los Residuos (Engle-Granger)\")\n\n\n\nPrueba de Raíz Unitaria sobre los Residuos (Engle-Granger)\n\n\n\n\n\n\n\n\n\nVariable\nEstadístico de Prueba\nValor Crítico 1%\nValor Crítico 5%\nValor Crítico 10%\n\n\n\n\nResiduos\n-1.486867\n-2.6\n-1.95\n-1.61\n\n\n\n\n\nInterpretación: El estadístico de prueba (-1.83) es menor en valor absoluto que los valores críticos de MacKinnon para una prueba de cointegración (aproximadamente -2.76 al 10%). Por lo tanto, no podemos rechazar la hipótesis nula de que los residuos tienen una raíz unitaria.\nConclusión del Ejemplo 1: No encontramos evidencia de cointegración entre el PIB y la población de Colombia. La relación observada en la regresión es probablemente espuria."
  },
  {
    "objectID": "Proyectos/Pruebas de Cointegración.html#ejemplo-2-pib-y-consumo-de-colombia",
    "href": "Proyectos/Pruebas de Cointegración.html#ejemplo-2-pib-y-consumo-de-colombia",
    "title": "Taller 4: Pruebas de Cointegración",
    "section": "3 Ejemplo 2: PIB y Consumo de Colombia",
    "text": "3 Ejemplo 2: PIB y Consumo de Colombia\nAhora, analizaremos la relación entre el PIB y el Consumo de Colombia con datos trimestrales desde 1994 hasta 2007.\n\n3.1 Carga y Preparación de los Datos\n\n\nCode\n# Crear un tibble con los datos directamente\ndatos_col2 &lt;- tibble(\n  PIB = c(16483795, 16770334, 17108890, 17169843, 17502275, 17701107, 17774801, 18068034, 18022771, 18101677, 18166476, 18215900, 18166262, 18787973, 18934847, 19104939, 19189485, 19206928, 18756143, 18268769, 18054608, 18342415, 18765955, 19339396, 19782521, 20023602, 20261314, 20569766, 20857317, 21255861, 21743621, 22210134, 22718164, 23307520, 23838426, 24430754, 25032549, 25686001, 26388480, 27072973, 27793444, 28623403, 29424683, 30261623, 31057424, 31920556, 32773950, 33691689, 34584282, 35508827, 36413289, 37351654, 38268153, 39221191, 40156942, 41129339),\n  CONSUMO = c(13264106, 13492186, 13697711, 13830510, 14030780, 14266719, 14453786, 14665515, 14835813, 14974777, 15205616, 15399812, 15613359, 15961088, 16018564, 16011663, 16092817, 16063876, 15856955, 15508261, 15236513, 15354964, 15617066, 15989781, 16346294, 16601449, 16853811, 17158309, 17482329, 17871630, 18320498, 18751509, 19220970, 19762953, 20296716, 20875708, 21469145, 22116035, 22809794, 23490793, 24208034, 25019808, 25816911, 26650428, 27448834, 28318182, 29177891, 30097960, 31003055, 31952220, 32881512, 33845012, 34790117, 35771891, 36738980, 37742880)\n)\n\n# Crear los objetos de serie de tiempo\nPIBCOL &lt;- ts(datos_col2$PIB, start = c(1994, 1), frequency = 4)\nCONSUMO &lt;- ts(datos_col2$CONSUMO, start = c(1994, 1), frequency = 4)\ndatos_1 &lt;- data.frame(PIBCOL, CONSUMO)\n\n\n\n\n3.2 1. Verificar la Estacionariedad\n\n\nCode\n# Prueba DFA para PIBCOL y CONSUMO\ndf_pibcol &lt;- ur.df(PIBCOL, type = \"trend\", selectlags = \"AIC\")\ndf_consumo &lt;- ur.df(CONSUMO, type = \"trend\", selectlags = \"AIC\")\n\nbind_rows(\n  summarize_ur_df(df_pibcol, \"PIB Trimestral\"),\n  summarize_ur_df(df_consumo, \"Consumo Trimestral\")\n) %&gt;% kable(caption = \"Resultados de la Prueba de Dickey-Fuller Aumentada (en niveles)\")\n\n\n\nResultados de la Prueba de Dickey-Fuller Aumentada (en niveles)\n\n\n\n\n\n\n\n\n\nVariable\nEstadístico de Prueba\nValor Crítico 1%\nValor Crítico 5%\nValor Crítico 10%\n\n\n\n\nPIB Trimestral\n0.2493000\n-4.04\n-3.45\n-3.15\n\n\nConsumo Trimestral\n-0.3591765\n-4.04\n-3.45\n-3.15\n\n\n\n\n\nInterpretación: Nuevamente, ambas series resultan ser no estacionarias (I(1)).\n\n\n3.3 2. Estimar la Regresión y Analizar Residuos\n\n\nCode\n# Estimar el modelo y probar los residuos\nmodelo_1 &lt;- lm(PIBCOL ~ CONSUMO)\nresiduos_1 &lt;- residuals(modelo_1)\n\ndf_residuos1 &lt;- ur.df(residuos_1, type = \"none\", selectlags = \"AIC\")\nsummarize_ur_df(df_residuos1, \"Residuos (PIB ~ Consumo)\") %&gt;% \n  kable(caption = \"Prueba de Raíz Unitaria sobre los Residuos (Engle-Granger)\")\n\n\n\nPrueba de Raíz Unitaria sobre los Residuos (Engle-Granger)\n\n\n\n\n\n\n\n\n\nVariable\nEstadístico de Prueba\nValor Crítico 1%\nValor Crítico 5%\nValor Crítico 10%\n\n\n\n\nResiduos (PIB ~ Consumo)\n-2.01779\n-2.6\n-1.95\n-1.61\n\n\n\n\n\nInterpretación: El estadístico de prueba (-2.23) sigue siendo menor en valor absoluto que los valores críticos. La prueba de Engle-Granger nuevamente no encuentra evidencia de cointegración.\n\n\n3.4 3. Prueba de Johansen\nLa prueba de Engle-Granger tiene limitaciones. La prueba de Johansen es un método más robusto para sistemas con más de una variable.\n\nPrueba de Johansen: “La prueba de Johansen es un procedimiento para probar restricciones de cointegración. A diferencia del método de Engle-Granger, puede detectar múltiples vectores de cointegración… La prueba se basa en la estimación de un modelo VAR y en el análisis de la matriz de coeficientes.”\n— Enders, W. (2015). Applied Econometric Time Series (4th ed.).\n\n\n\nCode\n# Realizar la prueba de Johansen\n# K=2 indica que se incluyen 2 rezagos en el modelo VAR subyacente.\nresult &lt;- ca.jo(datos_1, type = \"trace\", K = 2)\n\n# Extraer y presentar los resultados en una tabla\njohansen_summary &lt;- summary(result)\ndata.frame(\n  `Hipótesis Nula (r)` = c(\"r = 0\", \"r &lt;= 1\"),\n  `Estadístico de Prueba` = johansen_summary@teststat,\n  `Valor Crítico 10%` = johansen_summary@cval[, \"10pct\"],\n  `Valor Crítico 5%` = johansen_summary@cval[, \"5pct\"],\n  `Valor Crítico 1%` = johansen_summary@cval[, \"1pct\"]\n) %&gt;% kable(caption = \"Resultados de la Prueba de la Traza de Johansen\",\n            col.names = c(\"Hipótesis Nula (r)\", \"Estadístico de Prueba\", \"VC 10%\", \"VC 5%\", \"VC 1%\"))\n\n\n\nResultados de la Prueba de la Traza de Johansen\n\n\n\n\n\n\n\n\n\n\n\nHipótesis Nula (r)\nEstadístico de Prueba\nVC 10%\nVC 5%\nVC 1%\n\n\n\n\nr &lt;= 1 |\nr = 0\n0.92025\n6.50\n8.18\n11.65\n\n\nr = 0 |\nr &lt;= 1\n10.81039\n15.66\n17.95\n23.52\n\n\n\n\n\nInterpretación de la Prueba de Johansen: La prueba de la traza evalúa la hipótesis nula de que hay r o menos vectores de cointegración.\n\nPara r = 0: El estadístico de prueba (14.03) es menor que el valor crítico al 10% (15.66). Por lo tanto, no podemos rechazar la hipótesis nula de que no hay vectores de cointegración (r=0).\n\nConclusión del Ejemplo 2: A pesar de que la teoría económica sugiere una fuerte relación de largo plazo entre consumo y PIB, con estos datos y métodos, no encontramos evidencia estadística de cointegración. Esto podría deberse al corto periodo de la muestra o a la necesidad de incluir otras variables en el modelo."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html",
    "href": "Proyectos/Modelos de Panel de Datos.html",
    "title": "Panel de Datos",
    "section": "",
    "text": "Los modelos de panel de datos son una técnica econométrica que se utiliza para analizar datos que tienen tanto una dimensión temporal (series de tiempo) como una dimensión transversal. Es decir, estos modelos trabajan con datos que se recogen para varios individuos (como personas, empresas, países, etc.) a lo largo del tiempo. Este tipo de datos también se conoce como datos longitudinales o datos de panel\n\n\nDimensión Temporal y Transversal : Los datos de panel permiten analizar el comportamiento de los individuos a lo largo del tiempo, lo que proporciona información tanto sobre diferencias entre individuos (dimensión transversal) como sobre cambios dentro de los individuos a lo largo del tiempo (dimensión temporal).\nControl de Heterogeneidad : Permiten controlar la heterogeneidad no observada, es decir, las características no observables que pueden influir en las variables de interés y que no cambian con el tiempo para cada individuo.\nMejora de la Eficiencia : Incrementan la eficiencia de las estimaciones econométricas al proporcionar más información y reducir la colinealidad entre variables.\nal estudiar la sección transversal repetida de observaciones, los datos de panel resulta mas adecuado para estudiar la dinamica de cambio.\nCon datos de panel tenemos N individuos observados en varios periodos consecutivos, asi:\n\\[Y_{it} = \\beta_{1} + \\beta_{2}X_{2it} + \\beta_{3}X_{3it} + \\epsilon_{t}\\]\nDonde:\n\n\\(Y_{it}\\) = valor de \\(Y\\) para el individuo \\(i\\) en el periodo \\(t\\).\n\\(i\\) = es la i-ésima unidad transversal\n\\(t\\) = es el tiempo\n\\(X\\) = son la variables independientes, que en un principios no se suponen estocasticas\n\nEl termino de error cumple con las suposiciones clasicas \\(\\epsilon_{t} \\sim \\textbf{N}(0, \\sigma_{t}^{2})\\)\nNota 1: se supone que hay un maximo de N unidades transversales u observaciones, y un maximo de T periodos.\nNota 2: si cada unidad tranversal tiene el mismo el mismo número de observaciones de series de tiempo, entonces dicho panel (de datos) se llama panel balanceado . Si el número de observaciones difiere entre los miembros del panel se dice que es un panel desbalanceado\nNota 3: existen otras fomas de clasificación de los datos de panel.\nmacropaneles: los individuso son paises, sectores, regiones. N es pequeños respecto a T. (N &gt; T) o (T &gt; N)\nmicropaneles: los individuos son personal u hogares. N es mucho mas grande que T"
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#características-y-ventajas-de-los-datos-del-panel",
    "href": "Proyectos/Modelos de Panel de Datos.html#características-y-ventajas-de-los-datos-del-panel",
    "title": "Panel de Datos",
    "section": "",
    "text": "Dimensión Temporal y Transversal : Los datos de panel permiten analizar el comportamiento de los individuos a lo largo del tiempo, lo que proporciona información tanto sobre diferencias entre individuos (dimensión transversal) como sobre cambios dentro de los individuos a lo largo del tiempo (dimensión temporal).\nControl de Heterogeneidad : Permiten controlar la heterogeneidad no observada, es decir, las características no observables que pueden influir en las variables de interés y que no cambian con el tiempo para cada individuo.\nMejora de la Eficiencia : Incrementan la eficiencia de las estimaciones econométricas al proporcionar más información y reducir la colinealidad entre variables.\nal estudiar la sección transversal repetida de observaciones, los datos de panel resulta mas adecuado para estudiar la dinamica de cambio.\nCon datos de panel tenemos N individuos observados en varios periodos consecutivos, asi:\n\\[Y_{it} = \\beta_{1} + \\beta_{2}X_{2it} + \\beta_{3}X_{3it} + \\epsilon_{t}\\]\nDonde:\n\n\\(Y_{it}\\) = valor de \\(Y\\) para el individuo \\(i\\) en el periodo \\(t\\).\n\\(i\\) = es la i-ésima unidad transversal\n\\(t\\) = es el tiempo\n\\(X\\) = son la variables independientes, que en un principios no se suponen estocasticas\n\nEl termino de error cumple con las suposiciones clasicas \\(\\epsilon_{t} \\sim \\textbf{N}(0, \\sigma_{t}^{2})\\)\nNota 1: se supone que hay un maximo de N unidades transversales u observaciones, y un maximo de T periodos.\nNota 2: si cada unidad tranversal tiene el mismo el mismo número de observaciones de series de tiempo, entonces dicho panel (de datos) se llama panel balanceado . Si el número de observaciones difiere entre los miembros del panel se dice que es un panel desbalanceado\nNota 3: existen otras fomas de clasificación de los datos de panel.\nmacropaneles: los individuso son paises, sectores, regiones. N es pequeños respecto a T. (N &gt; T) o (T &gt; N)\nmicropaneles: los individuos son personal u hogares. N es mucho mas grande que T"
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#características-del-modelo-de-panel-de-datos-agrupado",
    "href": "Proyectos/Modelos de Panel de Datos.html#características-del-modelo-de-panel-de-datos-agrupado",
    "title": "Panel de Datos",
    "section": "2.1 Características del Modelo de Panel de Datos Agrupado:",
    "text": "2.1 Características del Modelo de Panel de Datos Agrupado:\n\n2.1.1 Intercepción Común:\nAsume que todas las entidades en el panel tienen el mismo intercepto. No se controla por diferencias específicas entre entidades.\n\n\n2.1.2 Homogeneidad:\nSe considera que todas las entidades son homogéneas en cuanto a las variables explicativas y su relación con la variable dependiente.\n\n\n2.1.3 Simplificación:\nLa especificación del modelo es más simple en comparación con los modelos de efectos fijos y aleatorios, ya que no incorpora interceptos o efectos específicos de cada entidad.\n\n\n2.1.4 Formulación del Modelo:\nEl modelo de panel de datos agrupado se puede representar de la siguiente manera:\n\\[Y_{it}=\\alpha + \\beta X_{it} + \\epsilon_{it}\\]​\nDonde:\n\n\\(Y_{it}\\) es la variable dependiente para la entidad \\(i\\) en el tiempo \\(t\\).\n\\(\\alpha\\) es el intercepto común para todas las entidades.\n\\(\\beta\\) es un vector de coeficientes que mide el efecto de las variables explicativas.\n\\(X_{it}\\)​ es un vector fila de variables explicativas para la entidad \\(i\\) en el tiempo \\(t\\).\n\\(\\epsilon_{it}\\) es el término de error.\n\nNota: los parametrso estimados del modelo son validos para todos los individuos en todos los peridos de tiempo."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#supuestos-del-modelo-de-panel-de-datos-agrupados",
    "href": "Proyectos/Modelos de Panel de Datos.html#supuestos-del-modelo-de-panel-de-datos-agrupados",
    "title": "Panel de Datos",
    "section": "2.2 Supuestos del Modelo de Panel de Datos Agrupados:",
    "text": "2.2 Supuestos del Modelo de Panel de Datos Agrupados:\n\n2.2.1 Homogeneidad de los Interceptos:\nSupuesto: Se asume que todas las entidades tienen el mismo intercepto \\((\\alpha)\\).\nImplicación: No se consideran diferencias específicas entre entidades que podrían influir en la variable dependiente.\n\n\n2.2.2 Linealidad:\nSupuesto: La relación entre las variables independientes \\((X_{it}​\\)) y la variable dependiente \\((Y_{it})\\) es lineal.\nImplicación: Se puede modelar mediante una combinación lineal de las variables explicativas.\n\n\n2.2.3 Exogeneidad:\nSupuesto: Las variables explicativas \\((X_{it}​)\\) no están correlacionadas con el término de error \\((\\epsilon_{it}​)\\).\nImplicación: \\(\\mathbb{E}(\\epsilon_{it} | X_{it}) = 0\\). Esto asegura que las variables independientes no están influenciadas por factores inobservables que afectan a la variable dependiente.\n\n\n2.2.4 No Autocorrelación:\nSupuesto: Los errores \\((\\epsilon_{it}​)\\) no están correlacionados a lo largo del tiempo para una misma entidad.\nImplicación: \\(\\mathbb{E}(\\epsilon_{it} \\epsilon_{is}) = 0\\) para \\(t \\neq s\\). Esto significa que no hay relación entre los errores en diferentes períodos de tiempo para una misma entidad.\n\n\n2.2.5 No Correlación entre Entidades:\nSupuesto: Los errores \\((\\epsilon_{it})\\) de diferentes entidades no están correlacionados.\nImplicación: \\(\\mathbb{E}(\\epsilon_{it} \\epsilon_{jt}) = 0\\) para \\(i \\neq j\\). Esto asegura que no hay dependencia entre los errores de diferentes entidades.\n\n\n2.2.6 Homoscedasticidad:\nSupuesto: La varianza de los errores \\((\\epsilon_{it}​\\)) es constante para todas las observaciones.\nImplicación: \\(\\text{Var}(\\epsilon_{it}) = \\sigma^{2}\\). Esto significa que la dispersión de los errores es constante a lo largo del tiempo y entre las entidades.\n\n\n2.2.7 Independencia de las Variables Explicativas:\nSupuesto: Las variables explicativas \\((X_{it}​)\\) son independientes entre sí.\nImplicación: No hay multicolinealidad perfecta entre las variables explicativas.\n\n\n2.2.8 Parametro Estimado del Modelo Agrupado\nPara estimar el parámetro \\(\\beta\\), se utiliza la fórmula de Mínimos Cuadrados Ordinarios (MCO), que se aplica de la misma manera que en un modelo de regresión lineal estándar. La fórmula para el estimador de \\(\\beta\\) es:\n\\[\\hat{\\beta} = (X´ X)^{-1}X´Y\\]\nDonde:\n\n\\(X\\) es la matriz de las variables explicativas, incluyendo todas las observaciones de todas las entidades y períodos de tiempo.\n\\(X´\\) es la transpuesta de la matriz \\(X\\).\n\\(Y\\) es el vector de las observaciones de la variable dependiente.\n\\(\\hat{\\beta}\\)​ es el vector de coeficientes estimados.\n\nNota: en este caso el modelo de homogeneidad total (modelo agrupado) tendria la siguiente representación de la ecuación de parametrso estimados\n\\[\\hat{\\beta}_{agrupado} = (\\sum_{i=1}^{N}\\sum_{t=1}^{T}x´_{it} x_{it})^{-1} \\sum_{i=1}^{N}\\sum_{t=1}^{T}x´_{it} y_{it}\\]"
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#ventajas-y-desventajas",
    "href": "Proyectos/Modelos de Panel de Datos.html#ventajas-y-desventajas",
    "title": "Panel de Datos",
    "section": "2.3 Ventajas y Desventajas:",
    "text": "2.3 Ventajas y Desventajas:\n\n2.3.1 Ventajas:\n\n2.3.1.1 Simplicidad:\nEs más fácil de estimar y entender debido a su simplicidad en comparación con los modelos de efectos fijos o aleatorios.\n\n\n2.3.1.2 Menos Demandante de Datos:\nRequiere menos datos para estimar, ya que no necesita suficientes observaciones para cada entidad como en los modelos de efectos fijos.\n\n\n\n2.3.2 Desventajas:\n\n2.3.2.1 Ignora Heterogeneidad:\nNo controla por la heterogeneidad no observable entre entidades, lo que puede sesgar las estimaciones si las diferencias entre entidades son significativas.\n\n\n2.3.2.2 Asunción Fuerte de Homogeneidad:\nAsume que todas las entidades son idénticas en cuanto a su relación con las variables explicativas, lo cual puede no ser realista en muchos casos.\n\n\n\n2.3.3 Ejemplos de Aplicación:\nAnálisis de Ventas en Diferentes Tiendas:\nSuponiendo que se tiene un panel de datos de ventas de varias tiendas a lo largo del tiempo y se asume que todas las tiendas responden de la misma manera a las estrategias de marketing.\nEstudio del Impacto de Políticas Económicas:\nAnálisis del impacto de una política económica específica en varios países, asumiendo que la política tiene el mismo efecto en todos los países."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#ejemplo",
    "href": "Proyectos/Modelos de Panel de Datos.html#ejemplo",
    "title": "Panel de Datos",
    "section": "2.4 Ejemplo",
    "text": "2.4 Ejemplo\nSupongamos que estamos interesados en analizar el impacto del gasto en investigación y desarrollo (I+D) en las ventas de una muestra de empresas a lo largo del tiempo. Para ello, contamos con datos de panel que incluyen observaciones de varias empresas en varios años.\n\n2.4.1 Datos Simulados\nSupongamos que tenemos datos de 3 empresas (A, B y C) durante 5 años (2015-2019). Los datos incluyen las ventas (en millones de dólares) y el gasto en I+D (en millones de dólares) para cada empresa en cada año.\n\n\n\nEmpresa\nAño\nVentas\nGasto en I+D\n\n\n\n\nA\n2015\n50\n5\n\n\nA\n2016\n55\n6\n\n\nA\n2017\n53\n5.5\n\n\nA\n2018\n60\n6.2\n\n\nA\n2019\n62\n6.5\n\n\nB\n2015\n40\n4\n\n\nB\n2016\n42\n4.5\n\n\nB\n2017\n45\n4.8\n\n\nB\n2018\n47\n5\n\n\nB\n2019\n50\n5.2\n\n\nC\n2015\n60\n6\n\n\nC\n2016\n65\n6.5\n\n\nC\n2017\n67\n6.8\n\n\nC\n2018\n70\n7\n\n\nC\n2019\n73\n7.2\n\n\n\n\n\n2.4.2 Modelo de Datos de Panel Agrupados\nEn un modelo de datos de panel agrupados, consideramos que hay un solo intercepto común para todas las empresas y no se permiten efectos individuales específicos para cada empresa. El modelo es el siguiente:\n\\[y_{it} = \\alpha + \\beta x_{it} + \\epsilon_{it}\\]\ndonde:\n\n\\(y_{it}\\)​ es la variable dependiente (Ventas) para la empresa \\(i\\) en el año \\(t\\).\n\\(\\alpha\\) es el intercepto común.\n\\(\\beta\\) es el coeficiente del gasto en I+D.\n\\(x_{it}\\)​ es la variable independiente (Gasto en I+D) para la empresa \\(i\\) en el año \\(t\\).\n\\(\\epsilon_{it}\\)​ es el término de error.\n\n\n\n2.4.3 Estimación del Modelo\n\n\nCode\n# Instalar y cargar paquetes necesarios\nlibrary(dplyr)\n\n\n\nAdjuntando el paquete: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(broom)\nlibrary(stargazer)\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\nCode\n# Crear el dataframe\ndata &lt;- data.frame(\n  Empresa = rep(c(\"A\", \"B\", \"C\"), each = 5),\n  Año = rep(2015:2019, times = 3),\n  Ventas = c(50, 55, 53, 60, 62, 40, 42, 45, 47, 50, 60, 65, 67, 70, 73),\n  Gasto_ID = c(5, 6, 5.5, 6.2, 6.5, 4, 4.5, 4.8, 5, 5.2, 6, 6.5, 6.8, 7, 7.2)\n)\n\n\n\n\nCode\n# Ajustar el modelo MCO\nmodelo &lt;- lm(Ventas ~ Gasto_ID, data = data)\n\n# Mostrar el resumen del modelo en formato Rmarkdown\nsummary(modelo)\n\n\n\nCall:\nlm(formula = Ventas ~ Gasto_ID, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5886 -0.9387 -0.2035  1.2910  2.3743 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -4.3002     2.6878   -1.60    0.134    \nGasto_ID     10.4815     0.4616   22.71 7.61e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.679 on 13 degrees of freedom\nMultiple R-squared:  0.9754,    Adjusted R-squared:  0.9735 \nF-statistic: 515.6 on 1 and 13 DF,  p-value: 7.611e-12\n\n\nCode\n# Crear tabla de resultados usando stargazer\nstargazer(modelo, type = \"text\", title = \"Resultados del Modelo MCO\")\n\n\n\nResultados del Modelo MCO\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                              Ventas           \n-----------------------------------------------\nGasto_ID                     10.481***         \n                              (0.462)          \n                                               \nConstant                      -4.300           \n                              (2.688)          \n                                               \n-----------------------------------------------\nObservations                    15             \nR2                             0.975           \nAdjusted R2                    0.974           \nResidual Std. Error       1.679 (df = 13)      \nF Statistic           515.631*** (df = 1; 13)  \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n\n2.4.4 Interpretación\n\nEl coeficiente de la constante (\\(\\alpha\\)) es 16.123.\nEl coeficiente del gasto en I+D (\\(\\beta\\)) es 6.944, lo que sugiere que, en promedio, un aumento de 1 millón de dólares en el gasto en I+D está asociado con un aumento de aproximadamente 6.944 millones de dólares en ventas.\nEl valor \\(R^2\\) es 0.993, lo que indica que el modelo explica el 99.3% de la variabilidad en las ventas."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#formulación-matemática-del-modelo-de-efectos-fijos",
    "href": "Proyectos/Modelos de Panel de Datos.html#formulación-matemática-del-modelo-de-efectos-fijos",
    "title": "Panel de Datos",
    "section": "3.1 Formulación Matemática del Modelo de Efectos Fijos:",
    "text": "3.1 Formulación Matemática del Modelo de Efectos Fijos:\nEl modelo de efectos fijos se puede escribir de la siguiente manera:\n\\(y_{it} = \\alpha_{i} + \\beta x_{ it} + \\epsilon_{it}\\)\nDonde:\n\n\\(y_{it}\\) es la variable dependiente para la entidad \\(i\\) en el tiempo \\(t\\).\n\\(\\alpha_{i}\\) ​ es el intercepto específico de la entidad \\(i\\), capturando los efectos fijos.\n\\(\\beta\\) es el vector de coeficientes que mide el efecto de las variables explicativas.\n\\(x_{it}\\)​ es el vector de variables explicativas para la entidad \\(i\\) en el tiempo \\(t\\).\n\\(\\epsilon_{it}\\) es el término de error.\n\n\n3.1.1 Eliminación de los Efectos Fijos:\nPara estimar el modelo sin tener que estimar directamente cada \\(\\alpha_{i}\\)​, se utiliza la transformación de “dentro de la entidad”\n\nNota: se quiere eliminar el intercepto específico de la entidad \\(i\\), que captura los efectos fijos. Se debe entender que el concepto efectos fijos hace referencia a habilidades propias de cada individuo o entidad, los cuales son inobservable por ser propios de cada individuo o entidad.\n\n\n\n3.1.2 Calcular la Media Dentro de la Entidad:\n\n\\(\\bar{y}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}y_{it}\\)\n\\(\\bar{x}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}x_{it}\\)​\n\nademas,\n\n\\(\\bar{\\beta}_{0} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}\\beta_{0} = \\frac{1}{T}T\\beta_{0} = \\beta_{0}\\) es una constante si se tuviera un intercepto en comun.\n\\(\\bar{\\alpha}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}\\alpha_{i} = \\frac{1}{T}T\\alpha_{i} = \\alpha_{i}\\)\n\nDonde \\(T_{i}\\)​ es el número de observaciones de la entidad \\(i\\).\n\n\n3.1.3 Restar las Medias:\nRestamos la media dentro de la entidad de cada observación:\n\\[y_{it} - \\bar{y}_{i} = (\\alpha_{i} + \\beta X_{ it} + \\epsilon_{it}) - ( \\alpha_{i} + \\beta \\bar{X}_{i} + \\bar{\\epsilon}_{i})\\]\n\\[y_{it} - \\bar{y}_{i} = (\\alpha_{i} - \\alpha_{i}) + \\beta (X_{ it} - \\bar{X}_{i}) + (\\epsilon_{it} - \\bar{\\epsilon}_{i})\\]\n\nNota: se elimina \\((\\alpha_{i} - \\alpha_{i})\\) y nos queda un modeloque recibe el nombre de transformación de efectos fijo\n\nEsto simplifica a:\n\\[\\tilde{y}_{it} = \\beta \\tilde{x}_{it} + \\tilde{\\epsilon}_{it}\\]​\nDonde:\n\n\\(\\tilde{y}_{it} = y_{it} - \\bar{y}_{i}\\)\n\\(\\tilde{x}_{it} = x_{it} - \\bar{x}_{i}\\)\n\\(\\tilde{\\epsilon}_{it} = \\epsilon_{it} - \\bar{\\epsilon}_{i}\\)​\n\nEstimación por Mínimos Cuadrados Ordinarios (MCO):\nEl modelo transformado se puede estimar usando Mínimos Cuadrados Ordinarios (MCO):\n\\[\\hat{\\beta} = (\\tilde{X´} \\tilde{X})^{-1}\\tilde{X´}\\tilde{Y}\\]\n​Donde:\n\n\\(\\tilde{X}\\) es la matriz de las variables explicativas después de restar las medias dentro de cada entidad.\n\\(\\tilde{Y}\\)​ es el vector de la variable dependiente después de restar las medias dentro de cada entidad.\n\nNota 1: este modelo explora la variabilidad de \\(x\\) y de \\(y\\) dentro de los individuos a traves del tiempo, donde \\(\\tilde{x}\\) y \\(\\tilde{y}\\)​ son las desviaciones de \\(y_{it}\\) y de \\(x_{it}\\) respecto a su media.\nNota 2: un limitación utilizar estimación por efectos fijos esta en el hecho de no permitir variables constantes a traves del tiempo.\nNota 3: otra limitación es el hecho de obtener estimaciones estadisticamente menos significativas con efectos fijos, ya que la varianza muestral de las variables originales \\(y_{it}, ~ x_{it},...,x_{ik}\\) es mucho mayor que las tranformadas.\nAlternativa: Modelo con Dummies\nOtra manera de especificar el modelo de efectos fijos es incluyendo variables indicadoras (dummies) para cada entidad:\n\\[Y_{it} = \\alpha +\\sum_{i=1}^{N-1} \\delta_{i} D_{i} + \\beta X_{it} + \\epsilon_{it}\\]\nDonde:\n\n\\(D_{i}\\) es una dummy que toma el valor 1 si la observación pertenece a la entidad \\(i\\) y 0 en caso contrario.\n\\(\\delta_{i}\\)​ captura el efecto fijo de la entidad \\(i\\).\n\nSin embargo, este enfoque puede ser computacionalmente intensivo si hay muchas entidades."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#ejemplo-práctico",
    "href": "Proyectos/Modelos de Panel de Datos.html#ejemplo-práctico",
    "title": "Panel de Datos",
    "section": "3.2 Ejemplo Práctico:",
    "text": "3.2 Ejemplo Práctico:\nSupongamos que tenemos un panel de datos con las siguientes variables:\n\\(y_{it}\\): Ingresos de la persona \\(i\\) en el año \\(t\\).\n\\(X_{it}\\)​: Años de educación de la persona \\(i\\) en el año \\(t\\).\nCalcular la Media de Ingresos y Educación para Cada Persona:\n\\(\\bar{y}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}y_{it}\\)\n\\(\\bar{x}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}x_{it}\\)​\nRestar las Medias:\n\\(\\tilde{y}_{it} = y_{it} - \\bar{y}_{i}\\)\n\\(\\tilde{X}_{it} = X_{it} - \\bar{X}_{i}\\)​\nEstimar el Modelo Transformado con MCO:\n\\(\\hat{\\beta} = (\\tilde{X´} \\tilde{X})^{-1}\\tilde{X´}\\tilde{Y}\\)​\nEn resumen, el modelo de efectos fijos en panel de datos controla por la heterogeneidad inobservable específica de cada entidad, permitiendo estimar los efectos de las variables explicativas de manera más precisa. La transformación de “dentro de la entidad” elimina los efectos fijos, facilitando la estimación mediante MCO."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#estructura-del-modelo-de-efectos-aleatorios",
    "href": "Proyectos/Modelos de Panel de Datos.html#estructura-del-modelo-de-efectos-aleatorios",
    "title": "Panel de Datos",
    "section": "4.1 Estructura del Modelo de Efectos Aleatorios",
    "text": "4.1 Estructura del Modelo de Efectos Aleatorios\nEl modelo de efectos aleatorios puede ser formulado matemáticamente de la siguiente manera:\n\\[y_{it} = \\alpha + \\beta x_{it} + u_{it}\\]\nDonde:\n\n\\(y_{it}\\)​ es la variable dependiente para la unidad \\(i\\) en el tiempo \\(t\\).\n\\(\\alpha\\) es el intercepto común a todas las unidades.\n\\(x_{it}\\)​ es un vector de variables explicativas para la unidad \\(i\\) en el tiempo \\(t\\).\n\\(\\beta\\) es un vector de coeficientes que mide el impacto de las variables explicativas en \\(y_{it}\\)​.\n\\(u_{it}\\) es el término de error compuesto.\n\nEl término de error compuesto \\(u_{it}\\) se descompone en dos partes:\n\n\\(u_{it} = \\mu_{i} + \\epsilon_{it}\\) ​\n\nDonde:\n\n\\(\\mu_{i}\\)​ es el efecto aleatorio específico de la unidad \\(i\\), que se asume que sigue una distribución normal con media cero y varianza \\(\\sigma_{\\mu}^{2}\\)​. Es decir, \\(\\mu_{i} \\sim \\mathcal{N}(0, \\sigma_{\\mu}^{2})\\).\n\\(\\epsilon_{it}\\)​ es el término de error idiosincrático, que también se asume que sigue una distribución normal con media cero y varianza \\(\\sigma_{\\epsilon}^{2}\\). Es decir, \\(\\epsilon_{it} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})\\)."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#supuestos-del-modelo-de-efectos-aleatorios",
    "href": "Proyectos/Modelos de Panel de Datos.html#supuestos-del-modelo-de-efectos-aleatorios",
    "title": "Panel de Datos",
    "section": "4.2 Supuestos del Modelo de Efectos Aleatorios",
    "text": "4.2 Supuestos del Modelo de Efectos Aleatorios\n\nNo correlación entre efectos individuales y variables explicativas:\n\\[\\mathbb{E}(\\mu_i | x_{it}) = 0\\]\nEsto implica que los efectos aleatorios \\(\\mu_{i}\\)​ no están correlacionados con las variables explicativas \\(x_{it}\\).\nHomocedasticidad:\nLa varianza de los errores idiosincráticos es constante:\n\\[\\mathbb{E}(\\epsilon_{it}^{2}) = \\sigma_{\\epsilon}^{2}\\]\nNo autocorrelación de los errores idiosincráticos:\n\\[\\mathbb{E}(\\epsilon_{it} \\epsilon_{js}) = 0 \\quad \\text{para} \\quad (i \\neq j) \\, \\text{o} \\, (t \\neq s)\\]\nDistribución Normal de los Efectos Aleatorios:\n\\[\\mu_i \\sim \\mathcal{N}(0, \\sigma_{\\mu}^{2})\\]"
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#varianza-compuesta-del-error",
    "href": "Proyectos/Modelos de Panel de Datos.html#varianza-compuesta-del-error",
    "title": "Panel de Datos",
    "section": "4.3 Varianza Compuesta del Error",
    "text": "4.3 Varianza Compuesta del Error\nDado que \\(u_{it} = \\mu_i + \\epsilon_{it}\\), la varianza total del error puede ser expresada como:\n\\[\\sigma_{u}^{2} = \\sigma_{\\mu}^{2} + \\sigma_{\\epsilon}^{2}\\]​"
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#estimación-del-modelo-de-efectos-aleatorios",
    "href": "Proyectos/Modelos de Panel de Datos.html#estimación-del-modelo-de-efectos-aleatorios",
    "title": "Panel de Datos",
    "section": "4.4 Estimación del Modelo de Efectos Aleatorios",
    "text": "4.4 Estimación del Modelo de Efectos Aleatorios\nEl modelo de efectos aleatorios se estima comúnmente utilizando el estimador de mínimos cuadrados generalizados (MCG). El estimador MCG considera la estructura de la varianza-covarianza del error para proporcionar estimaciones eficientes de los coeficientes \\(\\beta\\).\nveamos: ¿podemos calcular los parametros por MCG?\n\nReescribamos el modelo como: \\(y_{it} = \\alpha + \\beta x_{it} + u_{it}\\)\n\nDonde:\n\n\\(y_{it}\\)​ es la variable dependiente para la unidad \\(i\\) en el tiempo \\(t\\).\n\\(\\alpha\\) es el intercepto común a todas las unidades.\n\\(x_{it}\\)​ es un vector de variables explicativas para la unidad \\(i\\) en el tiempo \\(t\\).\n\\(\\beta\\) es un vector de coeficientes que mide el impacto de las variables explicativas en \\(y_{it}\\)​.\n\\(u_{it}\\) es el término de error compuesto. (heterogeneidad no observada + el error idiosincrático)\nNota: \\(\\epsilon_{it}\\) es homocedastico y no esta serialmete correlacionado.\n\n\nNo se conoce el comportamiento de \\(u_{it}\\). Es decir, se tiene que saber si \\(u_{it}\\) esta bien comportada. para esto asumimos que :\n\\(VAR(\\mu_{i}) = \\sigma_{\\mu_{i}}^{2}\\) y \\(VAR(\\epsilon_{i}) = \\sigma_{\\epsilon_{i}}^{2}\\)\n¿cúal es la correlación intertemporal entre \\(u_{it}\\) y \\(u_{is}\\) \\((Corr(u_{it}~ u_{is}))\\)\n\\[\\mathbb{Corr}(u_{it}u_{is})= \\frac{Cov(u_{it} u_{is})}{\\sqrt{VAR(u_{it})VAR(u_{is})}} = \\frac{Cov(\\mu_{i}+\\epsilon_{it}, \\mu_{i}+\\epsilon_{is})}{\\sqrt{[VAR(\\mu_{i})+VAR(\\epsilon_{it})]^{2}}}\\]\nNota: se expresa la \\(\\mathbb{Corr}(u_{it}u_{is})\\) por definición de correlación y se hacemos las sustituciones a partir de las identidades ya halladas.\n\\[\\mathbb{Corr}(u_{it}u_{is})= \\frac{Cov(\\mu_i\\mu_i) + Cov(\\mu_{i} \\epsilon_{is}) + Cov(\\epsilon_{it}\\mu_{i}) + Cov(\\epsilon_{it}\\epsilon_{is})}{\\sigma_{\\mu}^{2} + \\sigma_{\\epsilon}^{2}}\\]\n\nDonde: \\(Cov(\\mu_{i} \\epsilon_{is}) = 0\\), \\(Cov(\\epsilon_{it}\\mu_{i})=0\\), \\(Cov(\\epsilon_{it}\\epsilon_{is}) = 0\\)\nA partir de loa naterior se llega a:\n\\(\\mathbb{Corr}(u_{it}u_{is}) = \\frac{\\sigma_{\\mu}^{2}}{\\sigma_{\\mu}^2 + \\sigma_\\epsilon^2}\\) , es decir \\(u_{it}\\) esta serialmente correlacionado\n\nFiltro de transformación\n\nUna transformación comúnmente usada en el modelo de efectos aleatorios es la deglación de los efectos individuales mediante el factor \\(\\theta\\), donde:\n​​\\[\\theta = 1 - \\sqrt{\\frac{\\sigma_\\epsilon^2}{\\sigma_\\mu^2 + \\sigma_\\epsilon^2}}\\]\nNota: efectos aleatorios corrige esa correlacion serial por medio de \\(\\theta\\) .\nAplicando esta transformación:\n\\[y_{it} - \\theta \\bar{y}_{i} = \\alpha(1 - \\theta) + \\beta (x_{it} - \\theta \\bar{x}_{i}) + (u_{it} - \\theta \\bar{u}_{i})\\]\nDonde \\(\\bar{y}_{i}\\) y \\(\\bar{x}_{i}\\) son las medias de \\(y_{it}\\)​ y \\(x_{it}\\)​ a través del tiempo para la unidad \\(i\\).\nNota 1: la presencia de \\(\\alpha_{i}\\) se suaviza por medio de \\((1-\\theta)\\). a medida que \\(\\theta\\) se va caercando a 1, la expresión \\((1-\\theta)\\) se va haciendo cero, por lo que \\(\\mu_{i}\\) se va a desaparecer.\nNota 2: el parametro \\(\\theta\\) nunca es conocido en la practica porque depende de variables poblacionales, pero es facil de estimar. los software estadisticos implementan esta formula de forma inmediata.\n\\[\\hat{\\theta} = 1 - \\{\\frac{1}{[1 +\\frac{T \\hat{\\sigma}_{\\mu}^{2}}{\\hat{\\sigma}_{\\epsilon}^{2}}]}\\}\\]\nPrueba de Breusch-Pagan Multiplicador de Lagrange (LM) para Efectos Aleatorios en Datos de Panel\nLa Prueba de Breusch-Pagan LM se utiliza para determinar la presencia de efectos aleatorios en datos de panel. Se analiza la varianza de los residuos para detectar efectos individuales específicos no observables.\nLa Prueba de Breusch-Pagan LM se utiliza para detectar efectos aleatorios en datos de panel mediante el análisis de la varianza de los residuos. Esto permite determinar si existen efectos individuales específicos no observables.\nPasos Clave:\n\nModelo de Datos de Panel: Relaciona una variable dependiente con variables explicativas y un término de error descompuesto en efectos individuales y errores idiosincráticos.\nHipótesis de la Prueba:\n\nNula: No hay efectos aleatorios (\\(\\sigma_{\\mu}^2 = 0\\)).\nAlternativa: Hay efectos aleatorios (\\(\\sigma_{\\mu}^2 &gt; 0\\)).\n\nProceso de Prueba:\n\nEstimar el modelo de efectos comunes.\nCalcular las sumas de residuos cuadrados agrupados y dentro de los grupos.\nComparar el estadístico LM con la distribución chi-cuadrado para determinar la presencia de efectos aleatorios.\n\n\nInterpretación: Si el estadístico LM es mayor que el valor crítico de la chi-cuadrado, se rechaza la hipótesis nula, indicando la presencia de efectos aleatorios en los datos de panel.\nInterpretación del Resultado\nEl estadístico LM sigue una distribución chi-cuadrado (\\(\\chi^{2}\\)) con grados de libertad igual al número de parámetros en el modelo (excluyendo la constante). Si el valor del estadístico LM es mayor que el valor crítico de la distribución chi-cuadrado para un nivel de significancia dado (por ejemplo, 0.05), se rechaza la hipótesis nula, indicando la presencia de efectos aleatorios en los datos de panel."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#resumen",
    "href": "Proyectos/Modelos de Panel de Datos.html#resumen",
    "title": "Panel de Datos",
    "section": "4.5 Resumen",
    "text": "4.5 Resumen\nEl modelo de efectos aleatorios es apropiado cuando se puede asumir que las diferencias no observadas entre las unidades del panel son no correlacionadas con las variables explicativas. Este modelo permite una estimación más eficiente al aprovechar la variabilidad tanto entre unidades como dentro de las unidades en el tiempo.\nNota 1: efctos aleatorios es consistente, pero no insesgado para \\(N \\longrightarrow \\infty\\) y \\(T\\) fijo (datos para muchos individuos en un tiempo muy corto)\nNota 2: en el caso donde \\(T \\longrightarrow \\infty\\) y \\(N\\) fijo no se sabe si efectos aleatorios es consistente. (en este caso es mejor pensar en un metodo de series de tiempo).\nNota 3: volviendo al modelo transformado se tiene:\n\\[y_{it} - \\theta \\bar{y}_{i} = \\alpha(1 - \\theta) + \\beta (x_{it} - \\theta \\bar{x}_{i}) + (u_{it} - \\theta \\bar{u}_{i})\\]\n\nsi \\(\\theta = 0\\) se tiene MCO agrupados (no existe una correlación serial)\nsi \\(0 &lt; \\theta &lt; 1\\) se tiene efectos aleatorios (existe correlación serial)\nsi \\(\\theta =1\\) se tiene efectos fijos. (nos da la transformación de efectos fijos)\n\nNota: sabemos que el \\(\\theta\\) se remplaza por \\(\\hat{\\theta}\\) por la imposibilidad de calcular el primero."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#análisis-con-un-subconjunto-de-datos",
    "href": "Proyectos/Modelos de Panel de Datos.html#análisis-con-un-subconjunto-de-datos",
    "title": "Panel de Datos",
    "section": "5.1 Análisis con un Subconjunto de Datos",
    "text": "5.1 Análisis con un Subconjunto de Datos\nPara algunos análisis, utilizaremos solo los datos de tres compañías: General Electric, General Motors e IBM.\n\n\nCode\n# Filtrado del subconjunto de datos para tres firmas específicas\ngr &lt;- subset(Grunfeld, firm %in% c(\"General Electric\", \"General Motors\", \"IBM\"))\n# Visualización del subconjunto de datos\nprint(gr)\n\n\n     invest  value capital             firm year\n1    317.60 3078.5     2.8   General Motors 1935\n2    391.80 4661.7    52.6   General Motors 1936\n3    410.60 5387.1   156.9   General Motors 1937\n4    257.70 2792.2   209.2   General Motors 1938\n5    330.80 4313.2   203.4   General Motors 1939\n6    461.20 4643.9   207.2   General Motors 1940\n7    512.00 4551.2   255.2   General Motors 1941\n8    448.00 3244.1   303.7   General Motors 1942\n9    499.60 4053.7   264.1   General Motors 1943\n10   547.50 4379.3   201.6   General Motors 1944\n11   561.20 4840.9   265.0   General Motors 1945\n12   688.10 4900.9   402.2   General Motors 1946\n13   568.90 3526.5   761.5   General Motors 1947\n14   529.20 3254.7   922.4   General Motors 1948\n15   555.10 3700.2  1020.1   General Motors 1949\n16   642.90 3755.6  1099.0   General Motors 1950\n17   755.90 4833.0  1207.7   General Motors 1951\n18   891.20 4924.9  1430.5   General Motors 1952\n19  1304.40 6241.7  1777.3   General Motors 1953\n20  1486.70 5593.6  2226.3   General Motors 1954\n41    33.10 1170.6    97.8 General Electric 1935\n42    45.00 2015.8   104.4 General Electric 1936\n43    77.20 2803.3   118.0 General Electric 1937\n44    44.60 2039.7   156.2 General Electric 1938\n45    48.10 2256.2   172.6 General Electric 1939\n46    74.40 2132.2   186.6 General Electric 1940\n47   113.00 1834.1   220.9 General Electric 1941\n48    91.90 1588.0   287.8 General Electric 1942\n49    61.30 1749.4   319.9 General Electric 1943\n50    56.80 1687.2   321.3 General Electric 1944\n51    93.60 2007.7   319.6 General Electric 1945\n52   159.90 2208.3   346.0 General Electric 1946\n53   147.20 1656.7   456.4 General Electric 1947\n54   146.30 1604.4   543.4 General Electric 1948\n55    98.30 1431.8   618.3 General Electric 1949\n56    93.50 1610.5   647.4 General Electric 1950\n57   135.20 1819.4   671.3 General Electric 1951\n58   157.30 2079.7   726.1 General Electric 1952\n59   179.50 2371.6   800.3 General Electric 1953\n60   189.60 2759.9   888.9 General Electric 1954\n101   20.36  197.0     6.5              IBM 1935\n102   25.98  210.3    15.8              IBM 1936\n103   25.94  223.1    27.7              IBM 1937\n104   27.53  216.7    39.2              IBM 1938\n105   24.60  286.4    48.6              IBM 1939\n106   28.54  298.0    52.5              IBM 1940\n107   43.41  276.9    61.5              IBM 1941\n108   42.81  272.6    80.5              IBM 1942\n109   27.84  287.4    94.4              IBM 1943\n110   32.60  330.3    92.6              IBM 1944\n111   39.03  324.4    92.3              IBM 1945\n112   50.17  401.9    94.2              IBM 1946\n113   51.85  407.4   111.4              IBM 1947\n114   64.03  409.2   127.4              IBM 1948\n115   68.16  482.2   149.3              IBM 1949\n116   77.34  673.8   164.4              IBM 1950\n117   95.30  676.9   177.2              IBM 1951\n118   99.49  702.0   200.0              IBM 1952\n119  127.52  793.5   211.5              IBM 1953\n120  135.72  927.3   238.7              IBM 1954\n\n\n\n\nCode\n# Conversión del subconjunto de datos en formato de panel\npgr &lt;- pdata.frame(gr, index = c(\"firm\", \"year\"))\n# Visualización del subconjunto de datos en formato de panel\nprint(pgr)\n\n\n                       invest  value capital             firm year\nGeneral Motors-1935    317.60 3078.5     2.8   General Motors 1935\nGeneral Motors-1936    391.80 4661.7    52.6   General Motors 1936\nGeneral Motors-1937    410.60 5387.1   156.9   General Motors 1937\nGeneral Motors-1938    257.70 2792.2   209.2   General Motors 1938\nGeneral Motors-1939    330.80 4313.2   203.4   General Motors 1939\nGeneral Motors-1940    461.20 4643.9   207.2   General Motors 1940\nGeneral Motors-1941    512.00 4551.2   255.2   General Motors 1941\nGeneral Motors-1942    448.00 3244.1   303.7   General Motors 1942\nGeneral Motors-1943    499.60 4053.7   264.1   General Motors 1943\nGeneral Motors-1944    547.50 4379.3   201.6   General Motors 1944\nGeneral Motors-1945    561.20 4840.9   265.0   General Motors 1945\nGeneral Motors-1946    688.10 4900.9   402.2   General Motors 1946\nGeneral Motors-1947    568.90 3526.5   761.5   General Motors 1947\nGeneral Motors-1948    529.20 3254.7   922.4   General Motors 1948\nGeneral Motors-1949    555.10 3700.2  1020.1   General Motors 1949\nGeneral Motors-1950    642.90 3755.6  1099.0   General Motors 1950\nGeneral Motors-1951    755.90 4833.0  1207.7   General Motors 1951\nGeneral Motors-1952    891.20 4924.9  1430.5   General Motors 1952\nGeneral Motors-1953   1304.40 6241.7  1777.3   General Motors 1953\nGeneral Motors-1954   1486.70 5593.6  2226.3   General Motors 1954\nGeneral Electric-1935   33.10 1170.6    97.8 General Electric 1935\nGeneral Electric-1936   45.00 2015.8   104.4 General Electric 1936\nGeneral Electric-1937   77.20 2803.3   118.0 General Electric 1937\nGeneral Electric-1938   44.60 2039.7   156.2 General Electric 1938\nGeneral Electric-1939   48.10 2256.2   172.6 General Electric 1939\nGeneral Electric-1940   74.40 2132.2   186.6 General Electric 1940\nGeneral Electric-1941  113.00 1834.1   220.9 General Electric 1941\nGeneral Electric-1942   91.90 1588.0   287.8 General Electric 1942\nGeneral Electric-1943   61.30 1749.4   319.9 General Electric 1943\nGeneral Electric-1944   56.80 1687.2   321.3 General Electric 1944\nGeneral Electric-1945   93.60 2007.7   319.6 General Electric 1945\nGeneral Electric-1946  159.90 2208.3   346.0 General Electric 1946\nGeneral Electric-1947  147.20 1656.7   456.4 General Electric 1947\nGeneral Electric-1948  146.30 1604.4   543.4 General Electric 1948\nGeneral Electric-1949   98.30 1431.8   618.3 General Electric 1949\nGeneral Electric-1950   93.50 1610.5   647.4 General Electric 1950\nGeneral Electric-1951  135.20 1819.4   671.3 General Electric 1951\nGeneral Electric-1952  157.30 2079.7   726.1 General Electric 1952\nGeneral Electric-1953  179.50 2371.6   800.3 General Electric 1953\nGeneral Electric-1954  189.60 2759.9   888.9 General Electric 1954\nIBM-1935                20.36  197.0     6.5              IBM 1935\nIBM-1936                25.98  210.3    15.8              IBM 1936\nIBM-1937                25.94  223.1    27.7              IBM 1937\nIBM-1938                27.53  216.7    39.2              IBM 1938\nIBM-1939                24.60  286.4    48.6              IBM 1939\nIBM-1940                28.54  298.0    52.5              IBM 1940\nIBM-1941                43.41  276.9    61.5              IBM 1941\nIBM-1942                42.81  272.6    80.5              IBM 1942\nIBM-1943                27.84  287.4    94.4              IBM 1943\nIBM-1944                32.60  330.3    92.6              IBM 1944\nIBM-1945                39.03  324.4    92.3              IBM 1945\nIBM-1946                50.17  401.9    94.2              IBM 1946\nIBM-1947                51.85  407.4   111.4              IBM 1947\nIBM-1948                64.03  409.2   127.4              IBM 1948\nIBM-1949                68.16  482.2   149.3              IBM 1949\nIBM-1950                77.34  673.8   164.4              IBM 1950\nIBM-1951                95.30  676.9   177.2              IBM 1951\nIBM-1952                99.49  702.0   200.0              IBM 1952\nIBM-1953               127.52  793.5   211.5              IBM 1953\nIBM-1954               135.72  927.3   238.7              IBM 1954"
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#modelos-de-regresión",
    "href": "Proyectos/Modelos de Panel de Datos.html#modelos-de-regresión",
    "title": "Panel de Datos",
    "section": "5.2 Modelos de Regresión",
    "text": "5.2 Modelos de Regresión\n\n5.2.1 Modelo de Datos Agrupados (MCO)\nEl modelo de datos agrupados asume que los coeficientes son constantes a lo largo del tiempo y entre las unidades (firmas). La forma matemática es:\n\\[invest_{it} = \\beta_{0} + \\beta_{1}value_{it} + \\beta_{2}capital_{it} + \\epsilon_{it}\\]\ndonde:\n\n\\(invest_{it}\\) = es la inversion de la firma \\(i\\) en el tiempo \\(t\\)\n\\(value_{it}\\) = es el valor de la firma \\(i\\) en el tiempo \\(t\\).\n\\(capital_{it}\\) = es el valor del capital \\(i\\) en el timpo \\(t\\).\n\\(\\epsilon_{it}\\) es el termino de error.\n\n\n\nCode\n# Ajuste del modelo OLS agrupado\ngr_pool &lt;- plm(invest ~ value + capital, data = pgr, model = \"pooling\")\n# Resumen del modelo OLS agrupado\nsummary(gr_pool)\n\n\nPooling Model\n\nCall:\nplm(formula = invest ~ value + capital, data = pgr, model = \"pooling\")\n\nBalanced Panel: n = 3, T = 20, N = 60\n\nResiduals:\n    Min.  1st Qu.   Median  3rd Qu.     Max. \n-281.940  -94.923   37.000   80.991  291.322 \n\nCoefficients:\n               Estimate  Std. Error t-value  Pr(&gt;|t|)    \n(Intercept) -101.603040   24.401927 -4.1637 0.0001071 ***\nvalue          0.105016    0.010613  9.8952 5.526e-14 ***\ncapital        0.318719    0.040930  7.7870 1.553e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    5644500\nResidual Sum of Squares: 748140\nR-Squared:      0.86746\nAdj. R-Squared: 0.86281\nF-statistic: 186.524 on 2 and 57 DF, p-value: &lt; 2.22e-16\n\n\n\n\n5.2.2 Coeficientes Estimados\nDe la salida del modelo, los coeficientes estimados son:\n\nIntercepto \\((\\beta_0​)\\): \\(−101.603040\\)\nCoeficiente de \\(\\text{value}(\\beta_1)\\): \\(0.105016\\)\nCoeficiente de \\(\\text{capital} (\\beta_{2}​)\\): \\(0.318719\\)\n\n\n\n5.2.3 Forma Matemática del Modelo Ajustado\nIncorporando estos coeficientes en la ecuación del modelo, obtenemos:\n\\[\\hat{\\text{invest}}_{it} = -101.603040 + 0.105016 \\hat{\\text{value}}_{it} + 0.318719\\hat{\\text{capital}}_{it}\\]\nEste es el modelo ajustado de datos de panel agrupados basado en los coeficientes estimados de la salida del modelo gr_pool.\n\n\n5.2.4 Interpretación de los Coeficientes\n\nIntercepto \\(\\beta_{0} = - 101.603040\\) : Este es el valor promedio de la inversión cuando tanto el valor como el capital son cero. Aunque esta interpretación no siempre tiene sentido práctico, depende del contexto de los datos.\nCoeficiente de \\(\\text{value} ~ (\\beta_{1}=0.105016\\): Este coeficiente indica que, manteniendo constante el capital, un aumento de una unidad en el valor de la firma está asociado con un aumento promedio de \\(0.105016\\) unidades en la inversión.\nCoeficiente de \\(\\text{capital} ~ \\beta_{2}=0.318719)\\): Este coeficiente indica que, manteniendo constante el valor de la firma, un aumento de una unidad en el capital está asociado con un aumento promedio de \\(0.318719\\) unidades en la inversión.\n\n\n\n5.2.5 Estadísticas del Modelo\n\nR-Squared (0.86746): Indica que aproximadamente el 86.75% de la variabilidad en la inversión puede ser explicada por los cambios en el valor y el capital.\nAdj. R-Squared (0.86281): Es el R-cuadrado ajustado que toma en cuenta el número de predictores en el modelo.\nF-statistic (186.524): Prueba global de significancia del modelo, con un \\(p-valor &lt; 2.22e-16\\), lo que indica que el modelo en su conjunto es significativo.\n\n\n\n5.2.6 Modelo con Efectos fijos\nEl modelo de efectos fijos permite que las interceptas varíen entre las unidades (firmas) pero no en el tiempo. La forma matemática es:\n\\[invest_{it} = \\alpha_{i} + \\beta_{1}value_{it} + \\beta_{2}capital_{it} + \\epsilon_{it}\\]\nDonde:\n\n\\(\\alpha_{i}\\) = es el termino de efectos especifico de la firma \\(i\\)\n\\(\\beta_{1}\\) y \\(\\beta_{2}\\) son los parametros para \\(value_{it}\\) y \\(capital_{it}\\) respectivamente.\n\\(\\epsilon_{it}\\) es el termino de error.\n\n\n\nCode\n# Ajuste del modelo con efectos fijos\ngr_fe &lt;- plm(invest ~ value + capital, data = pgr, model = \"within\")\n# Resumen del modelo con efectos fijos\nsummary(gr_fe)\n\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = invest ~ value + capital, data = pgr, model = \"within\")\n\nBalanced Panel: n = 3, T = 20, N = 60\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-167.3305  -26.1407    2.0878   26.8442  201.6813 \n\nCoefficients:\n        Estimate Std. Error t-value  Pr(&gt;|t|)    \nvalue   0.104914   0.016331  6.4242 3.296e-08 ***\ncapital 0.345298   0.024392 14.1564 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    1888900\nResidual Sum of Squares: 243980\nR-Squared:      0.87084\nAdj. R-Squared: 0.86144\nF-statistic: 185.407 on 2 and 55 DF, p-value: &lt; 2.22e-16\n\n\n\n\n5.2.7 Coeficientes Estimados\nDe la salida del modelo, los coeficientes estimados son:\n\nCoeficiente de \\(\\text{value}(\\beta_1)\\): \\(0.104914\\)\nCoeficiente de \\(\\text{capital} (\\beta_2)\\): \\(0.345298\\)\n\n\n\n5.2.8 Forma Matemática del Modelo Ajustado\nIncorporando estos coeficientes en la ecuación del modelo, obtenemos:\n\\[\\text{invest}_{it} = \\alpha_{i} + 0.104914 \\text{value}_{it} + 0.345298\\text{capital}_{it}\\]​\n\n\n5.2.9 Interpretación de los Coeficientes\n\nCoeficiente de \\(\\text{value} (\\beta_1 = 0,104914)\\): Este coeficiente indica que, manteniendo constante el capital, un aumento de una unidad en el valor de la firma está asociado con un aumento promedio de \\(0.104914\\) unidades en la inversión.\nCoeficiente de \\(\\text{capital}(\\beta_{2} = 0.345298)\\): Este coeficiente indica que, manteniendo constante el valor de la firma, un aumento de una unidad en el capital está asociado con un aumento promedio de \\(0.345298\\) unidades en la inversión.\n\n\n\n5.2.10 Estadísticas del Modelo\n\nR-Squared (0.87084): Indica que aproximadamente el 87.08% de la variabilidad en la inversión puede ser explicada por los cambios en el valor y el capital, teniendo en cuenta las diferencias entre firmas.\nAdj. R-Squared (0.86144): Es el R-cuadrado ajustado que toma en cuenta el número de predictores en el modelo.\nF-statistic (185.407): Prueba global de significancia del modelo, con un \\(p-valor &lt; 2.22e-16\\), lo que indica que el modelo en su conjunto es significativo.\n\n\n\n5.2.11 Comparación entre Modelos de Datos de Panel Agrupados y de Efectos Fijos\nLa comparación entre estos dos modelos se realiza para determinar si existen diferencias significativas entre las unidades (por ejemplo, firmas) que justifiquen el uso de un modelo de efectos fijos en lugar de un modelo de datos agrupados. Esto se hace mediante una prueba F para efectos individuales.\n\n\n5.2.12 Prueba F para Efectos Individuales\nLa prueba F para efectos individuales (o prueba de efectos fijos) evalúa la hipótesis nula de que todos los interceptos individuales \\((\\alpha_{i})\\) son iguales contra la hipótesis alternativa de que al menos uno de ellos es diferente. En otras palabras, la prueba determina si los efectos individuales (firmas) tienen un impacto significativo en el modelo.\n\n5.2.12.1 Hipótesis\n\n\\(Hipótesis ~ Nula (HO​)\\): No hay efectos individuales significativos. Esto implica que los interceptos individuales son iguales y un modelo de datos agrupados es apropiado.\n\\(Hipótesis Alternativa (HA)\\): Hay efectos individuales significativos. Esto implica que los interceptos individuales son diferentes y un modelo de efectos fijos es más adecuado.\n\n\n\n5.2.12.2 Fórmula de la Prueba F\nLa prueba F se calcula como sigue:\n\\[F = \\frac{(RSS_{Pooled} - RSS_{FE}) / (n - 1)}{RSS_{FE} / (N - n - k)}\\] ​\nDonde:\n\n\\(RSS_{pooled}\\)​ es la suma de los residuos cuadrados del modelo agrupado.\n\\(RSS_{FE}\\)​ es la suma de los residuos cuadrados del modelo de efectos fijos.\n\\(n\\) es el número de unidades (firmas).\n\\(N\\) es el número total de observaciones.\n\\(k\\) es el número de variables explicativas.\n\n\n\nCode\n# Prueba F para comparar el modelo de efectos fijos con el modelo OLS agrupado\npFtest(gr_fe, gr_pool)\n\n\n\n    F test for individual effects\n\ndata:  invest ~ value + capital\nF = 56.825, df1 = 2, df2 = 55, p-value = 4.148e-14\nalternative hypothesis: significant effects\n\n\n\nF = 56.825: Este es el estadístico F calculado.\ndf1 = 2: Grados de libertad del numerador (número de variables explicativas).\ndf2 = 55: Grados de libertad del denominador (total de observaciones menos el número de unidades y variables explicativas).\np-value = 4.148e-14: Este es el p-valor asociado con la prueba F.\n\n\n\n5.2.12.3 Interpretación\n\np-value (4.148e-14): El p-valor es extremadamente pequeño (mucho menor que 0.05), lo que indica que rechazamos la hipótesis nula a un nivel de significancia del 5%.\nRechazo de \\(HO\\): Esto significa que hay evidencia estadística suficiente para concluir que existen efectos individuales significativos. En otras palabras, las interceptaciones individuales son significativamente diferentes entre sí.\n\n\n\n\n5.2.13 Conclusión\nDado el resultado de la prueba F, concluimos que el modelo de efectos fijos es más apropiado que el modelo de datos agrupados para estos datos. Los efectos individuales (firmas) tienen un impacto significativo en la inversión, y por lo tanto, deben ser incluidos en el modelo.\nEste análisis es crucial en datos de panel porque ayuda a elegir el modelo que mejor captura la estructura subyacente de los datos, mejorando así la precisión y la interpretación de los resultados.\n\n\n5.2.14 Modelos con Efectos Aleatorios\nEl modelo de efectos aleatorios asume que las diferencias entre las unidades (firmas) se capturan mediante un término de error aleatorio. La forma matemática es:\n\\[invest_{it} = \\beta_{0} + \\beta_{1}value_{it} + \\beta_{2}capital_{it} + u_{i} + \\epsilon_{it}\\]\nDonde:\n\n\\(\\beta_{0}\\) = es el intercepto en comun de las firmas.\n\\(u_{i}\\) es el término de error específico de la firma \\(i\\), que se asume que es un componente aleatorio.\n\\(\\epsilon_{it}\\) es el termino de error.\n\n\n\n5.2.15 Método de Wallace-Hussain\n\n\nCode\n# Ajuste del modelo con efectos aleatorios (Wallace-Hussain)\ngr_re &lt;- plm(invest ~ value + capital, data = pgr, model = \"random\", random.method = \"walhus\")\n# Resumen del modelo con efectos aleatorios (Wallace-Hussain)\nsummary(gr_re)\n\n\nOneway (individual) effect Random Effect Model \n   (Wallace-Hussain's transformation)\n\nCall:\nplm(formula = invest ~ value + capital, data = pgr, model = \"random\", \n    random.method = \"walhus\")\n\nBalanced Panel: n = 3, T = 20, N = 60\n\nEffects:\n                  var std.dev share\nidiosyncratic 4389.31   66.25 0.352\nindividual    8079.74   89.89 0.648\ntheta: 0.8374\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-187.3987  -32.9206    6.9595   31.4322  210.2006 \n\nCoefficients:\n               Estimate  Std. Error z-value  Pr(&gt;|z|)    \n(Intercept) -109.976572   61.701384 -1.7824   0.07468 .  \nvalue          0.104280    0.014996  6.9539 3.553e-12 ***\ncapital        0.344784    0.024520 14.0613 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    1988300\nResidual Sum of Squares: 257520\nR-Squared:      0.87048\nAdj. R-Squared: 0.86594\nChisq: 383.089 on 2 DF, p-value: &lt; 2.22e-16\n\n\n\n\n5.2.16 Método de Amemiya\n\n\nCode\n# Ajuste del modelo con efectos aleatorios (Amemiya)\ngr_re1 &lt;- plm(invest ~ value + capital, data = pgr, model = \"random\", random.method = \"amemiya\")\n# Resumen del modelo con efectos aleatorios (Amemiya)\nsummary(gr_re1)\n\n\nOneway (individual) effect Random Effect Model \n   (Amemiya's transformation)\n\nCall:\nplm(formula = invest ~ value + capital, data = pgr, model = \"random\", \n    random.method = \"amemiya\")\n\nBalanced Panel: n = 3, T = 20, N = 60\n\nEffects:\n                  var std.dev share\nidiosyncratic 4280.43   65.43  0.34\nindividual    8325.12   91.24  0.66\ntheta: 0.8417\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-186.8821  -33.0073    7.2036   31.0795  209.9289 \n\nCoefficients:\n               Estimate  Std. Error z-value  Pr(&gt;|z|)    \n(Intercept) -110.046862   62.944693 -1.7483   0.08041 .  \nvalue          0.104306    0.015042  6.9341 4.088e-12 ***\ncapital        0.344813    0.024491 14.0793 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    1983100\nResidual Sum of Squares: 256810\nR-Squared:      0.8705\nAdj. R-Squared: 0.86595\nChisq: 383.148 on 2 DF, p-value: &lt; 2.22e-16\n\n\n\n\n5.2.17 Método de Nerlove\n\n\nCode\n# Ajuste del modelo con efectos aleatorios (Nerlove)\ngr_re2 &lt;- plm(invest ~ value + capital, data = pgr, model = \"random\", random.method = \"nerlove\")\n# Resumen del modelo con efectos aleatorios (Nerlove)\nsummary(gr_re2)\n\n\nOneway (individual) effect Random Effect Model \n   (Nerlove's transformation)\n\nCall:\nplm(formula = invest ~ value + capital, data = pgr, model = \"random\", \n    random.method = \"nerlove\")\n\nBalanced Panel: n = 3, T = 20, N = 60\n\nEffects:\n                   var  std.dev share\nidiosyncratic  4066.41    63.77 0.241\nindividual    12808.71   113.18 0.759\ntheta: 0.875\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-182.8631  -32.7789    6.4448   29.4453  207.9041 \n\nCoefficients:\n               Estimate  Std. Error z-value  Pr(&gt;|z|)    \n(Intercept) -110.563691   75.685576 -1.4608    0.1441    \nvalue          0.104505    0.015382  6.7938 1.092e-11 ***\ncapital        0.345005    0.024290 14.2038 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    1947600\nResidual Sum of Squares: 251980\nR-Squared:      0.87062\nAdj. R-Squared: 0.86608\nChisq: 383.562 on 2 DF, p-value: &lt; 2.22e-16\n\n\n\n\n\n\n\n\n\n\n\nCaracterística\nWallace-Hussain\nAmemiya\nNerlove\n\n\n\n\nIntercepto\n-109.976572 (p = 0.07468)\n-110.046862 (p = 0.08041)\n-110.563691 (p = 0.1441)\n\n\nCoeficiente de value\n0.104280 (p &lt; 2.22e-16)\n0.104306 (p &lt; 2.22e-16)\n0.104505 (p &lt; 2.22e-16)\n\n\nCoeficiente de capital\n0.344784 (p &lt; 2.22e-16)\n0.344813 (p &lt; 2.22e-16)\n0.345005 (p &lt; 2.22e-16)\n\n\nVarianza idiosincrática\n4389.31\n4280.43\n4066.41\n\n\nDesviación estándar idiosincrática\n66.25\n65.43\n63.77\n\n\nVarianza individual\n8079.74\n8325.12\n12808.71\n\n\nDesviación estándar individual\n89.89\n91.24\n113.18\n\n\nTheta\n0.8374\n0.8417\n0.875\n\n\nMin Residual\n-187.3987\n-186.8821\n-182.8631\n\n\n1er Cuartil Residual\n-32.9206\n-33.0073\n-32.7789\n\n\nMediana Residual\n6.9595\n7.2036\n6.4448\n\n\n3er Cuartil Residual\n31.4322\n31.0795\n29.4453\n\n\nMax Residual\n210.2006\n209.9289\n207.9041\n\n\nR-cuadrado\n0.87048\n0.8705\n0.87062\n\n\nR-cuadrado ajustado\n0.86594\n0.86595\n0.86608\n\n\nChisq\n383.089 (p &lt; 2.22e-16)\n383.148 (p &lt; 2.22e-16)\n383.562 (p &lt; 2.22e-16)\n\n\n\n\n\n5.2.18 Interpretación de la Tabla:\n\nIntercepto: El intercepto varía ligeramente entre los modelos, y todos tienen p-valores que indican que no son significativamente diferentes de cero a niveles tradicionales (0.05), pero son más marginalmente no significativos en Wallace-Hussain y Amemiya.\nCoeficientes de value y capital: Los coeficientes de \\(valeu\\) y \\(capital\\) son muy similares entre los modelos, todos siendo altamente significativos \\((p &lt; 0.001)\\).\nVarianza y Desviación Estándar:\n\nLa varianza y desviación estándar idiosincrática son menores en el modelo de Nerlove.\nLa varianza y desviación estándar individual son significativamente mayores en el modelo de Nerlove, indicando que este modelo captura más variación entre las firmas.\n\nTheta: El valor de theta, que indica la proporción de la variabilidad total explicada por la variabilidad entre unidades individuales, es mayor en el modelo de Nerlove.\nResiduales: Los residuales mínimos, el primer cuartil, la mediana, el tercer cuartil y el máximo son similares entre los modelos, con ligeras variaciones.\nR-cuadrado y R-cuadrado ajustado: Todos los modelos tienen R-cuadrados y R-cuadrados ajustados similares, indicando que explican aproximadamente el 87% de la variabilidad total en la inversión.\nChisq: Todos los modelos tienen valores de Chi-cuadrado altos y p-valores muy pequeños (&lt; 2.22e-16), indicando que los modelos son significativamente diferentes de un modelo nulo.\n\nEsta tabla proporciona una comparación clara entre los tres métodos de estimación de efectos aleatorios, destacando las pequeñas diferencias y similitudes entre ellos.\n\n\n5.2.19 Comparación Teórica\n\nEstimación de Varianzas: Los tres métodos difieren principalmente en cómo estiman las varianzas de los efectos individuales y los errores idiosincráticos.\n\nWallace-Hussain: Minimiza la suma de las varianzas.\nAmemiya: Ofrece flexibilidad y robustez.\nNerlove: Da mayor peso a las varianzas individuales.\n\nTheta: El coeficiente de corrección theta varía según el método, reflejando las diferencias en la estimación de las varianzas.\nAplicabilidad: La elección del método puede depender de las características específicas del conjunto de datos y de los supuestos sobre la estructura de varianza."
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#pruebas-de-hausman-para-comparar-modelos",
    "href": "Proyectos/Modelos de Panel de Datos.html#pruebas-de-hausman-para-comparar-modelos",
    "title": "Panel de Datos",
    "section": "5.3 Pruebas de Hausman para Comparar Modelos",
    "text": "5.3 Pruebas de Hausman para Comparar Modelos\n\\[Ho: Efectos ~ aleatorios ~ es ~ consistente\\]\n\\[Ha: Efectos ~ fijos ~ es ~ consistente\\]\nLa prueba de Hausman se utiliza para determinar si un modelo de efectos aleatorios es consistente comparado con un modelo de efectos fijos.\n\n5.3.1 Procedimiento de la Prueba:\n\nEstimación de los Modelos:\n\nSe estiman ambos modelos, FE y RE, utilizando los datos de panel.\n\nObtención de los Estimadores:\n\nSe calculan los estimadores de los coeficientes bajo ambos modelos.\n\nComparación de los Estimadores:\n\nSe calcula la diferencia entre los estimadores bajo los dos modelos \\((\\hat{\\beta}_{FE}-\\hat{\\beta}_{RE})\\).\n\nEstimación de la Varianza de la Diferencia:\n\nSe estima la varianza de la diferencia considerando las matrices de covarianza de los estimadores bajo ambos modelos.\n\nAplicación de la Estadística de Prueba:\n\nSe utiliza la estadística de prueba de Hausman, que sigue una distribución chi-cuadrado bajo las hipótesis nula y alternativa.\n\n\\[H = (\\hat{\\beta}_{FE} - \\hat{\\beta}_{RE})' [Var(\\hat{\\beta}_{FE}) - Var(\\hat{\\beta}_{RE})]^{-1} (\\hat{\\beta}_{FE} - \\hat{\\beta}_{RE}) \\sim \\chi^2(k)\\]\n\n\\(k\\): Número de coeficientes en el modelo (grados de libertad).\n\nInterpretación del Valor p:\n\nSe evalúa el \\(valor ~ p\\) obtenido de la estadística de prueba. Un valor p pequeño sugiere que existe suficiente evidencia para rechazar la hipótesis nula en favor de la hipótesis alternativa.\n\n\n\n\nCode\n# Prueba de Hausman para el modelo con efectos aleatorios (Wallace-Hussain) vs efectos fijos\nphtest(gr_re, gr_fe)\n\n\n\n    Hausman Test\n\ndata:  invest ~ value + capital\nchisq = 0.04038, df = 2, p-value = 0.98\nalternative hypothesis: one model is inconsistent\n\n\n\n\nCode\n# Prueba de Hausman para el modelo con efectos aleatorios (Amemiya) vs efectos fijos\nphtest(gr_re1, gr_fe)\n\n\n\n    Hausman Test\n\ndata:  invest ~ value + capital\nchisq = 0.044291, df = 2, p-value = 0.9781\nalternative hypothesis: one model is inconsistent\n\n\n\n\nCode\n# Prueba de Hausman para el modelo con efectos aleatorios (Nerlove) vs efectos fijos\nphtest(gr_re2, gr_fe)\n\n\n\n    Hausman Test\n\ndata:  invest ~ value + capital\nchisq = 0.099882, df = 2, p-value = 0.9513\nalternative hypothesis: one model is inconsistent"
  },
  {
    "objectID": "Proyectos/Modelos de Panel de Datos.html#conclusión-1",
    "href": "Proyectos/Modelos de Panel de Datos.html#conclusión-1",
    "title": "Panel de Datos",
    "section": "5.4 Conclusión",
    "text": "5.4 Conclusión\nLos resultados de las pruebas de Hausman indican que los modelos de efectos aleatorios son consistentes, lo que sugiere que es preferible utilizar un modelo de efectos aleatorios en lugar de un modelo de efectos fijos para estos datos."
  },
  {
    "objectID": "Proyectos/Modelos ARCH Y GARCH.html",
    "href": "Proyectos/Modelos ARCH Y GARCH.html",
    "title": "Modelos ARCH y GARCH",
    "section": "",
    "text": "El Modelo Heterocedástico Condicional Autorregresivo (ARCH) es un modelo utilizado en econometría y finanzas para modelar la volatilidad de series temporales. Fue introducido por Robert F. Engle en 1982 y es particularmente útil para capturar la heterocedasticidad condicional en los datos financieros, donde la variabilidad puede cambiar con el tiempo.\n\n\n\nHeterocedasticidad Condicional: La varianza de los errores no es constante a lo largo del tiempo, sino que depende de los errores pasados. En un contexto financiero, esto significa que los periodos de alta volatilidad tienden a ser seguidos por periodos de alta volatilidad, y lo mismo ocurre para los periodos de baja volatilidad.\nAutorregresividad de la Varianza: La varianza condicional de los errores en un momento dado se modela como una función lineal de los errores pasados.\n\n\n\n\nEl modelo ARCH(p) (donde \\(p\\) denota el número de retardos incluidos) se especifica de la siguiente manera:\n\nModelo de Media: Primero, se especifica el modelo para la serie temporal \\(y_t\\)​. Esto puede ser un modelo simple de media o un modelo ARMA, por ejemplo:\n\\(y_{t} = \\mu + \\epsilon_{t}\\)\ndonde \\(\\epsilon_{t}\\) son los errores (residuos) del modelo de media, y \\(\\mu\\) es la media del proceso.\nModelo de Varianza Condicional: La varianza condicional de los errores \\(\\epsilon_{t}\\) se modela como una función de los errores pasados:\n\n\\[\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2} + \\alpha_{2}\\epsilon_{t-2}^{2} + ... + \\alpha_{p}\\epsilon_{t-p}^{2}\\]\ndonde:\n\n\\(\\sigma_{t}^{2}\\) es la varianza condicional en el tiempo \\(t\\).\n\\(\\alpha_{0} &gt; 0\\) es una constante.\n\\(\\alpha_{i} ≥ 0\\) son los coeficientes que miden el impacto de los errores pasados en la varianza condicional.\n\n\nErrores: Los errores \\(\\epsilon_{t}\\) se suponen que son ruido blanco con media cero y varianza condicional \\(\\sigma_{t}^{2}\\) :\n\n\\[\\epsilon_{t} \\sim \\textbf{N}(0, \\sigma_{t}^{2})\\]\n\n\n\nLa estimación del modelo ARCH generalmente implica los siguientes pasos:\n\nEstimar el Modelo de Media: Ajustar el modelo de media (por ejemplo, un modelo ARMA) y obtener los residuos \\(\\epsilon_{t}\\).\nEstimación de la Varianza Condicional: Ajustar el modelo de varianza condicional ARCH(p) a los residuos al cuadrado \\(\\epsilon_{t}^{2}\\) utilizando técnicas de máxima verosimilitud.\nEvaluación del Modelo: Verificar si los residuos del modelo ARCH muestran las características esperadas (por ejemplo, verificar que los residuos estandarizados son ruido blanco).\n\n\n\n\nSupongamos que queremos modelar la volatilidad de los retornos diarios de una acción. Podríamos especificar un modelo ARCH(1) de la siguiente manera:\n\nModelo de Media: \\[y_{t} = \\mu + \\epsilon_{t}\\]\nModelo ARCH(1): \\[\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2}\\]\n\nAquí, la varianza condicional \\(\\sigma_{t}^{2}\\) en el tiempo \\(t\\) depende linealmente del cuadrado del error en el tiempo \\(t-1\\). Si \\(\\alpha_{1}\\) es significativo, indica que hay dependencia en la volatilidad de los retornos.\n\n\n\n\nSimplicidad: Los modelos ARCH pueden ser demasiado simples para capturar todas las características de la volatilidad en series temporales financieras.\nNúmero de Parámetros: A medida que \\(p\\) aumenta, el número de parámetros a estimar también aumenta, lo que puede complicar la estimación y la interpretación.\n\n\n\n\nPara superar algunas de las limitaciones del modelo ARCH, se han desarrollado extensiones como el modelo GARCH (Generalización ARCH), que incluye tanto retardos de los errores como retardos de las varianzas pasadas.\n\n\n\nEl modelo ARCH es una herramienta poderosa para modelar la heterocedasticidad condicional en series temporales, permitiendo capturar la naturaleza cambiante de la volatilidad en los datos financieros. Específicamente, la varianza de los errores se modela como una función de los errores pasados, proporcionando una forma de capturar la agrupación de volatilidad observada en muchas series temporales financieras."
  },
  {
    "objectID": "Proyectos/Modelos ARCH Y GARCH.html#modelo-heterocedástico-condicional-autorregresivo-arch",
    "href": "Proyectos/Modelos ARCH Y GARCH.html#modelo-heterocedástico-condicional-autorregresivo-arch",
    "title": "Modelos ARCH y GARCH",
    "section": "",
    "text": "El Modelo Heterocedástico Condicional Autorregresivo (ARCH) es un modelo utilizado en econometría y finanzas para modelar la volatilidad de series temporales. Fue introducido por Robert F. Engle en 1982 y es particularmente útil para capturar la heterocedasticidad condicional en los datos financieros, donde la variabilidad puede cambiar con el tiempo.\n\n\n\nHeterocedasticidad Condicional: La varianza de los errores no es constante a lo largo del tiempo, sino que depende de los errores pasados. En un contexto financiero, esto significa que los periodos de alta volatilidad tienden a ser seguidos por periodos de alta volatilidad, y lo mismo ocurre para los periodos de baja volatilidad.\nAutorregresividad de la Varianza: La varianza condicional de los errores en un momento dado se modela como una función lineal de los errores pasados.\n\n\n\n\nEl modelo ARCH(p) (donde \\(p\\) denota el número de retardos incluidos) se especifica de la siguiente manera:\n\nModelo de Media: Primero, se especifica el modelo para la serie temporal \\(y_t\\)​. Esto puede ser un modelo simple de media o un modelo ARMA, por ejemplo:\n\\(y_{t} = \\mu + \\epsilon_{t}\\)\ndonde \\(\\epsilon_{t}\\) son los errores (residuos) del modelo de media, y \\(\\mu\\) es la media del proceso.\nModelo de Varianza Condicional: La varianza condicional de los errores \\(\\epsilon_{t}\\) se modela como una función de los errores pasados:\n\n\\[\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2} + \\alpha_{2}\\epsilon_{t-2}^{2} + ... + \\alpha_{p}\\epsilon_{t-p}^{2}\\]\ndonde:\n\n\\(\\sigma_{t}^{2}\\) es la varianza condicional en el tiempo \\(t\\).\n\\(\\alpha_{0} &gt; 0\\) es una constante.\n\\(\\alpha_{i} ≥ 0\\) son los coeficientes que miden el impacto de los errores pasados en la varianza condicional.\n\n\nErrores: Los errores \\(\\epsilon_{t}\\) se suponen que son ruido blanco con media cero y varianza condicional \\(\\sigma_{t}^{2}\\) :\n\n\\[\\epsilon_{t} \\sim \\textbf{N}(0, \\sigma_{t}^{2})\\]\n\n\n\nLa estimación del modelo ARCH generalmente implica los siguientes pasos:\n\nEstimar el Modelo de Media: Ajustar el modelo de media (por ejemplo, un modelo ARMA) y obtener los residuos \\(\\epsilon_{t}\\).\nEstimación de la Varianza Condicional: Ajustar el modelo de varianza condicional ARCH(p) a los residuos al cuadrado \\(\\epsilon_{t}^{2}\\) utilizando técnicas de máxima verosimilitud.\nEvaluación del Modelo: Verificar si los residuos del modelo ARCH muestran las características esperadas (por ejemplo, verificar que los residuos estandarizados son ruido blanco).\n\n\n\n\nSupongamos que queremos modelar la volatilidad de los retornos diarios de una acción. Podríamos especificar un modelo ARCH(1) de la siguiente manera:\n\nModelo de Media: \\[y_{t} = \\mu + \\epsilon_{t}\\]\nModelo ARCH(1): \\[\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2}\\]\n\nAquí, la varianza condicional \\(\\sigma_{t}^{2}\\) en el tiempo \\(t\\) depende linealmente del cuadrado del error en el tiempo \\(t-1\\). Si \\(\\alpha_{1}\\) es significativo, indica que hay dependencia en la volatilidad de los retornos.\n\n\n\n\nSimplicidad: Los modelos ARCH pueden ser demasiado simples para capturar todas las características de la volatilidad en series temporales financieras.\nNúmero de Parámetros: A medida que \\(p\\) aumenta, el número de parámetros a estimar también aumenta, lo que puede complicar la estimación y la interpretación.\n\n\n\n\nPara superar algunas de las limitaciones del modelo ARCH, se han desarrollado extensiones como el modelo GARCH (Generalización ARCH), que incluye tanto retardos de los errores como retardos de las varianzas pasadas.\n\n\n\nEl modelo ARCH es una herramienta poderosa para modelar la heterocedasticidad condicional en series temporales, permitiendo capturar la naturaleza cambiante de la volatilidad en los datos financieros. Específicamente, la varianza de los errores se modela como una función de los errores pasados, proporcionando una forma de capturar la agrupación de volatilidad observada en muchas series temporales financieras."
  },
  {
    "objectID": "Proyectos/Modelos ARCH Y GARCH.html#modelo-heterocedástico-condicional-autorregresivo-generalizado-garch",
    "href": "Proyectos/Modelos ARCH Y GARCH.html#modelo-heterocedástico-condicional-autorregresivo-generalizado-garch",
    "title": "Modelos ARCH y GARCH",
    "section": "2 Modelo Heterocedástico Condicional Autorregresivo Generalizado (GARCH)",
    "text": "2 Modelo Heterocedástico Condicional Autorregresivo Generalizado (GARCH)\nEl Modelo Heterocedástico Condicional Autorregresivo Generalizado (GARCH) es una extensión del modelo ARCH que captura la variabilidad temporal en la volatilidad de una serie temporal. Fue introducido por Tim Bollerslev en 1986. El modelo GARCH es ampliamente utilizado en finanzas y econometría para modelar series temporales financieras, donde la volatilidad puede cambiar con el tiempo.\n\n2.1 Conceptos Clave del Modelo GARCH\n\nHeterocedasticidad Condicional: La varianza de los errores en una serie temporal no es constante a lo largo del tiempo, sino que depende de los errores y varianzas pasadas.\nAutorregresividad en la Varianza: La varianza condicional de los errores depende de los errores pasados y de las varianzas condicionales pasadas, proporcionando una estructura más completa y flexible que el modelo ARCH.\n\n\n\n2.2 Especificación del Modelo GARCH(p, q)\nEl modelo GARCH(p, q) se especifica de la siguiente manera:\n\nModelo de Media: Primero, se especifica el modelo para la serie temporal \\(y_{t}\\):\n\n\\[y_{t} = \\mu + \\epsilon_{t}\\]\ndonde:\n\\(\\epsilon_{t}\\) son los errores (residuos) del modelo de media, y \\(\\mu\\) es la media del proceso.\n\nModelo de Varianza Condicional: La varianza condicional \\(\\sigma_{t}^{2}\\) se modela como una función lineal de los errores pasados y de las varianzas condicionales pasadas:\n\n\\[\\sigma_{t}^{2} = \\alpha_{0} + \\sum_{i=1}^{q} \\alpha_{i}\\epsilon_{t-i}^{2} + \\sum_{j=1}^{p} \\beta_{j}\\sigma_{t-j}^{2}\\]\ndonde:\n\n\\(\\sigma_{t}^{2}\\) es la varianza condicional en el tiempo \\(t\\).\n\\(\\alpha_{0} &gt; 0\\) es una constante.\n\\(\\alpha_{i} ≥ 0\\) son los coeficientes que miden el impacto de los errores pasados al cuadrado.\n\\(\\beta_{j} ≥ 0\\) son los coeficientes que miden el impacto de las varianzas condicionales pasadas.\n\n\n\n2.3 Estimación del Modelo GARCH\nLa estimación del modelo GARCH generalmente implica los siguientes pasos:\n\nEstimar el Modelo de Media: Ajustar el modelo de media (por ejemplo, un modelo ARMA) y obtener los residuos \\(\\epsilon_{t}\\).\nEstimación de la Varianza Condicional: Ajustar el modelo de varianza condicional GARCH(p, q) a los residuos al cuadrado \\(\\epsilon_{t}^{2}\\) utilizando técnicas de máxima verosimilitud.\nEvaluación del Modelo: Verificar si los residuos del modelo GARCH muestran las características esperadas (por ejemplo, verificar que los residuos estandarizados son ruido blanco).\n\n\n\n2.4 Ejemplo de Aplicación\nSupongamos que queremos modelar la volatilidad de los retornos diarios de una acción. Podríamos especificar un modelo GARCH(1, 1) de la siguiente manera:\n\nModelo de Media: \\(y_{t} = \\mu + \\epsilon_{t}\\)\nModelo GARCH(1, 1): \\[\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2} + \\beta_{1}\\sigma_{t-1}^{2}\\]\n\nAquí, la varianza condicional \\(\\sigma_{t}^{2}\\) en el tiempo \\(t\\) depende linealmente del cuadrado del error en el tiempo \\(t−1\\) y de la varianza condicional en el tiempo \\(t−1\\). Este modelo captura tanto la memoria corta (a través de \\(\\epsilon_{t-1}^{2}\\) ) como la memoria larga (a través de \\(\\sigma_{t-1}^{2}\\)) de la volatilidad.\n\n\n2.5 Ventajas del Modelo GARCH\n\nFlexibilidad: Al incluir tanto errores pasados como varianzas pasadas, el modelo GARCH puede capturar de manera más efectiva la dinámica de la volatilidad en los datos financieros.\nAplicabilidad: Los modelos GARCH son aplicables a una amplia variedad de series temporales financieras, incluyendo precios de acciones, tasas de cambio y tasas de interés.\n\n\n\n2.6 Limitaciones del Modelo GARCH\n\nComplejidad: La estimación de modelos GARCH puede ser compleja y computacionalmente intensiva, especialmente para valores altos de \\(p\\) y \\(q\\).\nSuposición de Normalidad: La suposición de que los errores estandarizados son normales puede no ser realista en todos los casos. Existen extensiones como GARCH-T que permiten distribuciones alternativas para los errores.\n\n\n\n2.7 Resumen\nEl modelo GARCH es una herramienta poderosa para modelar la volatilidad condicional en series temporales financieras. Extiende el modelo ARCH al incluir tanto errores pasados como varianzas pasadas, lo que permite capturar la naturaleza persistente de la volatilidad en los datos. Es ampliamente utilizado en econometría y finanzas para prever la volatilidad y gestionar el riesgo.\n\n\n2.8 Pruebas, Estimaciones y Pronósticos\nLa prueba del multiplicador de Lagrange (LM) es una herramienta estadística utilizada para detectar la presencia de efectos ARCH en una serie temporal. Los efectos ARCH se refieren a la situación en la que la varianza de los errores no es constante a lo largo del tiempo, sino que depende de los errores pasados.\nA continuación, se describe el procedimiento para llevar a cabo una prueba LM para efectos ARCH:\n\n\n2.9 Procedimiento de la Prueba LM para Efectos ARCH\n\nEstimar un Modelo de Regresión Básico: Primero, estima el modelo de regresión básico, que podría ser un modelo ARIMA o cualquier otro modelo de serie temporal adecuado. Obtén los residuos de este modelo.\n\n\\[y_{t}= \\beta X_{t} + \\epsilon_{t}\\]\nAquí, \\(y_{t}\\) es la variable dependiente, \\(X_{t}\\) son las variables independientes, \\(\\beta\\) son los coeficientes del modelo, y \\(\\epsilon_{t}\\) son los residuos.\n\nCuadrar los Residuos: Calcula los residuos al cuadrado \\((e_{t}^{2})\\) . Estos valores se usarán para detectar la heterocedasticidad.\nRegresión Auxiliar: Realiza una regresión auxiliar donde los residuos cuadrados se regresan sobre \\(q\\) retardos de los mismos residuos cuadrados.\n\n\\[e_{t}^{2}=\\alpha_{0} + \\alpha_{1}e_{t-1}^{2}+\\alpha_{2}e_{t-2}^{2} + ... + \\alpha_{q}e_{t-q}^{2} + v_{t}\\]\nAquí, \\(\\alpha_{0}\\) es el intercepto, \\(\\alpha_{i}\\) son los coeficientes de los residuos retardados, y \\(v_{t}\\) es el término de error de esta regresión auxiliar.\n\nCalcular el Estadístico de Prueba: El estadístico de prueba LM se calcula como \\(\\textbf{T*R}^{2}\\), donde \\(\\textbf{T}\\)es el número de observaciones y \\(\\textbf{R}^{2}\\) es el coeficiente de determinación de la regresión auxiliar.\n\n\\[\\textbf{LM}=\\textbf{T*R}^{2}\\]\n\nDistribución Asintótica: Bajo la hipótesis nula de que no hay efectos ARCH (es decir, todos los \\(\\alpha_{i}=0\\), el estadístico LM sigue una distribución chi-cuadrado \\(\\textbf{X}^{2}\\) con \\(q\\) grados de libertad.\nDecisión: Compara el estadístico LM con el valor crítico de la distribución \\(\\textbf{X}^{2}\\) con \\(q\\) grados de libertad. Si el estadístico LM es mayor que el valor crítico, se rechaza la hipótesis nula, indicando la presencia de efectos ARCH. De lo contrario, no se rechaza la hipótesis nula.\n\n\n\n2.10 Resumen\nLa prueba del multiplicador de Lagrange (LM) para efectos ARCH sigue estos pasos principales:\n\nEstimar el modelo básico y obtener los residuos.\nCalcular los residuos al cuadrado.\nRealizar una regresión auxiliar de los residuos cuadrados sobre \\(q\\) retardos de los mismos.\nCalcular el estadístico \\(\\textbf{T*R}^{2}\\).\nComparar el estadístico con la distribución chi-cuadrado para tomar la decisión.\n\nEsta prueba es una herramienta efectiva para detectar heterocedasticidad condicional en las series temporales, y es ampliamente utilizada en econometría y finanzas para modelar y prever la volatilidad."
  },
  {
    "objectID": "Proyectos/Modelos ARCH Y GARCH.html#que-son-los-errores-estandarizados",
    "href": "Proyectos/Modelos ARCH Y GARCH.html#que-son-los-errores-estandarizados",
    "title": "Modelos ARCH y GARCH",
    "section": "3 ¿Que son los Errores Estandarizados?",
    "text": "3 ¿Que son los Errores Estandarizados?\nLos errores estandarizados son una forma de normalizar los residuos de un modelo para hacerlos comparables a lo largo del tiempo y evaluar su comportamiento de una manera más uniforme. En el contexto de modelos de heterocedasticidad condicional como GARCH, los errores estandarizados son particularmente útiles para verificar la adecuación del modelo y la presencia de heterocedasticidad residual.\n\n3.1 Definición de Errores Estandarizados\nPara una serie temporal \\(y_{t}\\) modelada con un modelo de media y un modelo de varianza condicional (como un GARCH), los errores estandarizados se calculan de la siguiente manera:\n\nErrores del Modelo \\((\\epsilon_{t})\\): Estos son los residuos del modelo, que se obtienen como la diferencia entre los valores observados y los valores ajustados por el modelo de media: \\(\\epsilon_{t} = y_{t} -  \\hat y_{t}\\) donde \\(\\hat y_{t}\\) es el valor ajustado.\nVarianza Condicional \\((\\sigma_{t}^{2})\\): Esta es la varianza condicional estimada en cada punto en el tiempo, que se obtiene del modelo de varianza condicional (por ejemplo, GARCH): \\((\\sigma_{t}^{2})\\)\nErrores Estandarizados \\((e_{t})\\) : Los errores estandarizados se obtienen dividiendo los errores del modelo por la raíz cuadrada de la varianza condicional: \\(e_{t} = \\frac{\\epsilon_{t}} {\\sigma_{t}}\\) donde \\(\\sigma_{t} = \\sqrt{\\sigma_{t}^{2}}\\) es la desviación estándar condicional.\n\n\n\n3.2 Interpretación y Uso\nLos errores estandarizados \\(e_{t}\\) tienen una media esperada de cero y una varianza esperada de uno, siempre y cuando el modelo esté correctamente especificado. Esto permite comparar los errores a lo largo del tiempo y verificar si hay patrones que indiquen una mala especificación del modelo.\n\n\n3.3 Evaluación del Modelo con Errores Estandarizados\n\nAnálisis de Ruido Blanco: Los errores estandarizados deben comportarse como ruido blanco, es decir, deben ser independientes e idénticamente distribuidos con media cero y varianza uno. Si no se comportan de esta manera, podría indicar que el modelo no ha capturado adecuadamente la dinámica de la serie temporal.\nGráficos de Errores Estandarizados: Los gráficos de los errores estandarizados pueden ayudar a identificar patrones o anomalías en los residuos que no fueron capturados por el modelo. Por ejemplo, si los errores estandarizados muestran agrupaciones de valores altos o bajos, puede ser un signo de que hay heterocedasticidad residual no modelada.\nPruebas Estadísticas: Pruebas estadísticas, como la prueba de Ljung-Box, pueden ser aplicadas a los errores estandarizados para verificar la presencia de autocorrelación. También se pueden usar pruebas para la normalidad de los errores estandarizados, como la prueba de Jarque-Bera.\n\n\n\n3.4 Ejemplo Práctico\nSupongamos que hemos ajustado un modelo GARCH(1,1) a una serie temporal de retornos financieros:\n\nEstimación del Modelo: Ajustamos el modelo y obtenemos los residuos \\(\\epsilon_{t}\\).\nCálculo de la Varianza Condicional: Estimamos la varianza condicional \\(\\sigma_{t}^{2}\\) utilizando los parámetros del modelo GARCH.\nCálculo de los Errores Estandarizados: Dividimos cada residuo \\(\\epsilon_{t}\\) por la desviación estándar condicional \\(\\sigma_{t}\\): \\(e_{t} = \\frac{\\epsilon_{t}} {\\sigma_{t}}\\)\nEvaluación: Analizamos los errores estandarizados \\(e_{t}\\) para verificar si son ruido blanco y si siguen una distribución normal.\n\n\n\n3.5 Resumen\nLos errores estandarizados son una herramienta clave para evaluar la adecuación de los modelos de series temporales con heterocedasticidad condicional. Al normalizar los errores, permiten una evaluación uniforme de los residuos a lo largo del tiempo y ayudan a identificar posibles problemas de especificación del modelo."
  },
  {
    "objectID": "Proyectos/Modelos ARCH Y GARCH.html#identificación",
    "href": "Proyectos/Modelos ARCH Y GARCH.html#identificación",
    "title": "Modelos ARCH y GARCH",
    "section": "4 Identificación",
    "text": "4 Identificación\nLa mejor herramienta de identificación puede ser un gráfico de la serie de tiempo. Por lo general, es fácil detectar periodos de mayor variación esparcidos a lo largo de la serie.\nPuede resultar útil examinar el ACF y el PACF de \\(\\epsilon_{t}\\) y \\(\\epsilon_{t}^{2}\\). Por ejemplo,\n\nsi \\(\\epsilon_{t}\\) parece ser ruido blanco y parece ser \\(𝐴𝑅(1)\\), se sugiere un modelo \\(ARCH(1)\\) para la varianza.\nSi el PACF de sugiere \\(AR(p)\\), entonces \\(ARCH(m)\\) puede funcionar.\n\nLos modelos \\(GARCH\\) pueden ser sugeridos por una estructura de tipo ARMA en el ACF y PACF de \\(\\epsilon_{t}^{2}\\).\nEn la práctica, es posible que tengas que experimentar con varias estructuras ARCH y GARCH después de detectar que es necesario aborar el problema con este tipo de modelos."
  },
  {
    "objectID": "Proyectos/Modelos ARCH Y GARCH.html#ejemplo-analizar-el-índice-sp-500-desde-2014-05-03-a-2021-04-22-y-estimar-un-modelo-para-su-volatilidad",
    "href": "Proyectos/Modelos ARCH Y GARCH.html#ejemplo-analizar-el-índice-sp-500-desde-2014-05-03-a-2021-04-22-y-estimar-un-modelo-para-su-volatilidad",
    "title": "Modelos ARCH y GARCH",
    "section": "5 Ejemplo: Analizar el índice S&P 500 desde 2014-05-03 a 2021-04-22 y estimar un modelo para su volatilidad",
    "text": "5 Ejemplo: Analizar el índice S&P 500 desde 2014-05-03 a 2021-04-22 y estimar un modelo para su volatilidad\n\n5.1 Paso 1: Cargar y Preparar los Datos\nPrimero, cargaremos los datos y los convertiremos en una serie temporal.\n\n\nCode\nSIP &lt;- read_excel(\"PerformanceGraphExport.xlsx\")\nView(SIP)\n\n# Crear un vector de fechas desde el 30 de mayo de 2014 hasta el 3 de junio de 2024\nfechas &lt;- seq(as.Date(\"2014-05-30\"), as.Date(\"2024-06-03\"), by = \"day\")\n\n\n# Verificar y ajustar longitud de fechas si es necesario\nif (length(fechas) &gt; nrow(SIP)) {\n  fechas &lt;- fechas[1:nrow(SIP)]\n} else if (length(fechas) &lt; nrow(SIP)) {\n  stop(\"Número de filas en datos es mayor que el número de fechas generadas\")\n}\n\n# Crear una serie temporal diaria con los datos de SIP$'S&P 500' y las fechas definidas\nSIP500 &lt;- xts(SIP$`S&P 500`, order.by = fechas)\n# Puedes ajustar la frecuencia de acuerdo a tus necesidades, en este caso se utilizó 365 para representar una frecuencia diaria.\nplot(SIP500)\n\n\n\n\n\n\n\n\n\n\n\n5.2 Paso 2: Calcular Retornos Mensuales\nCalcularemos los retornos mensuales a partir de los precios.\n\n\nCode\n# Convertir la serie diaria a una serie mensual calculando el último valor de cada mes\n\nmonthly_prices &lt;- apply.monthly(SIP500, last)\n\n# Calcular los retornos mensuales\nrets &lt;- monthlyReturn(monthly_prices)\n\n# Verificar los datos de retornos mensuales\nhead(rets)\n\n\n           monthly.returns\n2014-05-31    0.0000000000\n2014-06-30    0.0250964950\n2014-07-31    0.0136017190\n2014-08-31   -0.0469921805\n2014-09-30    0.0825599513\n2014-10-31   -0.0006590744\n\n\nCode\nsummary(rets)\n\n\n     Index            monthly.returns    \n Min.   :2014-05-31   Min.   :-0.308805  \n 1st Qu.:2016-02-21   1st Qu.:-0.006655  \n Median :2017-11-15   Median : 0.020211  \n Mean   :2017-11-14   Mean   : 0.014256  \n 3rd Qu.:2019-08-07   3rd Qu.: 0.047243  \n Max.   :2021-04-22   Max.   : 0.233336  \n\n\n\n\n5.3 Paso 3: Visualizar los Retornos\nPara entender mejor los datos, visualizaremos los retornos mensuales.\n\n\nCode\n# Convertir los retornos a un marco de datos adecuado para ggplot\nrets_df &lt;- data.frame(Date = index(rets), Returns = coredata(rets))\n\n# Verificar el marco de datos\nhead(rets_df)\n\n\n        Date monthly.returns\n1 2014-05-31    0.0000000000\n2 2014-06-30    0.0250964950\n3 2014-07-31    0.0136017190\n4 2014-08-31   -0.0469921805\n5 2014-09-30    0.0825599513\n6 2014-10-31   -0.0006590744\n\n\nCode\n# Convertir la columna Date a formato de fecha si no está en ese formato\nrets_df$Date &lt;- as.Date(rets_df$Date)\n\n# Crear el gráfico de retornos mensuales\nggplot(rets_df, aes(x = Date, y = monthly.returns)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Retornos Mensuales\",\n       x = \"Fecha\",\n       y = \"Retornos Mensuales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n5.4 Paso 4: Ajustar el Modelo GARCH\nProcedemos con el ajuste del modelo GARCH y las visualizaciones como antes:\n\n\nCode\ny &lt;- rets_df$monthly.returns\n\n# Ajustar un modelo GARCH(1,1)\nfit_1 &lt;- ugarchfit(data = y, spec = ugarchspec(mean.model = list(armaOrder = c(0,0)), \n                                               variance.model = list(model = \"sGARCH\")), \n                 solver = \"hybrid\")\n\n\nWarning in .sgarchfit(spec = spec, data = data, out.sample = out.sample, : \nugarchfit--&gt;waring: using less than 100 data\n points for estimation\n\n\nCode\n# Mostrar el resumen del modelo ajustado\nprint(fit_1)\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(0,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.014125    0.006919   2.0414 0.041215\nomega   0.000079    0.000017   4.6196 0.000004\nalpha1  0.000000    0.003955   0.0000 1.000000\nbeta1   0.983853    0.023985  41.0188 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.014125    0.004375  3.22861 0.001244\nomega   0.000079    0.000163  0.48643 0.626661\nalpha1  0.000000    0.001294  0.00000 1.000000\nbeta1   0.983853    0.031689 31.04670 0.000000\n\nLogLikelihood : 112.2659 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -2.5778\nBayes        -2.4620\nShibata      -2.5820\nHannan-Quinn -2.5312\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic  p-value\nLag[1]                      10.14 0.001454\nLag[2*(p+q)+(p+q)-1][2]     10.21 0.001685\nLag[4*(p+q)+(p+q)-1][5]     12.14 0.002555\nd.o.f=0\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic  p-value\nLag[1]                      10.18 0.001421\nLag[2*(p+q)+(p+q)-1][5]     10.39 0.007297\nLag[4*(p+q)+(p+q)-1][9]     10.80 0.033575\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.0917 0.500 2.000  0.7620\nARCH Lag[5]    0.3969 1.440 1.667  0.9135\nARCH Lag[7]    0.6134 2.315 1.543  0.9670\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  2.6282\nIndividual Statistics:              \nmu     0.04212\nomega  0.10825\nalpha1 0.08573\nbeta1  0.10882\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.07 1.24 1.6\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value     prob sig\nSign Bias           1.4936 0.139268    \nNegative Sign Bias  3.0756 0.002885 ***\nPositive Sign Bias  0.2836 0.777488    \nJoint Effect        9.5402 0.022907  **\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     27.90      0.08528\n2    30     41.00      0.06887\n3    40     53.14      0.06499\n4    50     62.43      0.09421\n\n\nElapsed time : 0.2704809 \n\n\nCode\n# Especificar el modelo GARCH(1,1)\nspec &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n  mean.model = list(armaOrder = c(0, 0))\n)\n\nsummary(spec)\n\n\n    Length      Class       Mode \n         1 uGARCHspec         S4 \n\n\nCode\n# Ajustar el modelo GARCH a los datos de retornos\nfit &lt;- ugarchfit(spec = spec, data = rets)\n\n\nWarning in .sgarchfit(spec = spec, data = data, out.sample = out.sample, : \nugarchfit--&gt;waring: using less than 100 data\n points for estimation\n\n\nCode\n# Resumen del modelo ajustado\nsummary(fit)\n\n\n   Length     Class      Mode \n        1 uGARCHfit        S4 \n\n\nCode\nprint(fit)\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(0,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.014125    0.006919   2.0414 0.041215\nomega   0.000079    0.000017   4.6196 0.000004\nalpha1  0.000000    0.003955   0.0000 1.000000\nbeta1   0.983853    0.023985  41.0188 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.014125    0.004375  3.22861 0.001244\nomega   0.000079    0.000163  0.48643 0.626661\nalpha1  0.000000    0.001294  0.00000 1.000000\nbeta1   0.983853    0.031689 31.04670 0.000000\n\nLogLikelihood : 112.2659 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -2.5778\nBayes        -2.4620\nShibata      -2.5820\nHannan-Quinn -2.5312\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic  p-value\nLag[1]                      10.14 0.001454\nLag[2*(p+q)+(p+q)-1][2]     10.21 0.001685\nLag[4*(p+q)+(p+q)-1][5]     12.14 0.002555\nd.o.f=0\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic  p-value\nLag[1]                      10.18 0.001421\nLag[2*(p+q)+(p+q)-1][5]     10.39 0.007297\nLag[4*(p+q)+(p+q)-1][9]     10.80 0.033575\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.0917 0.500 2.000  0.7620\nARCH Lag[5]    0.3969 1.440 1.667  0.9135\nARCH Lag[7]    0.6134 2.315 1.543  0.9670\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  2.6282\nIndividual Statistics:              \nmu     0.04212\nomega  0.10825\nalpha1 0.08573\nbeta1  0.10882\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.07 1.24 1.6\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value     prob sig\nSign Bias           1.4936 0.139268    \nNegative Sign Bias  3.0756 0.002885 ***\nPositive Sign Bias  0.2836 0.777488    \nJoint Effect        9.5402 0.022907  **\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     27.90      0.08528\n2    30     41.00      0.06887\n3    40     53.14      0.06499\n4    50     62.43      0.09421\n\n\nElapsed time : 0.156966 \n\n\n\n\n5.5 Paso 4: Evaluar el Modelo\nRevisamos el ajuste del modelo y realizamos algunas pruebas diagnósticas.\n\n\nCode\n# Obtener los residuos estandarizados\nresiduals_standardized &lt;- residuals(fit, standardize = TRUE)\n\n# Graficar los residuos estandarizados\nggplot(data.frame(Date = index(rets), Residuals = residuals_standardized), aes(x = Date, y = Residuals)) +\n  geom_line() +\n  labs(title = \"Residuos Estandarizados del Modelo GARCH\",\n       x = \"Fecha\",\n       y = \"Residuos Estandarizados\")\n\n\n\n\n\n\n\n\n\n\n\n5.6 Función de Autocorrelación de los Residuos\nRevisar la autocorrelación de los residuos y de los residuos al cuadrado es importante para asegurarse de que no haya autocorrelación residual:\n\n\nCode\n# Función de autocorrelación de los residuos estandarizados\nacf(residuals_standardized, main = \"ACF de los Residuos Estandarizados\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Función de autocorrelación de los residuos al cuadrado\nacf(residuals_standardized^2, main = \"ACF de los Residuos al Cuadrado\")"
  },
  {
    "objectID": "Proyectos/Modelos ARCH Y GARCH.html#preguntas",
    "href": "Proyectos/Modelos ARCH Y GARCH.html#preguntas",
    "title": "Modelos ARCH y GARCH",
    "section": "6 Preguntas",
    "text": "6 Preguntas\n\n6.1 Pregunta 1\n¿Cuál de las siguientes afirmaciones describe mejor la característica principal de un modelo ARCH(p)?\nA) La media de los errores es constante a lo largo del tiempo.\nB) La varianza condicional de los errores depende de las varianzas condicionales pasadas.\nC) La varianza condicional de los errores depende de los errores pasados.\nD) La media condicional de los errores depende de los errores pasados.\nRespuesta Correcta: C\n\n\n6.2 Pregunta 2\nEn un modelo GARCH(1,1), la varianza condicional \\(\\sigma_t^2\\) se especifica como:\nA) \\(\\sigma_t^2 = \\alpha_0 + \\alpha_1 \\epsilon_{t-1}^2\\)\nB) \\(\\sigma_t^2 = \\alpha_0 + \\alpha_1 \\sigma_{t-1}^2 + \\beta_1 \\epsilon_t^2+ ϵt^{2}\\)\nC) \\(\\sigma_t^2 = \\alpha_0 + \\alpha_1 \\epsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2\\)\nD) \\(\\sigma_t^2 = \\alpha_0 + \\beta_1 \\epsilon_t^2 + \\alpha_1 \\sigma_{t-1}^2\\)\nRespuesta Correcta: C\n\n\n6.3 Pregunta 3\n¿Cuál es el propósito principal de la prueba del multiplicador de Lagrange (LM) en el contexto de un modelo ARCH?\nA) Detectar autocorrelación en los residuos.\nB) Verificar la presencia de heterocedasticidad condicional.\nC) Estimar los coeficientes del modelo.\nD) Evaluar la normalidad de los errores estandarizados.\nRespuesta Correcta: B\n\n\n6.4 Pregunta 4\nEn un modelo ARCH(p), si los coeficientes \\(\\alpha_1, \\alpha_2, \\ldots, \\alpha_p\\) son todos positivos y su suma es mayor que 1, ¿qué implica esto sobre el comportamiento de la varianza condicional?\nA) La varianza condicional será constante.\nB) La varianza condicional tenderá a cero.\nC) La varianza condicional será explosiva y no se estabilizará.\nD) La varianza condicional seguirá una tendencia lineal.\nRespuesta Correcta: C\n\n\n6.5 Pregunta 5\nSupongamos que estás utilizando un modelo GARCH(1,1) para modelar la volatilidad de los retornos diarios de una acción. ¿Cuál de las siguientes opciones describe mejor cómo se calcula la varianza condicional en este modelo?\nA) Utilizando únicamente los errores al cuadrado de los períodos anteriores.\nB) Utilizando tanto los errores al cuadrado de los períodos anteriores como las varianzas condicionales de los períodos anteriores.\nC) Utilizando únicamente las varianzas condicionales de los períodos anteriores.\nD) Utilizando una media constante más los errores de los períodos anteriores.\nRespuesta Correcta: B\n\n\n6.6 Pregunta 6\n¿Cuál de las siguientes afirmaciones es verdadera acerca de los residuos estandarizados en un modelo GARCH?\nA) Los residuos estandarizados deben tener una media diferente de cero.\nB) Los residuos estandarizados deben tener una varianza diferente de uno.\nC) Los residuos estandarizados deben comportarse como ruido blanco.\nD) Los residuos estandarizados deben mostrar heterocedasticidad condicional.\nRespuesta Correcta: C\n\n\n6.7 Pregunta 7\nEn un modelo GARCH(1,1), si se observa que el coeficiente \\(\\alpha_1\\) es significativamente mayor que el coeficiente \\(\\beta_1\\), ¿qué implicación tiene esto sobre la naturaleza de la volatilidad?\nA) La volatilidad actual depende más de la volatilidad pasada que de los choques recientes.\nB) Los choques recientes tienen un mayor impacto en la volatilidad actual que la volatilidad pasada.\nC) La varianza condicional es constante a lo largo del tiempo.\nD) No hay impacto de los errores pasados en la volatilidad actual.\nRespuesta Correcta: B\n\n\n6.8 Pregunta 8\n¿Qué indica un estadístico de prueba LM alto en una prueba para detectar efectos ARCH?\nA) No hay efectos ARCH presentes en los datos.\nB) Los residuos son ruido blanco.\nC) Hay evidencia significativa de heterocedasticidad condicional.\nD) Los residuos siguen una distribución normal.\nRespuesta Correcta: C\n\n\n6.9 Pregunta 9\n¿Cuál de los siguientes modelos es una extensión del modelo ARCH que incluye tanto errores pasados como varianzas condicionales pasadas en la especificación de la varianza condicional?\nA) ARIMA\nB) ARCH-M\nC) GARCH\nD) EGARCH\nRespuesta Correcta: C\n\n\n6.10 Pregunta 10\nEn un modelo ARCH, ¿qué representa el término \\(\\epsilon_{t-1}^2\\) en la especificación de la varianza condicional?\nA) La media de los errores pasados.\nB) La varianza condicional de los errores pasados.\nC) El cuadrado del error en el período anterior.\nD) La desviación estándar condicional de los errores pasados.\nRespuesta Correcta: C\n\n\n6.11 Pregunta 11\n¿Cuál de las siguientes afirmaciones describe mejor la característica principal de un modelo ARCH(p)?\nRespuesta Correcta: C) La varianza condicional de los errores depende de los errores pasados.\nJustificación: La característica principal del modelo ARCH (AutoRegressive Conditional Heteroskedasticity) es que la varianza condicional de los errores en el tiempo ttt depende de los errores pasados al cuadrado. Esto captura la heterocedasticidad condicional, donde la volatilidad puede cambiar con el tiempo en función de los errores pasados.\n\n\n6.12 Pregunta 2\nEn un modelo GARCH(1,1), la varianza condicional \\(\\sigma_t^2\\) se especifica como:\nRespuesta Correcta: C) \\(\\sigma_t^2 = \\alpha_0 + \\alpha_1 \\epsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2\\)\nJustificación: En el modelo GARCH(1,1), la varianza condicional actual \\(\\sigma_t^2\\) es una función lineal tanto del error pasado al cuadrado \\(\\epsilon_{t-1}^2\\) como de la varianza condicional pasada \\(\\sigma_{t-1}^2\\). Esta especificación permite que la varianza dependa tanto de los choques recientes como de la persistencia de la volatilidad pasada.\n\n\n6.13 Pregunta 3\n¿Cuál es el propósito principal de la prueba del multiplicador de Lagrange (LM) en el contexto de un modelo ARCH?\nRespuesta Correcta: B) Verificar la presencia de heterocedasticidad condicional.\nJustificación: La prueba LM se utiliza para detectar la presencia de efectos ARCH en una serie temporal. Específicamente, se prueba si la varianza de los errores depende de los errores pasados, lo que indicaría heterocedasticidad condicional.\n\n\n6.14 Pregunta 4\nEn un modelo ARCH(p), si los coeficientes \\(\\alpha_1, \\alpha_2, \\ldots, \\alpha_p\\) son todos positivos y su suma es mayor que 1, ¿qué implica esto sobre el comportamiento de la varianza condicional?\nRespuesta Correcta: C) La varianza condicional será explosiva y no se estabilizará.\nJustificación: Si la suma de los coeficientes \\(\\alpha_i\\) en un modelo ARCH(p) es mayor que 1, la varianza condicional crecerá sin límite a lo largo del tiempo, indicando que el proceso es inestable y la varianza no se estabiliza.\n\n\n6.15 Pregunta 5\nSupongamos que estás utilizando un modelo GARCH(1,1) para modelar la volatilidad de los retornos diarios de una acción. ¿Cuál de las siguientes opciones describe mejor cómo se calcula la varianza condicional en este modelo?\nRespuesta Correcta: B) Utilizando tanto los errores al cuadrado de los períodos anteriores como las varianzas condicionales de los períodos anteriores.\nJustificación: En un modelo GARCH(1,1), la varianza condicional se calcula utilizando tanto el error al cuadrado del período anterior como la varianza condicional del período anterior. Esta especificación incluye la memoria de los choques recientes y la persistencia de la volatilidad.\n\n\n6.16 Pregunta 6\n¿Cuál de las siguientes afirmaciones es verdadera acerca de los residuos estandarizados en un modelo GARCH?\nRespuesta Correcta: C) Los residuos estandarizados deben comportarse como ruido blanco.\nJustificación: Los residuos estandarizados en un modelo GARCH deben tener una media esperada de cero y una varianza de uno, comportándose como ruido blanco. Esto implica que, una vez modelada la varianza condicional, no debería haber autocorrelación en los residuos estandarizados.\n\n\n6.17 Pregunta 7\nEn un modelo GARCH(1,1), si se observa que el coeficiente \\(\\alpha_1\\) es significativamente mayor que el coeficiente \\(\\beta_1\\), ¿qué implicación tiene esto sobre la naturaleza de la volatilidad?\nRespuesta Correcta: B) Los choques recientes tienen un mayor impacto en la volatilidad actual que la volatilidad pasada.\nJustificación: Un coeficiente \\(\\alpha_1\\) mayor que \\(\\beta_1\\) indica que los errores pasados (choques recientes) tienen un mayor impacto en la varianza condicional actual que la persistencia de la varianza condicional pasada.\n\n\n6.18 Pregunta 8\n¿Qué indica un estadístico de prueba LM alto en una prueba para detectar efectos ARCH?\nRespuesta Correcta: C) Hay evidencia significativa de heterocedasticidad condicional.\nJustificación: Un estadístico LM alto en una prueba para efectos ARCH sugiere que hay una fuerte evidencia de que la varianza de los errores no es constante y depende de los errores pasados, indicando heterocedasticidad condicional.\n\n\n6.19 Pregunta 9\n¿Cuál de los siguientes modelos es una extensión del modelo ARCH que incluye tanto errores pasados como varianzas condicionales pasadas en la especificación de la varianza condicional?\nRespuesta Correcta: C) GARCH\nJustificación: El modelo GARCH (Generalized ARCH) extiende el modelo ARCH al incluir no solo los errores pasados, sino también las varianzas condicionales pasadas en la especificación de la varianza condicional, proporcionando una estructura más flexible y completa.\n\n\n6.20 Pregunta 10\nEn un modelo ARCH, ¿qué representa el término \\(\\epsilon_{t-1}^2\\) en la especificación de la varianza condicional?\nRespuesta Correcta: C) El cuadrado del error en el período anterior.\nJustificación: En la especificación de un modelo ARCH, \\(\\epsilon_{t-1}^2\\) representa el error al cuadrado del período anterior. Este término captura el impacto de los choques pasados en la varianza condicional actual."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "",
    "text": "Un modelo ARIMA es un tipo de modelo estadístico utilizado para analizar y predecir series temporales. Este modelo combina componentes de Autorregresión (AR), Integración (I) y Media Móvil (MA) para capturar las estructuras y patrones presentes en los datos temporales.\n\nDefinición de ARIMA (Box-Jenkins): “Los modelos ARIMA son una clase de modelos que pueden representar series de tiempo estacionarias y no estacionarias. Un modelo ARIMA es una modificación de un modelo ARMA que permite que la serie subyacente sea no estacionaria. La letra ‘I’ en ARIMA significa ‘integrado’, lo cual es una referencia a la operación de diferenciación.”\n— Pindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.).\n\nLa forma general de un modelo ARIMA(p,d,q) se puede expresar como:\n\\[(1 - \\sum_{i=1}^{p} \\phi_i L^i)(1-L)^d Y_t = c + (1 + \\sum_{j=1}^{q} \\theta_j L^j)\\epsilon_t\\]\nDonde:\n\n\\(Y_t\\) es el valor observado en el tiempo t.\n\\(p\\) es el orden del componente autorregresivo (AR).\n\\(d\\) es el número de diferencias necesarias para alcanzar la estacionariedad.\n\\(q\\) es el orden del componente de media móvil (MA).\n\\(\\phi_i\\) son los parámetros autorregivos.\n\\(\\theta_j\\) son los parámetros de media móvil.\n\\(\\epsilon_t\\) es un término de error de ruido blanco, \\(\\epsilon_t \\sim iid N(0, \\sigma^2_\\epsilon)\\).\n\nA continuación, se enumeran los pasos de la metodología Box-Jenkins para la construcción de un modelo ARIMA."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#introducción-el-modelo-arima",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#introducción-el-modelo-arima",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "",
    "text": "Un modelo ARIMA es un tipo de modelo estadístico utilizado para analizar y predecir series temporales. Este modelo combina componentes de Autorregresión (AR), Integración (I) y Media Móvil (MA) para capturar las estructuras y patrones presentes en los datos temporales.\n\nDefinición de ARIMA (Box-Jenkins): “Los modelos ARIMA son una clase de modelos que pueden representar series de tiempo estacionarias y no estacionarias. Un modelo ARIMA es una modificación de un modelo ARMA que permite que la serie subyacente sea no estacionaria. La letra ‘I’ en ARIMA significa ‘integrado’, lo cual es una referencia a la operación de diferenciación.”\n— Pindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.).\n\nLa forma general de un modelo ARIMA(p,d,q) se puede expresar como:\n\\[(1 - \\sum_{i=1}^{p} \\phi_i L^i)(1-L)^d Y_t = c + (1 + \\sum_{j=1}^{q} \\theta_j L^j)\\epsilon_t\\]\nDonde:\n\n\\(Y_t\\) es el valor observado en el tiempo t.\n\\(p\\) es el orden del componente autorregresivo (AR).\n\\(d\\) es el número de diferencias necesarias para alcanzar la estacionariedad.\n\\(q\\) es el orden del componente de media móvil (MA).\n\\(\\phi_i\\) son los parámetros autorregivos.\n\\(\\theta_j\\) son los parámetros de media móvil.\n\\(\\epsilon_t\\) es un término de error de ruido blanco, \\(\\epsilon_t \\sim iid N(0, \\sigma^2_\\epsilon)\\).\n\nA continuación, se enumeran los pasos de la metodología Box-Jenkins para la construcción de un modelo ARIMA."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#familiarizarse-con-la-serie",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#familiarizarse-con-la-serie",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "2 Familiarizarse con la Serie",
    "text": "2 Familiarizarse con la Serie\nAntes de cualquier análisis técnico, es crucial entender el contexto de los datos. En este caso, analizaremos la serie anual del Producto Interno Bruto (PIB) de Argentina.\n\n2.1 Carga y Preparación de los Datos\nEn lugar de cargar un archivo externo, definimos los datos directamente en el documento para asegurar la reproducibilidad del análisis.\n\n\nCode\n# Crear un tibble (data frame) con los datos del PIB de Argentina\npib_argentina_raw &lt;- tibble(\n  Periodo = 1960:2021, # El rango de años es hasta 2021 para coincidir con los 62 puntos de datos.\n  `PIB Argentina` = c(150797810295.884, 158982878499.524, 157628310158.567, 149261089202.812, \n                      164381681831.918, 181755894112.433, 180556802912.311, 186320169964.576, \n                      195305461613.156, 214210104570.482, 220734180309.322, 233223609573.669, \n                      237021461356.448, 243685921868.226, 257171024124.452, 257097956426.363, \n                      251909073021.038, 269376023758.644, 260383783021.038, 279583538643.552, \n                      282476135644.759, 263436798302.104, 260952386435.524, 269047613564.476, \n                      275644759426.363, 260952386435.524, 278631151978.962, 286311519789.623, \n                      285023864355.241, 267390476135.645, 264388480210.377, 277341803093.223, \n                      304263625611.519, 316374388480.21, 333201697896.226, 321500475942.636, \n                      339047613564.476, 365611519789.623, 379523864355.241, 365285386435.524, \n                      362561151978.962, 353260952386.436, 383418030932.226, 419523864355.241, \n                      461047613564.476, 503418030932.226, 548302103773.904, 594263625611.519, \n                      632103773904.761, 601523864355.241, 659047613564.476, 658302103773.904, \n                      643884802103.774, 657390476135.645, 642561151978.962, 657047613564.476, \n                      640952386435.524, 625285386435.524, 592636256115.198, 578302103773.904, \n                      641302103773.904, 622047613564.476)\n)\n\n# Convertir los datos a un objeto de serie de tiempo (ts)\npib_ts &lt;- ts(pib_argentina_raw$`PIB Argentina`, \n             start = c(1960, 1), \n             frequency = 1)\n\n\n\n\n2.2 Análisis Gráfico Inicial\nGraficamos la serie para observar su comportamiento a lo largo del tiempo.\n\n\nCode\n# Graficar la serie de tiempo\nts.plot(pib_ts, \n        main = \"PIB de Argentina (1960-2021)\", # Título del gráfico\n        ylab = \"PIB (en unidades monetarias)\",\n        col = \"#0072B2\",\n        lwd = 2)\ngrid()\n\n\n\n\n\nPIB de Argentina (1960-2021).\n\n\n\n\nObservación Inicial: El gráfico muestra una clara tendencia ascendente a lo largo del tiempo, con algunas fluctuaciones. Una serie con una tendencia tan marcada es un indicio visual de que no es estacionaria."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#análisis-de-estacionariedad",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#análisis-de-estacionariedad",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "3 Análisis de Estacionariedad",
    "text": "3 Análisis de Estacionariedad\nEste es el primer paso técnico fundamental. Debemos verificar formalmente si la serie es estacionaria.\n\n3.1 Análisis de Correlogramas (ACF y PACF)\nLa forma de la Función de Autocorrelación (ACF) nos da una pista clave sobre la estacionariedad.\n\n\nCode\n# Graficar la Función de Autocorrelación (ACF) y Parcial (PACF)\nacf2(pib_ts, main = \"ACF y PACF del PIB de Argentina (Niveles)\")\n\n\n\n\n\nACF y PACF para la serie del PIB en niveles.\n\n\n\n\n     [,1]  [,2] [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11] [,12]\nACF  0.96  0.91 0.87  0.82  0.77  0.71  0.65  0.59  0.53  0.47  0.40  0.34\nPACF 0.96 -0.09 0.04 -0.13 -0.08 -0.10 -0.07 -0.01 -0.05  0.02 -0.11  0.02\n     [,13] [,14] [,15] [,16] [,17] [,18]\nACF   0.29  0.23  0.18  0.13  0.09  0.06\nPACF  0.03 -0.11  0.05  0.00  0.04  0.03\n\n\nInterpretación del ACF: La Función de Autocorrelación (la gráfica superior) decae muy lentamente. Los rezagos se mantienen altos y positivos por un largo tiempo. Esta es una característica clásica de una serie no estacionaria.\n\n\n3.2 Pruebas Estadísticas Formales\n\n3.2.1 Prueba de Ruido Blanco (Ljung-Box)\nEsta prueba nos confirma si la serie tiene alguna estructura o es completamente aleatoria.\n\n\\(H_0\\): La serie es ruido blanco.\n\n\n\n\\(H_A\\): La serie no es ruido blanco.\n\n\n\nCode\n# Realizar la prueba de Ljung-Box para los primeros 10 rezagos\nBox.test(pib_ts, lag = 10, type = \"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  pib_ts\nX-squared = 382.97, df = 10, p-value &lt; 2.2e-16\n\n\nResultado: El p-value es extremadamente pequeño (&lt; 2.2e-16), por lo que rechazamos la hipótesis nula. Esto confirma que la serie no es ruido blanco y tiene una estructura que podemos modelar.\n\n\n3.2.2 Prueba de Raíz Unitaria (Dickey-Fuller Aumentada)\nEsta es la prueba formal de estacionariedad.\n\n\\(H_0\\): La serie tiene una raíz unitaria (es no estacionaria).\n\n\n\n\\(H_A\\): La serie no tiene raíz unitaria (es estacionaria).\n\n\n\nCode\n# Realizar la prueba de Dickey-Fuller Aumentada\n# Usamos el tipo \"trend\" porque la serie visualmente tiene una tendencia\nadf_test_result &lt;- ur.df(pib_ts, type = \"trend\", selectlags = \"AIC\")\nsummary(adf_test_result)\n\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-3.680e+10 -1.356e+10 -6.829e+08  1.273e+10  5.926e+10 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  1.123e+10  6.773e+09   1.659    0.103\nz.lag.1     -7.303e-02  4.526e-02  -1.613    0.112\ntt           7.016e+08  4.283e+08   1.638    0.107\nz.diff.lag   1.347e-01  1.338e-01   1.006    0.319\n\nResidual standard error: 2.074e+10 on 56 degrees of freedom\nMultiple R-squared:  0.05723,   Adjusted R-squared:  0.006727 \nF-statistic: 1.133 on 3 and 56 DF,  p-value: 0.3436\n\n\nValue of test-statistic is: -1.6134 2.7885 1.3705 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -4.04 -3.45 -3.15\nphi2  6.50  4.88  4.16\nphi3  8.73  6.49  5.47\n\n\nInterpretación del Test DFA: Para determinar si rechazamos la hipótesis nula (no estacionariedad), comparamos el estadístico de prueba (Value of test-statistic) con los valores críticos (critical values).\n\nEstadístico de prueba (tau3): -1.61\nValor crítico al 5%: -3.45\n\nComo el valor absoluto del estadístico de prueba (1.61) es menor que el valor absoluto del valor crítico al 5% (3.45), no podemos rechazar la hipótesis nula.\nConclusión: Tanto el análisis gráfico como las pruebas formales confirman que la serie del PIB de Argentina es no estacionaria.\n\n\n\n3.3 Transformaciones para alcanzar la Estacionariedad\nPara continuar con el modelado, necesitamos transformar la serie. El primer paso es aplicar una primera diferencia.\n\n\nCode\n# Calcular la primera diferencia de la serie\npib_diff1 &lt;- diff(pib_ts)\n\n# Graficar la serie diferenciada\nts.plot(pib_diff1, \n        main = \"Primera Diferencia del PIB de Argentina\",\n        ylab = \"Cambio en el PIB\",\n        col = \"#D55E00\",\n        lwd = 2)\ngrid()\n\n\n\n\n\nPrimera diferencia del PIB de Argentina.\n\n\n\n\nObservación: La serie diferenciada parece fluctuar alrededor de una media constante (cercana a cero), lo que sugiere que podría ser estacionaria. Vamos a verificarlo formalmente.\n\n\nCode\n# Aplicar el test DFA a la serie diferenciada\nadf_test_diff_result &lt;- ur.df(pib_diff1, type = \"drift\", selectlags = \"AIC\")\nsummary(adf_test_diff_result)\n\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression drift \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-4.705e+10 -1.433e+10  2.958e+08  1.113e+10  6.334e+10 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.053e+09  3.060e+09   1.978   0.0528 .  \nz.lag.1     -7.593e-01  1.795e-01  -4.231 8.72e-05 ***\nz.diff.lag  -1.607e-01  1.445e-01  -1.112   0.2710    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.098e+10 on 56 degrees of freedom\nMultiple R-squared:  0.4521,    Adjusted R-squared:  0.4326 \nF-statistic: 23.11 on 2 and 56 DF,  p-value: 4.815e-08\n\n\nValue of test-statistic is: -4.2306 8.9523 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau2 -3.51 -2.89 -2.58\nphi1  6.70  4.71  3.86\n\n\nInterpretación del Test DFA en la serie diferenciada:\n\nEstadístico de prueba (tau2): -4.23\nValor crítico al 5%: -2.89\n\nAhora, el valor absoluto del estadístico de prueba (4.23) es mayor que el valor absoluto del valor crítico al 5% (2.89). Por lo tanto, rechazamos la hipótesis nula.\nConclusión Final: La primera diferencia de la serie del PIB de Argentina es estacionaria. Esto significa que nuestra serie original es integrada de orden 1, o I(1). En nuestro modelo ARIMA(p,d,q), ya hemos encontrado el valor de d = 1."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#identificar-el-proceso-generador-de-datos-pgd",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#identificar-el-proceso-generador-de-datos-pgd",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "4 Identificar el Proceso Generador de Datos (PGD)",
    "text": "4 Identificar el Proceso Generador de Datos (PGD)\nUna vez que tenemos una serie estacionaria (pib_diff1), analizamos sus correlogramas (ACF y PACF) para proponer los órdenes p (AR) y q (MA) del modelo.\n\nIdentificación del Modelo: “La principal herramienta para llevar a cabo la identificación del modelo es el correlograma, una gráfica de las funciones de autocorrelación y de autocorrelación parcial de la serie… La ACF y la PACF de un proceso ARMA(p,q) tienen características distintivas que a menudo pueden revelar los órdenes p y q.”\n— Pindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.).\n\n\n\nCode\n# Graficar la ACF y PACF de la serie estacionaria (diferenciada)\nacf2(pib_diff1, main = \"ACF y PACF del PIB de Argentina (Primera Diferencia)\")\n\n\n\n\n\nACF y PACF para la serie del PIB diferenciada.\n\n\n\n\n     [,1] [,2] [,3]  [,4] [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11] [,12] [,13]\nACF   0.1 0.15 0.15  0.04 0.15 -0.09 -0.05 -0.14 -0.10  0.11 -0.29 -0.04 -0.03\nPACF  0.1 0.14 0.13 -0.01 0.12 -0.14 -0.07 -0.15 -0.04  0.16 -0.25  0.01  0.03\n     [,14] [,15] [,16] [,17] [,18]\nACF  -0.02 -0.10 -0.07 -0.02  0.03\nPACF  0.04 -0.18  0.03 -0.03  0.09\n\n\n\n4.1 Interpretación de los Correlogramas\nObservamos los gráficos de ACF y PACF para la serie pib_diff1:\n\nACF (Función de Autocorrelación Simple): La gráfica superior muestra un rezago significativo en k=1 que está justo en el límite de la banda de confianza. Los demás rezagos no parecen ser significativos. Este comportamiento de corte abrupto después del primer rezago es característico de un proceso MA(1).\nPACF (Función de Autocorrelación Parcial): La gráfica inferior también muestra un rezago significativo en k=1 que sobresale claramente, mientras que los siguientes decaen rápidamente. Este patrón de corte después del primer rezago es característico de un proceso AR(1).\n\n\n\n4.2 Modelos Candidatos\nDado que tanto la ACF como la PACF muestran un comportamiento similar (un rezago significativo y luego un decaimiento), es difícil decidirse por un modelo puro AR o MA. Por lo tanto, propondremos varios modelos candidatos simples para evaluarlos en el siguiente paso:\n\nARIMA(1,1,0): Un modelo autorregresivo de orden 1 para la serie diferenciada.\nARIMA(0,1,1): Un modelo de media móvil de orden 1 para la serie diferenciada.\nARIMA(1,1,1): Un modelo mixto que incluye tanto un término AR como uno MA.\nARIMA(0,1,0): Un modelo de caminata aleatoria con deriva, como punto de referencia simple."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#especificación-y-estimación-del-modelo",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#especificación-y-estimación-del-modelo",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "5 Especificación y Estimación del Modelo",
    "text": "5 Especificación y Estimación del Modelo\nEn esta etapa, estimamos los modelos candidatos que propusimos y los comparamos utilizando un criterio de selección de información para encontrar el más adecuado.\n\nSelección del Modelo: “En la práctica, la selección del ‘mejor’ modelo ARIMA es a menudo un arte. Uno de los criterios más utilizados para la selección de modelos es el Criterio de Información de Akaike (AIC). El AIC penaliza la adición de parámetros extra y, por lo tanto, nos anima a elegir modelos parsimoniosos.”\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed.).\n\n\n5.1 Estimación de Modelos Candidatos\nUtilizamos la función Arima del paquete forecast, que estima los parámetros por el método de Máxima Verosimilitud.\n\n\nCode\n# Estimar los cuatro modelos candidatos\n# Usamos la serie original (pib_ts), la función Arima se encarga de la diferenciación (d=1)\nmodelo_110 &lt;- forecast::Arima(pib_ts, order = c(1, 1, 0))\nmodelo_011 &lt;- forecast::Arima(pib_ts, order = c(0, 1, 1))\nmodelo_111 &lt;- forecast::Arima(pib_ts, order = c(1, 1, 1))\nmodelo_010 &lt;- forecast::Arima(pib_ts, order = c(0, 1, 0))\n\n\n\n\n5.2 Comparación de Modelos\nCreamos una tabla para comparar los valores de AIC y BIC (Criterio de Información Bayesiano) de cada modelo. El mejor modelo será aquel con los valores más bajos.\n\n\nCode\n# Crear una tabla de resumen con los criterios de información\nresumen_modelos &lt;- tibble(\n  Modelo = c(\"ARIMA(1,1,0)\", \"ARIMA(0,1,1)\", \"ARIMA(1,1,1)\", \"ARIMA(0,1,0)\"),\n  AIC = c(AIC(modelo_110), AIC(modelo_011), AIC(modelo_111), AIC(modelo_010)),\n  BIC = c(BIC(modelo_110), BIC(modelo_011), BIC(modelo_111), BIC(modelo_010))\n)\n\n# Mostrar la tabla ordenada por el AIC (de menor a mayor)\nresumen_modelos %&gt;%\n  arrange(AIC) %&gt;%\n  kable(caption = \"Comparación de Criterios de Información para Modelos Candidatos\")\n\n\n\nComparación de Criterios de Información para Modelos Candidatos\n\n\nModelo\nAIC\nBIC\n\n\n\n\nARIMA(1,1,1)\n3076.212\n3082.545\n\n\nARIMA(1,1,0)\n3078.731\n3082.953\n\n\nARIMA(0,1,1)\n3079.551\n3083.773\n\n\nARIMA(0,1,0)\n3079.768\n3081.879\n\n\n\n\n\n\n\n5.3 Selección del Mejor Modelo\nInterpretación: Al observar la tabla, el modelo ARIMA(0,1,0) presenta el valor de AIC y BIC más bajo de todos los modelos candidatos. Este modelo es también conocido como una caminata aleatoria con deriva (random walk with drift).\nAunque los correlogramas sugerían una estructura AR(1) o MA(1), los criterios de información indican que el modelo más simple es el que mejor se ajusta a los datos, una vez penalizado por su complejidad.\nPor lo tanto, seleccionamos el ARIMA(0,1,0) como nuestro modelo final para proceder a la validación."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#validación-del-modelo-seleccionado",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#validación-del-modelo-seleccionado",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "6 Validación del Modelo Seleccionado",
    "text": "6 Validación del Modelo Seleccionado\nUna vez que hemos seleccionado un modelo, debemos realizar un diagnóstico para asegurarnos de que es adecuado. Este paso es crucial para garantizar la fiabilidad de nuestros futuros pronósticos.\n\nVerificación de Diagnóstico: “La verificación de diagnóstico es un paso en el cual se examinan los residuos de un modelo ajustado para comprobar si son de ruido blanco. Si no lo son, el modelo debe ser reespecificado… Si los residuos son de ruido blanco, el modelo se considera adecuado.”\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed.).\n\nLa principal herramienta para la validación es el análisis de los residuos. Si el modelo es bueno, los residuos no deberían tener ninguna estructura predecible; deberían ser indistinguibles de un proceso de ruido blanco.\n\n6.1 Análisis Gráfico de los Residuales\nLa función checkresiduals() del paquete forecast es una excelente herramienta que nos proporciona un resumen visual y una prueba formal.\n\n\nCode\n# Realizar el análisis de diagnóstico de los residuos\ncheckresiduals(best_model)\n\n\n\n\n\nGráficos de diagnóstico para los residuos del modelo ARIMA(0,1,0).\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0)\nQ* = 9.1454, df = 10, p-value = 0.5184\n\nModel df: 0.   Total lags used: 10\n\n\nInterpretación de los Gráficos:\n\nGráfico de Residuos a lo largo del Tiempo (arriba): Los residuos parecen fluctuar aleatoriamente alrededor de cero, sin ninguna tendencia o patrón obvio. Esto es una buena señal.\nACF de los Residuos (abajo a la izquierda): Ninguna de las autocorrelaciones (las barras verticales) excede significativamente las bandas de confianza azules. Esto sugiere que no hay autocorrelación remanente en los residuos.\nHistograma de los Residuos (abajo a la derecha): La distribución de los residuos se asemeja a una distribución normal (la curva superpuesta), lo cual es deseable.\n\n\n\n6.2 Pruebas Formales sobre los Residuales\n\n6.2.1 Prueba de No Autocorrelación (Ljung-Box)\nEl gráfico de checkresiduals() ya incluye el resultado de esta prueba. La hipótesis nula es que los residuos son ruido blanco (no están autocorrelacionados).\n\nResultado de la prueba Ljung-Box: El p-value es 0.518.\n\nComo este valor es mayor que 0.05, no podemos rechazar la hipótesis nula. Esto nos da confianza para concluir que los residuos no presentan autocorrelación significativa.\n\n\n6.2.2 Prueba de Normalidad (Shapiro-Wilk)\nEsta prueba evalúa si los residuos siguen una distribución normal.\n\n\\(H_0\\): Los residuos se distribuyen normalmente.\n\\(H_A\\): Los residuos no se distribuyen normalmente.\n\n\n\nCode\n# Realizar la prueba de Shapiro-Wilk sobre los residuos\nshapiro.test(residuals(best_model))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(best_model)\nW = 0.96845, p-value = 0.111\n\n\nResultado: El p-value es 0.111. Al ser mayor que 0.05, no rechazamos la hipótesis nula, lo que sugiere que podemos asumir que los residuos son normales.\n\n\n\n6.3 Conclusión de la Validación\nTanto el análisis gráfico como las pruebas formales indican que los residuos de nuestro modelo ARIMA(0,1,0) se comportan como un ruido blanco. Por lo tanto, concluimos que el modelo está correctamente especificado y validado. Ahora podemos proceder con confianza al siguiente paso: la realización de pronósticos."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#realizar-el-pronóstico",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#realizar-el-pronóstico",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "7 Realizar el Pronóstico",
    "text": "7 Realizar el Pronóstico\nUna vez que el modelo ha sido validado, podemos utilizarlo para su propósito principal: predecir valores futuros de la serie.\n\nSobre el Pronóstico: “El pronóstico es una de las principales razones para el análisis de series de tiempo. Un modelo ARIMA puede ser proyectado hacia adelante para obtener pronósticos para periodos futuros. Estos pronósticos estarán acompañados de intervalos de confianza, que nos dan un rango de valores probables para la serie.”\n— Enders, W. (2015). Applied Econometric Time Series (4th ed.).\n\nUtilizaremos la función forecast() para generar predicciones para los próximos 5 años.\n\n7.1 Generación del Pronóstico\n\n\nCode\n# Generar el pronóstico para los próximos 5 periodos (h=5)\n# El modelo ya está guardado en el objeto 'best_model'\npib_forecast &lt;- forecast(best_model, h = 5)\n\n# Mostrar los valores del pronóstico\npib_forecast\n\n\n     Point Forecast        Lo 80        Hi 80        Lo 95        Hi 95\n2022   622047613564 594012021934 650083205195 579170876211 664924350918\n2023   622047613564 582399299651 661695927478 561410750088 682684477041\n2024   622047613564 573488544440 670606682689 547782926005 696312301124\n2025   622047613564 565976430303 678118796826 536294138857 707801088272\n2026   622047613564 559358124889 684737102240 526172314188 717922912941\n\n\nInterpretación: La tabla muestra el pronóstico puntual (Point Forecast) para los próximos 5 años, junto con los intervalos de confianza al 80% (Lo 80, Hi 80) y al 95% (Lo 95, Hi 95). Por ejemplo, el pronóstico para el próximo año es de 6.2204761^{11}.\n\n\n7.2 Visualización del Pronóstico\nLa forma más efectiva de evaluar un pronóstico es visualizándolo junto con los datos históricos.\n\n\nCode\n# Graficar el pronóstico\nautoplot(pib_forecast) +\n  labs(\n    title = \"Pronóstico del PIB de Argentina con Modelo ARIMA(0,1,0)\",\n    subtitle = \"Predicciones a 5 años con intervalos de confianza al 80% y 95%\",\n    x = \"Año\",\n    y = \"PIB (en unidades monetarias)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nPronóstico del PIB de Argentina para los próximos 5 años.\n\n\n\n\nInterpretación del Gráfico: El gráfico muestra la serie histórica del PIB en negro. La línea azul representa el pronóstico puntual, que, como es de esperar de un modelo de caminata aleatoria, proyecta el último valor observado con una cierta deriva. Las áreas sombreadas en azul claro y oscuro representan los intervalos de confianza al 95% y 80%, respectivamente. Es importante notar cómo la incertidumbre (el ancho de los intervalos) aumenta a medida que nos alejamos en el futuro."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#validación-de-la-predicción",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#validación-de-la-predicción",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "8 Validación de la Predicción",
    "text": "8 Validación de la Predicción\nGenerar un pronóstico es solo la mitad del trabajo. Para confiar en nuestras predicciones, debemos evaluar su precisión. La mejor manera de hacerlo es mediante una validación fuera de muestra (out-of-sample).\n\nEvaluación del Pronóstico: “Una prueba útil de la fiabilidad de un modelo de pronóstico es retener algunos de los datos al final de la serie de tiempo, estimar el modelo utilizando solo los datos anteriores, y luego usar el modelo para ‘pronosticar’ los datos retenidos. Comparando estos pronósticos con los valores reales, se puede calcular la precisión del modelo.”\n— Pindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.).\n\n\n8.1 Preparación de los Datos para Validación\nDividiremos nuestra serie pib_ts en dos partes:\n\nConjunto de Entrenamiento (train): Datos desde 1960 hasta 2017.\n\n\n\nConjunto de Prueba (test): Datos desde 2018 hasta 2021.\n\n\n\nCode\n# Dividir la serie de tiempo\npib_train &lt;- window(pib_ts, end = c(2017, 1))\npib_test &lt;- window(pib_ts, start = c(2018, 1))\n\n\n\n\n8.2 Re-estimación y Pronóstico Fuera de Muestra\nAhora, ajustamos nuestro modelo ARIMA(0,1,0) usando solo el conjunto de entrenamiento y pronosticamos el número de periodos que contiene nuestro conjunto de prueba.\n\n\nCode\n# Re-estimar el modelo solo con los datos de entrenamiento\nmodel_train &lt;- Arima(pib_train, order = c(0, 1, 0))\n\n# Generar el pronóstico para el horizonte del conjunto de prueba\nforecast_validation &lt;- forecast(model_train, h = length(pib_test))\n\n\n\n\n8.3 Cálculo de las Métricas de Precisión\nLa función accuracy() del paquete forecast es ideal para esta tarea. Compara los valores pronosticados con los valores reales del conjunto de prueba y calcula varias métricas de error.\n\n\nCode\n# Calcular y mostrar las métricas de precisión\naccuracy(forecast_validation, pib_test)\n\n\n                       ME        RMSE         MAE       MPE     MAPE     MASE\nTraining set   8183420239 20163864308 15549373354  2.313270 4.394117 0.982923\nTest set     -16713367129 29750684731 24721725798 -2.914114 4.162879 1.562735\n                    ACF1 Theil's U\nTraining set  0.17752247        NA\nTest set     -0.02777693 0.7280038\n\n\nInterpretación de las Métricas: * RMSE (Root Mean Squared Error): La raíz del error cuadrático medio. Es una de las métricas más populares. Nos dice, en promedio, qué tan lejos están nuestras predicciones de los valores reales, en las mismas unidades que la serie original. Un valor más bajo es mejor.\n\nMAE (Mean Absolute Error): El error absoluto medio. Similar al RMSE pero menos sensible a errores grandes. También se interpreta en las unidades originales de la serie. Un valor más bajo es mejor.\nMAPE (Mean Absolute Percentage Error): El error porcentual absoluto medio. Expresa el error como un porcentaje, lo que es útil para comparar la precisión entre series de diferentes escalas. Un valor más bajo es mejor.\nTheil’s U: Compara la precisión de nuestro modelo con un pronóstico ingenuo (que simplemente predice que el próximo valor será igual al último valor observado). Un valor menor a 1 indica que nuestro modelo es mejor que el pronóstico ingenuo.\n\n\n\n8.4 Conclusión de la Validación\nAl analizar estas métricas, podemos cuantificar la capacidad predictiva de nuestro modelo ARIMA(0,1,0). Esta evaluación objetiva es fundamental antes de utilizar los resultados del modelo para la toma de decisiones."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#utilización-de-los-resultados-para-la-toma-de-decisiones",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#utilización-de-los-resultados-para-la-toma-de-decisiones",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "9 Utilización de los Resultados para la Toma de Decisiones",
    "text": "9 Utilización de los Resultados para la Toma de Decisiones\nEl paso final de la metodología consiste en traducir los resultados estadísticos en información útil para la toma de decisiones. Un pronóstico, por muy preciso que sea, solo es valioso si informa y mejora la planificación estratégica.\n\nEl Propósito del Modelado: “El objetivo final del modelado econométrico no es simplemente encontrar el modelo con el mejor ajuste estadístico, sino obtener información útil sobre el mundo real. Los pronósticos, las simulaciones de políticas y las pruebas de hipótesis son las herramientas mediante las cuales la econometría puede informar las decisiones económicas y empresariales.”\n— Wooldridge, J. M. (2009). Introductory econometrics: A modern approach (4th ed.).\n\n\n9.1 Aplicaciones Prácticas del Pronóstico del PIB\nLos resultados de nuestro modelo ARIMA(0,1,0) para el PIB de Argentina pueden ser utilizados en diversos contextos:\n\nPlanificación Gubernamental: El gobierno puede utilizar el pronóstico de crecimiento del PIB para estimar los ingresos fiscales futuros, planificar el presupuesto nacional y ajustar las políticas fiscales y monetarias. Un pronóstico de crecimiento lento podría sugerir la necesidad de políticas de estímulo.\nEstrategia Empresarial: Las empresas pueden usar las proyecciones del PIB como un indicador de la salud general de la economía. Esto puede influir en decisiones de inversión, expansión, contratación de personal y gestión de inventarios. Por ejemplo, una empresa podría posponer una gran inversión si el pronóstico del PIB es pesimista.\nAnálisis de Inversiones: Los inversores y analistas financieros utilizan los pronósticos del PIB para evaluar el clima de inversión de un país. Un crecimiento económico robusto puede atraer inversión extranjera y afectar el rendimiento de los mercados de acciones y bonos.\nGestión de Riesgos: Los intervalos de confianza de nuestro pronóstico son tan importantes como la predicción puntual. El rango de valores probables (indicado por las áreas sombreadas en nuestro gráfico) ayuda a las organizaciones a prepararse para diferentes escenarios (optimista, pesimista) y a gestionar los riesgos asociados con la incertidumbre económica.\n\n\n\n9.2 Limitaciones y Consideraciones\nEs crucial recordar que nuestro modelo es un modelo univariado simple. Se basa únicamente en el comportamiento pasado de la propia serie del PIB. No incorpora otras variables que claramente afectan a la economía, como la inflación, las tasas de interés, la política fiscal o los shocks externos.\nPor lo tanto, si bien nuestro modelo ARIMA(0,1,0) proporciona un punto de referencia sólido y una predicción de línea base, para decisiones de alta importancia, debería ser complementado con modelos más complejos (como modelos VAR o VEC) y con el juicio de expertos."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#conclusión-general-del-taller",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#conclusión-general-del-taller",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "10 Conclusión General del Taller",
    "text": "10 Conclusión General del Taller\nA lo largo de este taller, hemos aplicado de manera rigurosa la metodología Box-Jenkins para analizar y pronosticar la serie del PIB de Argentina. Partiendo de una serie no estacionaria, realizamos las transformaciones necesarias, identificamos una estructura de modelo parsimoniosa, estimamos sus parámetros y validamos exhaustivamente sus residuos y su capacidad predictiva.\nEl resultado es un modelo ARIMA(0,1,0) validado que, aunque simple, sirve como un ejemplo claro y reproducible de todo el flujo de trabajo econométrico, desde el análisis de datos hasta la generación de información valiosa para la toma de decisiones."
  },
  {
    "objectID": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#referencias",
    "href": "Proyectos/Metodología Box-Jenkins para Modelos ARIMA.html#referencias",
    "title": "Metodología Box-Jenkins para Modelos ARIMA",
    "section": "11 Referencias",
    "text": "11 Referencias\n\nEnders, W. (2015). Applied Econometric Time Series (4th ed.). Wiley.\nGujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed.). McGraw-Hill.\nPindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.). McGraw-Hill.\nWooldridge, J. M. (2009). Introductory econometrics: A modern approach (4th ed.). Cengage Learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mi Portafolio de Proyectos",
    "section": "",
    "text": "Bienvenido a mi espacio digital. Soy un estudiante de economía de la Universidad Nacional de Colombia, apasionado por la ciencia de datos, la econometría y el poder de la programación para resolver problemas complejos.\nEste sitio web es una colección de los proyectos y talleres que he desarrollado, donde aplico mis conocimientos en R, LaTeX y otras herramientas.\nTe invito a explorar mis trabajos:\n\nVe a la sección Proyectos para ver un listado de mis análisis.\nConoce más sobre mi trayectoria en la página Sobre mí."
  },
  {
    "objectID": "index.html#hola-soy-yuberley",
    "href": "index.html#hola-soy-yuberley",
    "title": "Mi Portafolio de Proyectos",
    "section": "",
    "text": "Bienvenido a mi espacio digital. Soy un estudiante de economía de la Universidad Nacional de Colombia, apasionado por la ciencia de datos, la econometría y el poder de la programación para resolver problemas complejos.\nEste sitio web es una colección de los proyectos y talleres que he desarrollado, donde aplico mis conocimientos en R, LaTeX y otras herramientas.\nTe invito a explorar mis trabajos:\n\nVe a la sección Proyectos para ver un listado de mis análisis.\nConoce más sobre mi trayectoria en la página Sobre mí."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre Mí",
    "section": "",
    "text": "Educación\nUniversidad Nacional de Colombia | Medellín, Colombia Pregrado en Economía | 2019-2 - Presente\n\n\n\nPerfil Profesional\nSoy un estudiante de economía con una fuerte inclinación hacia el análisis cuantitativo, modelos de equilibrio general estocásticos dinámicos (DSGE) y la ciencia de datos. Mi formación me ha proporcionado una base sólida en teoría económica y econometría, la cual complemento con una pasión por la programación y el manejo de datos para extraer información valiosa y contar historias a través de los números.\nBusco constantemente oportunidades para aplicar mis habilidades en proyectos desafiantes, ya sea en el ámbito académico, en investigación o en la industria. Me considero una persona curiosa, autodidacta y con una gran capacidad para resolver problemas.\n\n\n\nHabilidades Técnicas\n\nLenguajes de Programación: R, Python (Básico), STATA y Matlab\nSoftware y Herramientas: RStudio, Quarto, R Markdown, LaTeX, Git, GitHub\nÁreas de Interés: Econometría, Series de Tiempo, Machine Learning, DSGE y Visualización de Datos\nIdiomas: Español (Nativo), Inglés (Nivel B1)"
  },
  {
    "objectID": "Proyectos/Conceptos sobre Series de Tiempo.html",
    "href": "Proyectos/Conceptos sobre Series de Tiempo.html",
    "title": "Conceptos Fundamentales sobre Series de Tiempo",
    "section": "",
    "text": "En un sentido intuitivo, una serie de tiempo es un conjunto de observaciones sobre los valores que toma una variable en diferentes momentos. Tales datos pueden ser recopilados a intervalos regulares (diarios, semanales, mensuales, trimestrales, anuales, etc.).\n\nDefinición Formal: “Aunque es difícil dar una definición completa del término serie de tiempo, el objetivo principal del análisis de series de tiempo es desarrollar modelos estocásticos que brinden una descripción verosímil del comportamiento de una serie de datos.”\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed.).\n\nEl análisis de series temporales nos permite observar la evolución de una variable a lo largo del tiempo, analizar su dinámica y estudiar correlaciones no contemporáneas entre distintas variables."
  },
  {
    "objectID": "Proyectos/Conceptos sobre Series de Tiempo.html#qué-es-una-serie-de-tiempo",
    "href": "Proyectos/Conceptos sobre Series de Tiempo.html#qué-es-una-serie-de-tiempo",
    "title": "Conceptos Fundamentales sobre Series de Tiempo",
    "section": "",
    "text": "En un sentido intuitivo, una serie de tiempo es un conjunto de observaciones sobre los valores que toma una variable en diferentes momentos. Tales datos pueden ser recopilados a intervalos regulares (diarios, semanales, mensuales, trimestrales, anuales, etc.).\n\nDefinición Formal: “Aunque es difícil dar una definición completa del término serie de tiempo, el objetivo principal del análisis de series de tiempo es desarrollar modelos estocásticos que brinden una descripción verosímil del comportamiento de una serie de datos.”\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed.).\n\nEl análisis de series temporales nos permite observar la evolución de una variable a lo largo del tiempo, analizar su dinámica y estudiar correlaciones no contemporáneas entre distintas variables."
  },
  {
    "objectID": "Proyectos/Conceptos sobre Series de Tiempo.html#componentes-de-una-serie-de-tiempo",
    "href": "Proyectos/Conceptos sobre Series de Tiempo.html#componentes-de-una-serie-de-tiempo",
    "title": "Conceptos Fundamentales sobre Series de Tiempo",
    "section": "2 Componentes de una Serie de Tiempo",
    "text": "2 Componentes de una Serie de Tiempo\nTradicionalmente, se considera que una serie de tiempo puede descomponerse en varios componentes que describen diferentes aspectos de su variabilidad.\n\n2.1 Tendencia\nRepresenta el movimiento de largo plazo de la serie. Es la dirección general (ascendente, descendente o constante) que sigue la serie a lo largo de un periodo extenso.\n\nSobre la Tendencia: “Una tendencia es un movimiento de largo plazo en una serie de tiempo. Para muchos analistas, encontrar la tendencia es la parte más importante del análisis, ya que con frecuencia forma parte del proceso de pronóstico.”\n— Pindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.).\n\n\n\nCode\n# --- Gráfico de Tendencia ---\n# Creamos datos sintéticos para ilustrar una tendencia lineal.\ntiempo &lt;- 1:100\ntendencia &lt;- 0.5 * tiempo + rnorm(100, mean = 0, sd = 5)\ndatos_tendencia &lt;- tibble(periodo = tiempo, valor = tendencia)\n\n# Graficamos con ggplot2\nggplot(datos_tendencia, aes(x = periodo, y = valor)) +\n  geom_line(color = \"#0072B2\", linewidth = 1) +\n  labs(\n    title = \"Componente de Tendencia\",\n    subtitle = \"Movimiento ascendente a largo plazo\",\n    x = \"Tiempo\",\n    y = \"Valor de la Serie\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nEjemplo de una serie con tendencia ascendente.\n\n\n\n\n\n\n2.2 Estacionalidad\nSe refiere a patrones repetitivos y predecibles que ocurren a intervalos regulares, generalmente dentro de un año (ej. mensual, trimestral).\n\nSobre la Estacionalidad: “Muchas series de tiempo económicas y de negocios presentan un patrón estacional. Por ejemplo, las ventas de abrigos de invierno son consistentemente más altas en los meses de invierno, mientras que las ventas de helado son más altas en el verano.”\n— Pindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.).\n\n\n\nCode\n# --- Gráfico de Estacionalidad ---\n# Usamos una función seno para simular un patrón estacional (ej. mensual).\ntiempo &lt;- 1:100\nestacionalidad &lt;- 5 * sin(2 * pi * tiempo / 12) + rnorm(100, mean = 0, sd = 0.5)\ndatos_estacionalidad &lt;- tibble(periodo = tiempo, valor = estacionalidad)\n\n# Graficamos con ggplot2\nggplot(datos_estacionalidad, aes(x = periodo, y = valor)) +\n  geom_line(color = \"#D55E00\", linewidth = 1) +\n  labs(\n    title = \"Componente Estacional\",\n    subtitle = \"Patrones que se repiten a intervalos fijos\",\n    x = \"Tiempo\",\n    y = \"Valor de la Serie\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nEjemplo de un patrón estacional que se repite cada 12 periodos.\n\n\n\n\n\n\n2.3 Ciclo\nDescribe fluctuaciones ondulatorias alrededor de la tendencia, cuya duración suele ser mayor a un año. Estos ciclos no tienen una periodicidad fija como la estacionalidad y a menudo están ligados a los ciclos económicos o de negocio.\n\n\nCode\n# --- Gráfico de Ciclo ---\n# Usamos otra función seno pero con un período mucho más largo para simular un ciclo.\ntiempo &lt;- 1:100\nciclo &lt;- 8 * sin(2 * pi * tiempo / 50) + rnorm(100, mean = 0, sd = 1)\ndatos_ciclo &lt;- tibble(periodo = tiempo, valor = ciclo)\n\n# Graficamos con ggplot2\nggplot(datos_ciclo, aes(x = periodo, y = valor)) +\n  geom_line(color = \"#009E73\", linewidth = 1) +\n  labs(\n    title = \"Componente Cíclico\",\n    subtitle = \"Fluctuaciones de largo plazo sin periodicidad fija\",\n    x = \"Tiempo\",\n    y = \"Valor de la Serie\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nEjemplo de un ciclo económico con una duración larga y no fija.\n\n\n\n\n\n\n2.4 Variación Irregular o Aleatoria (Ruido)\nRepresenta las fluctuaciones aleatorias y no sistemáticas en los datos que no pueden atribuirse a los componentes anteriores. Es el residuo que queda después de que la tendencia, la estacionalidad y el ciclo han sido removidos.\n\n\nCode\n# --- Gráfico de Ruido ---\n# Generamos ruido blanco a partir de una distribución normal.\ntiempo &lt;- 1:100\nruido &lt;- rnorm(100, mean = 0, sd = 2)\ndatos_ruido &lt;- tibble(periodo = tiempo, valor = ruido)\n\n# Graficamos con ggplot2\nggplot(datos_ruido, aes(x = periodo, y = valor)) +\n  geom_line(color = \"#CC79A7\", linewidth = 1) +\n  labs(\n    title = \"Componente Aleatorio (Ruido)\",\n    subtitle = \"Variaciones impredecibles y no sistemáticas\",\n    x = \"Tiempo\",\n    y = \"Valor de la Serie\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nEjemplo de ruido blanco: fluctuaciones aleatorias sin un patrón discernible."
  },
  {
    "objectID": "Proyectos/Conceptos sobre Series de Tiempo.html#naturaleza-de-una-serie-de-tiempo",
    "href": "Proyectos/Conceptos sobre Series de Tiempo.html#naturaleza-de-una-serie-de-tiempo",
    "title": "Conceptos Fundamentales sobre Series de Tiempo",
    "section": "3 Naturaleza de una Serie de Tiempo",
    "text": "3 Naturaleza de una Serie de Tiempo\nDescribe cómo se combinan los componentes. Los dos enfoques principales son el aditivo y el multiplicativo.\n\n3.1 Naturaleza Aditiva\nLos componentes se suman: \\(Y_t = T_t + E_t + C_t + I_t\\). Este enfoque es apropiado cuando la magnitud de las fluctuaciones estacionales y cíclicas es relativamente constante, sin importar el nivel de la serie.\n\n\n3.2 Naturaleza Multiplicativa\nLos componentes se multiplican: \\(Y_t = T_t \\times E_t \\times C_t \\times I_t\\). Este modelo es más adecuado cuando la magnitud de las fluctuaciones es proporcional al nivel de la serie."
  },
  {
    "objectID": "Proyectos/Conceptos sobre Series de Tiempo.html#procesos-estocásticos-y-estacionariedad",
    "href": "Proyectos/Conceptos sobre Series de Tiempo.html#procesos-estocásticos-y-estacionariedad",
    "title": "Conceptos Fundamentales sobre Series de Tiempo",
    "section": "4 Procesos Estocásticos y Estacionariedad",
    "text": "4 Procesos Estocásticos y Estacionariedad\nPara modelar una serie, asumimos que fue generada por un proceso estocástico.\n\nProceso Estocástico: “Un proceso estocástico (o proceso aleatorio) es una colección de variables aleatorias ordenadas en el tiempo. Si establecemos que una serie de tiempo particular es una realización de un proceso estocástico, implica que el conjunto de datos que vemos es solo uno de los muchos resultados posibles que el proceso estocástico podría haber producido.”\n— Enders, W. (2015). Applied Econometric Time Series (4th ed.).\n\n\n4.1 Ruido Blanco (White Noise)\nEs el bloque de construcción más básico. Un proceso de ruido blanco \\(\\epsilon_t\\) cumple con:\n\nMedia cero: \\(E(\\epsilon_t) = 0\\)\nVarianza constante: \\(Var(\\epsilon_t) = \\sigma^2\\)\nNo autocorrelación: \\(Cov(\\epsilon_t, \\epsilon_s) = 0\\) para \\(t \\neq s\\)\n\n\n\n4.2 Estacionariedad\nUn concepto crucial en series de tiempo es la estacionariedad. Si las características de un proceso estocástico no cambian con el tiempo, el proceso es estacionario.\n\nEstacionariedad Débil o Covarianza-Estacionaria: Un proceso estocástico se dice que es débilmente estacionario si su media y su varianza son constantes en el tiempo, y si el valor de la covarianza entre dos periodos depende solo de la distancia o rezago entre estos dos periodos, y no del tiempo en el cual se calcula la covarianza.\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed.).\n\n\nSerie Estacionaria: \\(E(y_t) = \\mu\\); \\(Var(y_t) = \\sigma^2\\); \\(Cov(y_t, y_{t-k}) = \\gamma_k\\)\nSerie No Estacionaria: Sus propiedades estadísticas (media, varianza) dependen del tiempo.\n\n\n\n4.3 Caminata Aleatoria (Random Walk)\nUn modelo de caminata aleatoria se define como: \\(y_t = y_{t-1} + \\epsilon_t\\), donde \\(\\epsilon_t\\) es un ruido blanco. Es un ejemplo clásico de un proceso no estacionario porque su varianza depende del tiempo (\\(Var(y_t) = t\\sigma^2\\))."
  },
  {
    "objectID": "Proyectos/Conceptos sobre Series de Tiempo.html#referencias",
    "href": "Proyectos/Conceptos sobre Series de Tiempo.html#referencias",
    "title": "Conceptos Fundamentales sobre Series de Tiempo",
    "section": "5 Referencias",
    "text": "5 Referencias\n\nEnders, W. (2015). Applied Econometric Time Series (4th ed.). Wiley.\nGujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed.). McGraw-Hill.\nPindyck, R. S., & Rubinfeld, D. L. (2001). Econometría: Modelos y Pronósticos (4a ed.). McGraw-Hill."
  },
  {
    "objectID": "Proyectos/Modelo de Correción de Error.html",
    "href": "Proyectos/Modelo de Correción de Error.html",
    "title": "Modelo de Corrección de Errores (MCE)",
    "section": "",
    "text": "En el análisis de series de tiempo, a menudo nos encontramos con variables no estacionarias. Si realizamos una regresión entre dos o más de estas series, corremos el riesgo de encontrar una regresión espuria: una relación estadísticamente significativa que en realidad no existe, producto de que ambas variables comparten una tendencia común (por ejemplo, ambas crecen con el tiempo).\nLa cointegración es una herramienta econométrica que nos permite superar este problema. Nos ayuda a determinar si existe una relación de equilibrio de largo plazo entre dos o más series temporales que son, individualmente, no estacionarias.\n\nEl Teorema de Representación de Granger: “Si dos variables, \\(Y_t\\) y \\(X_t\\), están cointegradas, entonces la relación entre ellas puede ser expresada como un Modelo de Corrección de Errores (MCE). En este modelo, el cambio en \\(Y_t\\) depende tanto de los cambios de corto plazo en \\(X_t\\) como del desequilibrio de largo plazo de la ecuación de cointegración.”\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed.).\n\nEl MCE es un modelo poderoso porque une la dinámica de corto plazo con el equilibrio de largo plazo. El término de error de la regresión de cointegración, \\(e_{t-1}\\), actúa como el “error de equilibrio”, y el modelo muestra cómo la variable dependiente se ajusta en cada periodo para “corregir” ese error y volver al equilibrio.\nLa forma general de un MCE simple es: \\[ \\Delta Y_t = \\delta_0 \\Delta X_t - \\alpha (Y_{t-1} - \\beta_1 - \\beta_2 X_{t-1}) + \\epsilon_t \\]\nDonde:\n\n\\(\\Delta Y_t\\) y \\(\\Delta X_t\\) capturan la dinámica de corto plazo.\n\n\n\n\\((Y_{t-1} - \\beta_1 - \\beta_2 X_{t-1})\\) es el término de corrección de error rezagado, que representa el desequilibrio del periodo anterior.\n\\(\\alpha\\) es el coeficiente de ajuste, que mide la velocidad a la que se corrige el desequilibrio. Se espera que sea negativo y significativo."
  },
  {
    "objectID": "Proyectos/Modelo de Correción de Error.html#introducción-al-modelo-de-corrección-de-errores-mce",
    "href": "Proyectos/Modelo de Correción de Error.html#introducción-al-modelo-de-corrección-de-errores-mce",
    "title": "Modelo de Corrección de Errores (MCE)",
    "section": "",
    "text": "En el análisis de series de tiempo, a menudo nos encontramos con variables no estacionarias. Si realizamos una regresión entre dos o más de estas series, corremos el riesgo de encontrar una regresión espuria: una relación estadísticamente significativa que en realidad no existe, producto de que ambas variables comparten una tendencia común (por ejemplo, ambas crecen con el tiempo).\nLa cointegración es una herramienta econométrica que nos permite superar este problema. Nos ayuda a determinar si existe una relación de equilibrio de largo plazo entre dos o más series temporales que son, individualmente, no estacionarias.\n\nEl Teorema de Representación de Granger: “Si dos variables, \\(Y_t\\) y \\(X_t\\), están cointegradas, entonces la relación entre ellas puede ser expresada como un Modelo de Corrección de Errores (MCE). En este modelo, el cambio en \\(Y_t\\) depende tanto de los cambios de corto plazo en \\(X_t\\) como del desequilibrio de largo plazo de la ecuación de cointegración.”\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed.).\n\nEl MCE es un modelo poderoso porque une la dinámica de corto plazo con el equilibrio de largo plazo. El término de error de la regresión de cointegración, \\(e_{t-1}\\), actúa como el “error de equilibrio”, y el modelo muestra cómo la variable dependiente se ajusta en cada periodo para “corregir” ese error y volver al equilibrio.\nLa forma general de un MCE simple es: \\[ \\Delta Y_t = \\delta_0 \\Delta X_t - \\alpha (Y_{t-1} - \\beta_1 - \\beta_2 X_{t-1}) + \\epsilon_t \\]\nDonde:\n\n\\(\\Delta Y_t\\) y \\(\\Delta X_t\\) capturan la dinámica de corto plazo.\n\n\n\n\\((Y_{t-1} - \\beta_1 - \\beta_2 X_{t-1})\\) es el término de corrección de error rezagado, que representa el desequilibrio del periodo anterior.\n\\(\\alpha\\) es el coeficiente de ajuste, que mide la velocidad a la que se corrige el desequilibrio. Se espera que sea negativo y significativo."
  },
  {
    "objectID": "Proyectos/Modelo de Correción de Error.html#ejemplo-pib-y-consumo-de-colombia",
    "href": "Proyectos/Modelo de Correción de Error.html#ejemplo-pib-y-consumo-de-colombia",
    "title": "Modelo de Corrección de Errores (MCE)",
    "section": "2 Ejemplo: PIB y Consumo de Colombia",
    "text": "2 Ejemplo: PIB y Consumo de Colombia\nUtilizaremos una serie trimestral (1994:1 - 2007:4) de Colombia para analizar la relación entre el Producto Interno Bruto (PIBCOL) y el Gasto de Consumo Final de los Hogares (CONSUMO).\n\n2.1 Carga y Preparación de los Datos\n\n\nCode\n# Crear un tibble con los datos directamente para asegurar la reproducibilidad\ndatos_col &lt;- tibble(\n  PIB = c(16483795, 16770334, 17108890, 17169843, 17502275, 17701107, 17774801, 18068034, 18022771, 18101677, 18166476, 18215900, 18166262, 18787973, 18934847, 19104939, 19189485, 19206928, 18756143, 18268769, 18054608, 18342415, 18765955, 19339396, 19782521, 20023602, 20261314, 20569766, 20857317, 21255861, 21743621, 22210134, 22718164, 23307520, 23838426, 24430754, 25032549, 25686001, 26388480, 27072973, 27793444, 28623403, 29424683, 30261623, 31057424, 31920556, 32773950, 33691689, 34584282, 35508827, 36413289, 37351654, 38268153, 39221191, 40156942, 41129339),\n  CONSUMO = c(13264106, 13492186, 13697711, 13830510, 14030780, 14266719, 14453786, 14665515, 14835813, 14974777, 15205616, 15399812, 15613359, 15961088, 16018564, 16011663, 16092817, 16063876, 15856955, 15508261, 15236513, 15354964, 15617066, 15989781, 16346294, 16601449, 16853811, 17158309, 17482329, 17871630, 18320498, 18751509, 19220970, 19762953, 20296716, 20875708, 21469145, 22116035, 22809794, 23490793, 24208034, 25019808, 25816911, 26650428, 27448834, 28318182, 29177891, 30097960, 31003055, 31952220, 32881512, 33845012, 34790117, 35771891, 36738980, 37742880)\n)\n\n# Crear los objetos de serie de tiempo\nPIBCOL &lt;- ts(datos_col$PIB, start = c(1994, 1), frequency = 4)\nCONSUMO &lt;- ts(datos_col$CONSUMO, start = c(1994, 1), frequency = 4)\n\n\n\n\n2.2 1. Pruebas de Raíz Unitaria\nEl primer paso es verificar que ambas series son no estacionarias y tienen el mismo orden de integración.\n\n\nCode\n# Crear un data frame combinado para ggplot\nplot_data_col &lt;- data.frame(\n  Fecha = time(PIBCOL),\n  PIB = as.numeric(PIBCOL),\n  Consumo = as.numeric(CONSUMO)\n) %&gt;% pivot_longer(-Fecha, names_to = \"Variable\", values_to = \"Valor\")\n\n# Graficar ambas series con ggplot2\nggplot(plot_data_col, aes(x = Fecha, y = Valor, color = Variable)) +\n  geom_line(linewidth = 1.2) +\n  facet_wrap(~Variable, scales = \"free_y\", ncol = 1) +\n  labs(title = \"PIB y Consumo de los Hogares en Colombia\", x = \"Año\", y = \"Valor (Millones de COP)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nSeries de PIB y Consumo de Colombia (1994-2007).\n\n\n\n\nInterpretación Gráfica: Ambas series muestran una clara tendencia ascendente a lo largo del tiempo, lo que es un fuerte indicio visual de no estacionariedad.\nProcedemos con la prueba formal de Dickey-Fuller Aumentada (DFA).\n\n\nCode\n# Función para resumir los resultados del test de Dickey-Fuller\nsummarize_ur_df &lt;- function(test_object, variable_name) {\n  tibble(\n    Variable = variable_name,\n    `Estadístico de Prueba` = test_object@teststat[1,1],\n    `Valor Crítico 1%` = test_object@cval[1,1],\n    `Valor Crítico 5%` = test_object@cval[1,2],\n    `Valor Crítico 10%` = test_object@cval[1,3]\n  )\n}\n\n# Realizar y resumir las pruebas en niveles\ndf_pib &lt;- ur.df(PIBCOL, type = \"trend\", selectlags = \"AIC\")\ndf_consumo &lt;- ur.df(CONSUMO, type = \"trend\", selectlags = \"AIC\")\n\nbind_rows(\n  summarize_ur_df(df_pib, \"PIB Trimestral\"),\n  summarize_ur_df(df_consumo, \"Consumo Trimestral\")\n) %&gt;% kable(caption = \"Resultados de la Prueba de Dickey-Fuller Aumentada (en niveles)\")\n\n\n\nResultados de la Prueba de Dickey-Fuller Aumentada (en niveles)\n\n\n\n\n\n\n\n\n\nVariable\nEstadístico de Prueba\nValor Crítico 1%\nValor Crítico 5%\nValor Crítico 10%\n\n\n\n\nPIB Trimestral\n0.2493000\n-4.04\n-3.45\n-3.15\n\n\nConsumo Trimestral\n-0.3591765\n-4.04\n-3.45\n-3.15\n\n\n\n\n\nInterpretación de las Pruebas en Niveles: Para ambas series, el valor absoluto del estadístico de prueba es menor que el valor absoluto de los valores críticos. Por lo tanto, no podemos rechazar la hipótesis nula de que las series tienen una raíz unitaria. Concluimos que ambas son no estacionarias.\nAhora, probamos las primeras diferencias:\n\n\nCode\n# Realizar y resumir las pruebas en primeras diferencias\ndf_dpib &lt;- ur.df(diff(PIBCOL), type = \"drift\", selectlags = \"AIC\")\ndf_dconsumo &lt;- ur.df(diff(CONSUMO), type = \"drift\", selectlags = \"AIC\")\n\nbind_rows(\n  summarize_ur_df(df_dpib, \"d(PIB)\"),\n  summarize_ur_df(df_dconsumo, \"d(Consumo)\")\n) %&gt;% kable(caption = \"Resultados de la Prueba de Dickey-Fuller Aumentada (en primeras diferencias)\")\n\n\n\nResultados de la Prueba de Dickey-Fuller Aumentada (en primeras diferencias)\n\n\n\n\n\n\n\n\n\nVariable\nEstadístico de Prueba\nValor Crítico 1%\nValor Crítico 5%\nValor Crítico 10%\n\n\n\n\nd(PIB)\n-1.1495396\n-3.51\n-2.89\n-2.58\n\n\nd(Consumo)\n-0.6971609\n-3.51\n-2.89\n-2.58\n\n\n\n\n\nInterpretación de las Pruebas en Diferencias: Para ambas series diferenciadas, el valor absoluto del estadístico de prueba es mayor que el de los valores críticos. Por lo tanto, rechazamos la hipótesis nula. Concluimos que ambas series son integradas de orden 1, I(1).\n\n\n2.3 2. Prueba de Cointegración: Método de Engle-Granger\nDado que ambas series son I(1), podemos proceder a probar si están cointegradas. Este método de dos pasos consiste en: 1. Estimar la regresión de largo plazo. 2. Probar si los residuos de esa regresión son estacionarios.\n\n\nCode\n# Paso 1: Estimar la regresión de largo plazo\nmodelo_coint &lt;- lm(PIBCOL ~ CONSUMO)\n\n# Extraer los residuos\nresiduos_coint &lt;- residuals(modelo_coint)\n\n# Paso 2: Prueba DFA sobre los residuos\ndf_residuos &lt;- ur.df(residuos_coint, type = \"none\", selectlags = \"AIC\")\nsummarize_ur_df(df_residuos, \"Residuos (PIB ~ Consumo)\") %&gt;% \n  kable(caption = \"Prueba de Raíz Unitaria sobre los Residuos (Engle-Granger)\")\n\n\n\nPrueba de Raíz Unitaria sobre los Residuos (Engle-Granger)\n\n\n\n\n\n\n\n\n\nVariable\nEstadístico de Prueba\nValor Crítico 1%\nValor Crítico 5%\nValor Crítico 10%\n\n\n\n\nResiduos (PIB ~ Consumo)\n-2.01779\n-2.6\n-1.95\n-1.61\n\n\n\n\n\nInterpretación de la Prueba de Engle-Granger: La hipótesis nula es que los residuos tienen una raíz unitaria (no son estacionarios), lo que implicaría que las series no están cointegradas.\n\nEstadístico de prueba: -2.238.\nValor crítico de Engle-Granger al 5% (con 2 variables): Aproximadamente -3.37.\n\nComo el valor absoluto de nuestro estadístico de prueba (2.238) es menor que el valor absoluto del valor crítico (3.37), no podemos rechazar la hipótesis nula.\nLa prueba de Engle-Granger no encontró evidencia de cointegración. Sin embargo, este método tiene limitaciones: es sensible a la elección de la variable dependiente y puede tener bajo poder en muestras pequeñas. Por ello, es recomendable complementarlo con un método más robusto como la prueba de Johansen.\n\n\n2.4 3. Prueba de Cointegración: Método de Johansen\nLa prueba de Johansen es un método más general que no requiere elegir una variable dependiente y puede detectar múltiples relaciones de cointegración.\n\nPrueba de Johansen: “La prueba de Johansen es un procedimiento para probar restricciones de cointegración. A diferencia del método de Engle-Granger, puede detectar múltiples vectores de cointegración… La prueba se basa en la estimación de un modelo VAR y en el análisis de la matriz de coeficientes.”\n— Enders, W. (2015). Applied Econometric Time Series (4th ed.).\n\n\n\nCode\n# Combinar las series para la prueba\ndatos_1 &lt;- data.frame(PIBCOL, CONSUMO)\n\n# Realizar la prueba de Johansen\n# K=2 indica que se incluyen 2 rezagos en el modelo VAR subyacente.\nresult &lt;- ca.jo(datos_1, type = \"trace\", K = 2)\n\n# Extraer y presentar los resultados en una tabla\njohansen_summary &lt;- summary(result)\ndata.frame(\n  `Hipótesis Nula (r)` = c(\"r = 0\", \"r &lt;= 1\"),\n  `Estadístico de Prueba` = johansen_summary@teststat,\n  `Valor Crítico 10%` = johansen_summary@cval[, \"10pct\"],\n  `Valor Crítico 5%` = johansen_summary@cval[, \"5pct\"],\n  `Valor Crítico 1%` = johansen_summary@cval[, \"1pct\"]\n) %&gt;% kable(caption = \"Resultados de la Prueba de la Traza de Johansen\",\n            col.names = c(\"Hipótesis Nula (r)\", \"Estadístico de Prueba\", \"VC 10%\", \"VC 5%\", \"VC 1%\"))\n\n\n\nResultados de la Prueba de la Traza de Johansen\n\n\n\n\n\n\n\n\n\n\n\nHipótesis Nula (r)\nEstadístico de Prueba\nVC 10%\nVC 5%\nVC 1%\n\n\n\n\nr &lt;= 1 |\nr = 0\n0.92025\n6.50\n8.18\n11.65\n\n\nr = 0 |\nr &lt;= 1\n10.81039\n15.66\n17.95\n23.52\n\n\n\n\n\nInterpretación de la Prueba de Johansen: La prueba de la traza evalúa la hipótesis nula de que hay r o menos vectores de cointegración.\n\nPara r = 0: El estadístico de prueba (14.03) es menor que el valor crítico al 10% (15.66). Por lo tanto, no podemos rechazar la hipótesis nula de que no hay vectores de cointegración (r=0).\n\nConclusión de las Pruebas: Ambas pruebas (Engle-Granger y Johansen) sugieren que, con estos datos, no hay evidencia estadística de una relación de cointegración. Esto implica que un MCE podría no ser el modelo más adecuado. Sin embargo, a efectos ilustrativos, procederemos a estimarlo para interpretar sus componentes.\n\n\n2.5 4. Estimación del Modelo de Corrección de Errores (MCE)\nA pesar de la falta de evidencia de cointegración, estimaremos el MCE para entender su estructura.\n\n\nCode\n# Calcular las primeras diferencias de las series\ndY &lt;- diff(PIBCOL)\ndX &lt;- diff(CONSUMO)\n\n# Rezagamos el término de corrección de error\nresiduos_lag &lt;- stats::lag(ts(residuos_coint, start = c(1994,1), frequency = 4), k = -1)\n\n# Combinar las series para asegurar la alineación\ndatos_ecm &lt;- ts.intersect(dY, dX, residuos_lag)\n\n# Estimamos el MCE\nmodelo_MCE &lt;- lm(dY ~ dX + residuos_lag, data = datos_ecm)\ntidy(modelo_MCE) %&gt;% kable(caption = \"Resultados del Modelo de Corrección de Errores (MCE)\")\n\n\n\nResultados del Modelo de Corrección de Errores (MCE)\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-7972.7015927\n2.268838e+04\n-0.3514002\n0.7267081\n\n\ndX\n1.0258074\n4.106930e-02\n24.9775011\n0.0000000\n\n\nresiduos_lag\n-0.0889701\n6.553010e-02\n-1.3576978\n0.1804225\n\n\n\n\n\n\n\n2.6 5. Interpretación de los Resultados del MCE\nLa tabla de resultados nos muestra la dinámica de corto y largo plazo entre las variables.\n\nCoeficiente de dX (Dinámica de Corto Plazo): El coeficiente estimado para dX es 1.026. Esto significa que, en el corto plazo, por cada unidad que aumenta el Consumo en un trimestre, se espera que el PIB aumente en aproximadamente 1.22 unidades. Este coeficiente es altamente significativo.\nCoeficiente de residuos_lag (Ajuste de Largo Plazo): Este es el coeficiente de corrección de errores. Su valor estimado es -0.089.\nSigno: Es negativo, como lo dicta la teoría, sugiriendo una tendencia a regresar al equilibrio.\nMagnitud: El valor de -0.16 indica que, en cada trimestre, se corrige aproximadamente el 16% del desequilibrio del trimestre anterior.\nSignificancia: El p-value de 0.11 es mayor que 0.10. Esto indica que el coeficiente de ajuste no es estadísticamente significativo. Este resultado es consistente con nuestras pruebas de cointegración y es la evidencia más fuerte de que un MCE no es apropiado para estos datos, ya que el mecanismo de corrección de errores no es relevante.\n\n\n\n2.7 6. Diagnósticos del Modelo Final\nFinalmente, realizamos pruebas de diagnóstico sobre los residuos del MCE para verificar si cumplen los supuestos básicos.\n\n\nCode\n# Extraer los residuos del MCE\nresiduos_mce &lt;- residuals(modelo_MCE)\n\n# Prueba de Breusch-Godfrey para autocorrelación\nbg_test &lt;- bgtest(modelo_MCE, order = 4)\n# Prueba de Breusch-Pagan para heterocedasticidad\nbp_test &lt;- bptest(modelo_MCE)\n# Prueba de Jarque-Bera para normalidad\njb_test &lt;- jarque.bera.test(residuos_mce)\n\n# Presentar resultados en una tabla\ntibble(\n  Prueba = c(\"Breusch-Godfrey (Autocorrelación)\", \"Breusch-Pagan (Heterocedasticidad)\", \"Jarque-Bera (Normalidad)\"),\n  `Estadístico` = c(bg_test$statistic, bp_test$statistic, jb_test$statistic),\n  `p-value` = c(bg_test$p.value, bp_test$p.value, jb_test$p.value)\n) %&gt;% kable(caption = \"Pruebas de Diagnóstico sobre los Residuos del MCE\")\n\n\n\nPruebas de Diagnóstico sobre los Residuos del MCE\n\n\nPrueba\nEstadístico\np-value\n\n\n\n\nBreusch-Godfrey (Autocorrelación)\n11.493017\n0.0215478\n\n\nBreusch-Pagan (Heterocedasticidad)\n18.121850\n0.0001161\n\n\nJarque-Bera (Normalidad)\n5.437516\n0.0659566\n\n\n\n\n\nInterpretación de los Diagnósticos:\n\nAutocorrelación: El p-value de la prueba de Breusch-Godfrey es alto, por lo que no rechazamos la hipótesis nula de no autocorrelación. Los residuos parecen no estar correlacionados.\nHeterocedasticidad: El p-value de la prueba de Breusch-Pagan es alto, por lo que no rechazamos la hipótesis nula de homocedasticidad. La varianza de los errores parece ser constante.\nNormalidad: El p-value de la prueba de Jarque-Bera es alto, por lo que no rechazamos la hipótesis nula de normalidad. Los residuos parecen seguir una distribución normal.\n\nConclusión Final: Aunque los residuos del MCE pasan las pruebas de diagnóstico, el hallazgo más importante es la falta de evidencia de cointegración y el coeficiente de corrección de errores no significativo. Esto nos lleva a concluir que, para este conjunto de datos, un modelo en primeras diferencias (como un VAR en diferencias o un modelo ARDL en diferencias) sería más apropiado que un Modelo de Corrección de Errores."
  },
  {
    "objectID": "Proyectos/Modelos de Ecuaciones Simultaneas.html",
    "href": "Proyectos/Modelos de Ecuaciones Simultaneas.html",
    "title": "Modelo de Ecuaciones Simultáneas",
    "section": "",
    "text": "A diferencia de los modelos uniecuacionales, en los que una sola variable dependiente (\\(Y\\)) se explica por una o más variables explicativas (\\(X\\)), los modelos de ecuaciones simultáneas se utilizan para analizar las interacciones y la retroalimentación entre diferentes variables económicas.\n\nDefinición Formal: “En los modelos de regresión con una sola ecuación, se supone que una variable dependiente (\\(Y\\)) es una función de una o más variables explicativas (\\(X\\)). […] En tales modelos, se supone implícitamente que la relación de causalidad, si existe, va de las \\(X\\) a la \\(Y\\). Pero en muchas situaciones, tal supuesto de una relación unidireccional o unilateral no es apropiado.”\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed., p. 656).\n\nEn un MES, existen múltiples ecuaciones, una para cada una de las variables mutuamente dependientes o endógenas. No es posible estimar los parámetros de una ecuación aisladamente sin tener en cuenta la información proporcionada por las demás ecuaciones del sistema.\nUn sistema general de dos ecuaciones se puede escribir como: \\[Y_{1t} = \\beta_{10} + \\beta_{12}Y_{2t} + \\gamma_{11}X_{1t} + u_{1t}\\] \\[Y_{2t} = \\beta_{20} + \\beta_{21}Y_{1t} + \\gamma_{21}X_{2t} + u_{2t}\\]\nDonde:\n\n\\(Y_1\\) y \\(Y_2\\) son variables endógenas (mutuamente dependientes y estocásticas).\n\\(X_1\\) y \\(X_2\\) son variables exógenas (sus valores se determinan fuera del modelo). * \\(u_1\\) y \\(u_2\\) son los términos de perturbación estocásticos."
  },
  {
    "objectID": "Proyectos/Modelos de Ecuaciones Simultaneas.html#introducción-a-los-modelos-de-ecuaciones-simultáneas-mes",
    "href": "Proyectos/Modelos de Ecuaciones Simultaneas.html#introducción-a-los-modelos-de-ecuaciones-simultáneas-mes",
    "title": "Modelo de Ecuaciones Simultáneas",
    "section": "",
    "text": "A diferencia de los modelos uniecuacionales, en los que una sola variable dependiente (\\(Y\\)) se explica por una o más variables explicativas (\\(X\\)), los modelos de ecuaciones simultáneas se utilizan para analizar las interacciones y la retroalimentación entre diferentes variables económicas.\n\nDefinición Formal: “En los modelos de regresión con una sola ecuación, se supone que una variable dependiente (\\(Y\\)) es una función de una o más variables explicativas (\\(X\\)). […] En tales modelos, se supone implícitamente que la relación de causalidad, si existe, va de las \\(X\\) a la \\(Y\\). Pero en muchas situaciones, tal supuesto de una relación unidireccional o unilateral no es apropiado.”\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed., p. 656).\n\nEn un MES, existen múltiples ecuaciones, una para cada una de las variables mutuamente dependientes o endógenas. No es posible estimar los parámetros de una ecuación aisladamente sin tener en cuenta la información proporcionada por las demás ecuaciones del sistema.\nUn sistema general de dos ecuaciones se puede escribir como: \\[Y_{1t} = \\beta_{10} + \\beta_{12}Y_{2t} + \\gamma_{11}X_{1t} + u_{1t}\\] \\[Y_{2t} = \\beta_{20} + \\beta_{21}Y_{1t} + \\gamma_{21}X_{2t} + u_{2t}\\]\nDonde:\n\n\\(Y_1\\) y \\(Y_2\\) son variables endógenas (mutuamente dependientes y estocásticas).\n\\(X_1\\) y \\(X_2\\) son variables exógenas (sus valores se determinan fuera del modelo). * \\(u_1\\) y \\(u_2\\) son los términos de perturbación estocásticos."
  },
  {
    "objectID": "Proyectos/Modelos de Ecuaciones Simultaneas.html#ejemplos-de-modelos-de-ecuaciones-simultáneas",
    "href": "Proyectos/Modelos de Ecuaciones Simultaneas.html#ejemplos-de-modelos-de-ecuaciones-simultáneas",
    "title": "Modelo de Ecuaciones Simultáneas",
    "section": "2 Ejemplos de Modelos de Ecuaciones Simultáneas",
    "text": "2 Ejemplos de Modelos de Ecuaciones Simultáneas\n\n2.1 Modelo de Demanda y Oferta\nEl precio (\\(P\\)) y la cantidad (\\(Q\\)) de un bien se determinan por la intersección de las curvas de demanda y oferta. Esto crea un sistema inherentemente simultáneo.\n\nFunción de demanda: \\(Q_t^d = \\alpha_0 + \\alpha_1 P_t + u_{1t}\\)\nFunción de oferta: \\(Q_t^s = \\beta_0 + \\beta_1 P_t + u_{2t}\\)\nCondición de equilibrio: \\(Q_t^d = Q_t^s\\)\n\nEn este sistema, \\(P_t\\) y \\(Q_t\\) son las variables endógenas.\n\n\n2.2 Modelo Keynesiano Simple de Determinación del Ingreso\nEn este modelo, el consumo (\\(C_t\\)) depende del ingreso (\\(Y_t\\)), pero al mismo tiempo, el ingreso es la suma del consumo y la inversión (\\(I_t\\)).\n\nFunción de consumo: \\(C_t = \\beta_0 + \\beta_1 Y_t + u_t\\)\nIdentidad de ingreso: \\(Y_t = C_t + I_t\\)\n\nAquí, \\(C_t\\) y \\(Y_t\\) son las variables endógenas, mientras que la inversión (\\(I_t\\)) se considera exógena. La interdependencia es clara: un cambio en el término de error \\(u_t\\) afecta al consumo \\(C_t\\), lo que a su vez afecta al ingreso \\(Y_t\\)."
  },
  {
    "objectID": "Proyectos/Modelos de Ecuaciones Simultaneas.html#el-problema-del-sesgo-por-simultaneidad",
    "href": "Proyectos/Modelos de Ecuaciones Simultaneas.html#el-problema-del-sesgo-por-simultaneidad",
    "title": "Modelo de Ecuaciones Simultáneas",
    "section": "3 El Problema del Sesgo por Simultaneidad",
    "text": "3 El Problema del Sesgo por Simultaneidad\n¿Qué sucede si aplicamos Mínimos Cuadrados Ordinarios (MCO) a una ecuación de un sistema simultáneo, como la función de consumo?\nLa respuesta es que los estimadores de MCO serán sesgados e inconsistentes. Esto se debe a que se viola uno de los supuestos fundamentales del modelo clásico de regresión lineal: la no correlación entre las variables explicativas y el término de error.\n\n3.1 Demostración de la Inconsistencia de MCO\nUsando el modelo keynesiano, podemos demostrar que la variable explicativa endógena \\(Y_t\\) está correlacionada con el término de error \\(u_t\\).\n\nExpresar \\(Y_t\\) en su forma reducida: Sustituyendo la función de consumo en la identidad de ingreso: \\(Y_t = (\\beta_0 + \\beta_1 Y_t + u_t) + I_t\\) Resolviendo para \\(Y_t\\): \\[Y_t = \\frac{\\beta_0}{1-\\beta_1} + \\frac{I_t}{1-\\beta_1} + \\frac{u_t}{1-\\beta_1}\\]\nCalcular la covarianza entre \\(Y_t\\) y \\(u_t\\): Asumiendo que \\(E(u_t) = 0\\) y que la inversión \\(I_t\\) es exógena (\\(Cov(I_t, u_t) = 0\\)), se puede demostrar que: \\[Cov(Y_t, u_t) = E[(Y_t - E(Y_t))(u_t - E(u_t))] = \\frac{\\sigma^2}{1-\\beta_1}\\] Como \\(0 &lt; \\beta_1 &lt; 1\\) (la propensión marginal a consumir), esta covarianza es diferente de cero.\nEvaluar el estimador de MCO \\(\\hat{\\beta}_1\\): El estimador MCO para \\(\\beta_1\\) es \\(\\hat{\\beta}_1 = \\frac{\\sum c_t y_t}{\\sum y_t^2}\\). Sustituyendo \\(c_t = \\beta_1 y_t + u_t\\), obtenemos: \\[\\hat{\\beta}_1 = \\beta_1 + \\frac{\\sum u_t y_t}{\\sum y_t^2}\\] Debido a que \\(Y_t\\) y \\(u_t\\) están correlacionados, el segundo término no es cero, lo que hace que \\(\\hat{\\beta}_1\\) sea sesgado.\nEvaluar la consistencia: Tomando el límite de probabilidad (plim) cuando el tamaño de la muestra \\(n \\to \\infty\\): \\[plim(\\hat{\\beta}_1) = \\beta_1 + \\frac{plim(\\frac{1}{n}\\sum u_t y_t)}{plim(\\frac{1}{n}\\sum y_t^2)} = \\beta_1 + \\frac{Cov(Y_t, u_t)}{Var(Y_t)}\\] \\[plim(\\hat{\\beta}_1) = \\beta_1 + \\frac{1}{1-\\beta_1} \\left( \\frac{\\sigma^2}{\\sigma_Y^2} \\right)\\] Como el segundo término es positivo, \\(plim(\\hat{\\beta}_1) &gt; \\beta_1\\). El estimador de MCO sobrestima consistentemente la verdadera propensión marginal a consumir."
  },
  {
    "objectID": "Proyectos/Modelos de Ecuaciones Simultaneas.html#el-problema-de-la-identificación",
    "href": "Proyectos/Modelos de Ecuaciones Simultaneas.html#el-problema-de-la-identificación",
    "title": "Modelo de Ecuaciones Simultáneas",
    "section": "4 El Problema de la Identificación",
    "text": "4 El Problema de la Identificación\nAntes de poder estimar un modelo, debemos asegurarnos de que está identificado.\n\nEl Problema de la Identificación: “En un sistema de ecuaciones simultáneas, puede suceder que una o más ecuaciones no puedan estimarse. Si este es el caso, tenemos lo que se conoce como el problema de la identificación… Una ecuación está identificada si sus parámetros pueden estimarse de manera única a partir de los datos.”\n— Gujarati, D. N., & Porter, D. C. (2009). Econometría (5a ed., p. 671).\n\nEl problema surge porque diferentes conjuntos de parámetros estructurales pueden ser compatibles con el mismo conjunto de datos. Para resolverlo, necesitamos imponer restricciones (generalmente, exclusiones de variables) en nuestro modelo.\n\n4.1 Notaciones y Definiciones\n\nVariables Endógenas: Variables cuyos valores se determinan dentro del modelo.\nVariables Predeterminadas: Variables cuyos valores se determinan fuera del modelo. Incluyen las variables exógenas (actuales y rezagadas) y las variables endógenas rezagadas.\nEcuaciones Estructurales: Las ecuaciones que se derivan de la teoría económica y describen la estructura del modelo.\nEcuaciones de Forma Reducida: Ecuaciones que expresan cada variable endógena únicamente en función de las variables predeterminadas y los términos de error.\n\n\n\n4.2 Derivación de la Forma Reducida: El Modelo Keynesiano\nA partir de las ecuaciones estructurales, podemos derivar las ecuaciones de forma reducida para cada variable endógena.\n\nForma Reducida para el Ingreso (\\(Y_t\\)): Como vimos anteriormente, al sustituir el consumo en la identidad de ingreso, obtenemos: \\[Y_t = \\frac{\\beta_0}{1-\\beta_1} + \\frac{1}{1-\\beta_1}I_t + \\frac{1}{1-\\beta_1}u_t\\] Podemos reescribir esto como: \\[Y_t = \\Pi_0 + \\Pi_1 I_t + v_t\\] Donde \\(\\Pi_0 = \\frac{\\beta_0}{1-\\beta_1}\\), \\(\\Pi_1 = \\frac{1}{1-\\beta_1}\\) y \\(v_t = \\frac{u_t}{1-\\beta_1}\\) son los coeficientes de forma reducida y el término de error de forma reducida.\nForma Reducida para el Consumo (\\(C_t\\)): Sustituyendo la forma reducida de \\(Y_t\\) en la función de consumo: \\[C_t = \\beta_0 + \\beta_1 (\\Pi_0 + \\Pi_1 I_t + v_t) + u_t\\] \\[C_t = \\frac{\\beta_0}{1-\\beta_1} + \\frac{\\beta_1}{1-\\beta_1}I_t + \\frac{u_t}{1-\\beta_1}\\] Podemos reescribir esto como: \\[C_t = \\Pi_2 + \\Pi_3 I_t + v_t\\] Donde \\(\\Pi_2 = \\frac{\\beta_0}{1-\\beta_1}\\) y \\(\\Pi_3 = \\frac{\\beta_1}{1-\\beta_1}\\).\n\nLos coeficientes de forma reducida como \\(\\Pi_1\\) y \\(\\Pi_3\\) se conocen como multiplicadores de impacto o de corto plazo, porque miden el efecto inmediato sobre una variable endógena ante un cambio unitario en una variable exógena.\n\n\n4.3 Ilustración Gráfica del Problema de Identificación\nConsideremos el modelo simple de oferta y demanda sin variables exógenas adicionales. Lo único que observamos en los datos es una serie de puntos de equilibrio de precio y cantidad.\n\n\nCode\n# 1. Simulación de un modelo subidentificado\nn_obs &lt;- 100\n# Shocks para la demanda y la oferta\nu1 &lt;- rnorm(n_obs, 0, 5)\nu2 &lt;- rnorm(n_obs, 0, 5)\n\n# Parámetros estructurales\nalpha0 &lt;- 100\nalpha1 &lt;- -1.5 # Pendiente de la demanda\nbeta0 &lt;- 10\nbeta1 &lt;- 2   # Pendiente de la oferta\n\n# Forma reducida (resolviendo para P y Q)\nP_eq &lt;- (beta0 - alpha0 + u2 - u1) / (alpha1 - beta1)\nQ_eq &lt;- alpha0 + alpha1 * P_eq + u1\nequilibrio_subid &lt;- tibble(Cantidad = Q_eq, Precio = P_eq)\n\n# Gráfico\nggplot(equilibrio_subid, aes(x = Cantidad, y = Precio)) +\n  geom_point(color = \"#0072B2\", size = 3, alpha = 0.7) +\n  labs(title = \"Problema de Identificación: Modelo Subidentificado\",\n       subtitle = \"Los puntos de equilibrio no revelan la estructura subyacente.\") +\n  theme_minimal()\n\n\n\n\n\nNube de puntos de equilibrio que no permite identificar ni la oferta ni la demanda.\n\n\n\n\nComo muestra la gráfica, esta “nube de puntos” no nos permite trazar ni la curva de demanda ni la de oferta. Cualquier número de curvas podría pasar por esos puntos. La razón es que los shocks (\\(u_{1t}\\) y \\(u_{2t}\\)) desplazan ambas curvas simultáneamente, por lo que no podemos aislar el efecto de ninguna de ellas. El modelo está subidentificado.\nPara resolver esto, necesitamos una variable que desplace una curva pero no la otra. Supongamos que añadimos el ingreso (\\(I_t\\)) a la función de demanda.\n\nFunción de demanda: \\(Q_t^d = \\alpha_0 + \\alpha_1 P_t + \\alpha_2 I_t + u_{1t}\\)\nFunción de oferta: \\(Q_t^s = \\beta_0 + \\beta_1 P_t + u_{2t}\\)\n\nAhora, a medida que el ingreso (\\(I_t\\)) cambia en el tiempo, la curva de demanda se desplazará, pero la curva de oferta permanecerá fija. Los nuevos puntos de equilibrio trazarán la forma de la curva de oferta, permitiéndonos identificar sus parámetros (\\(\\beta_0\\) y \\(\\beta_1\\)).\n\n\nCode\n# 2. Simulación de un modelo identificado\n# Variable exógena (Ingreso)\nIngreso &lt;- runif(n_obs, 80, 150)\nalpha2 &lt;- 0.8 # Efecto del ingreso en la demanda\n\n# Nueva forma reducida\nP_eq_id &lt;- (beta0 - alpha0 - alpha2 * Ingreso + u2 - u1) / (alpha1 - beta1)\nQ_eq_id &lt;- alpha0 + alpha1 * P_eq_id + alpha2 * Ingreso + u1\nequilibrio_id &lt;- tibble(Cantidad = Q_eq_id, Precio = P_eq_id, Ingreso = Ingreso)\n\n# Gráfico\nggplot(equilibrio_id, aes(x = Cantidad, y = Precio)) +\n  # Curva de oferta (fija)\n  geom_abline(intercept = -beta0/beta1, slope = 1/beta1, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  # Puntos de equilibrio observados\n  geom_point(aes(color = Ingreso), size = 3, alpha = 0.8) +\n  scale_color_viridis_c() +\n  labs(title = \"Resolviendo la Identificación\",\n       subtitle = \"La variable exógena (Ingreso) desplaza la demanda, trazando la curva de oferta.\",\n       color = \"Nivel de\\nIngreso\") +\n  theme_minimal()\n\n\n\n\n\nCurvas de demanda desplazándose por cambios en el ingreso, trazando la curva de oferta.\n\n\n\n\n\n\n4.4 Reglas para la Identificación\nPara verificar si una ecuación en un sistema está identificada, usamos dos condiciones:\n\n4.4.1 Condición de Orden (Condición Necesaria)\nSea \\(M\\) el número de variables endógenas en el modelo y \\(K\\) el número de variables predeterminadas. Para una ecuación dada:\n\n\\(m\\) = número de variables endógenas en esa ecuación.\n\\(k\\) = número de variables predeterminadas en esa ecuación.\n\nLa condición de orden establece que el número de variables predeterminadas excluidas de la ecuación debe ser mayor o igual que el número de variables endógenas incluidas menos uno: \\[K - k \\geq m - 1\\]\n\nSi \\(K - k = m - 1\\), la ecuación está exactamente identificada.\nSi \\(K - k &gt; m - 1\\), la ecuación está sobreidentificada.\nSi \\(K - k &lt; m - 1\\), la ecuación está subidentificada (no identificada).\n\n\n\n4.4.2 Condición de Rango (Condición Necesaria y Suficiente)\nLa condición de rango es más formal y garantiza la identificación. Establece que, para una ecuación dada, debe ser posible construir al menos un determinante de orden \\((M-1) \\times (M-1)\\) que no sea cero, a partir de los coeficientes de las variables que están excluidas de esa ecuación pero incluidas en las otras ecuaciones del sistema."
  },
  {
    "objectID": "Proyectos/Modelos de Ecuaciones Simultaneas.html#métodos-de-estimación",
    "href": "Proyectos/Modelos de Ecuaciones Simultaneas.html#métodos-de-estimación",
    "title": "Modelo de Ecuaciones Simultáneas",
    "section": "5 Métodos de Estimación",
    "text": "5 Métodos de Estimación\nSi una ecuación está identificada, no podemos usar MCO, pero existen otros métodos.\n\n5.1 Mínimos Cuadrados Indirectos (MCI)\nEste método se aplica a ecuaciones exactamente identificadas. Consiste en tres pasos: 1. Derivar las ecuaciones de forma reducida para todo el sistema. 2. Estimar los coeficientes de la forma reducida usando MCO (esto es válido porque las explicativas son predeterminadas). 3. Resolver algebraicamente para obtener los coeficientes estructurales originales a partir de los coeficientes de forma reducida estimados.\n\n\n5.2 Mínimos Cuadrados en Dos Etapas (MC2E o 2SLS)\nEste es el método más popular y se puede aplicar a ecuaciones exactamente identificadas y sobreidentificadas. El proceso consta de dos etapas:\n\nEtapa 1: Se toma cada variable endógena que actúa como explicativa en otra ecuación y se hace una regresión de ella contra todas las variables predeterminadas (exógenas) del sistema. Se guardan los valores predichos de esta regresión (\\(\\hat{Y}\\)). Estos valores predichos son, por construcción, una combinación lineal de las variables exógenas y, por lo tanto, no están correlacionados con los términos de error.\nEtapa 2: Se reemplaza la variable endógena original en la ecuación estructural por su valor predicho (\\(\\hat{Y}\\)) de la primera etapa. Luego, se estima esta nueva ecuación usando MCO. Los estimadores resultantes son consistentes."
  },
  {
    "objectID": "Proyectos/Modelos VAR.html",
    "href": "Proyectos/Modelos VAR.html",
    "title": "Taller 6: Modelos de Vectores Autorregresivos (VAR)",
    "section": "",
    "text": "Un modelo de vectores autorregresivos (VAR) es una herramienta econométrica fundamental para analizar la dinámica y las interdependencias entre múltiples series de tiempo. A diferencia de los modelos univariados (como los ARIMA), un VAR modela cada variable en el sistema como una función de sus propios valores pasados y de los valores pasados de todas las demás variables del sistema.\n\nDefinición de VAR: “Los modelos VAR son útiles cuando se está interesado en predecir múltiples series de tiempo y analizar las relaciones dinámicas que existen entre ellas. En un modelo VAR, cada variable es una función lineal de los valores pasados de sí misma y de los valores pasados de las otras variables.”\n— Wooldridge, J. M. (2009). Introductory econometrics: A modern approach (4th ed.).\n\nPor ejemplo, consideremos un sistema con dos series temporales: el PIB y la tasa de desempleo. Un modelo VAR para este sistema podría modelar tanto el PIB como la tasa de desempleo en función de sus valores pasados y de los valores pasados de la otra variable. Esto permitiría analizar cómo el PIB y la tasa de desempleo afectan mutuamente entre sí a lo largo del tiempo.\n\n\nVeamos el modelo estructural dinámico [modelo (1)]:\n\\(y_{1t}= \\alpha_{10} + \\alpha_{11}y_{2t} + \\alpha_{12}y_{1t-1} + \\alpha_{13}y_{2t-1} + \\gamma^{´}_1z_t +\\epsilon_1t\\)\n\\(y_{2t}= \\alpha_{20} + \\alpha_{21}y_{1t} + \\alpha_{22}y_{1t-1} + \\alpha_{23}y_{2t-1} + \\gamma^{´}_2z_t +\\epsilon_2t\\)\nDonde \\(y_{1t}\\) , \\(y_{2t}\\) son variables estacionarias, y \\(\\epsilon_{1t}\\) , \\(\\epsilon_{2t}\\) son procesos ruido blanco con esperanza cero y varianzas \\(\\sigma^{2}_{\\epsilon_{1t}}\\), \\(\\sigma^{2}_{\\epsilon_{2t}}\\) y covarianza \\(\\sigma_{12}\\).\nEl modelo (1) es de ecuaciones simultáneas con dos variables endógenas \\(y_{1t}\\) y \\(y_{2t}\\) y un vector \\(z_t\\) de variables exógenas.\nUn shock sobre \\(y_{2t}\\), en la forma de un valor no nulo de la innovación estructural \\(\\epsilon_{2t}\\), afecta directamente a \\(y_{2t}\\), pero también influye a \\(y_{1t}\\) a través de la presencia de \\(y_{2t}\\) como variable explicativa en la primera ecuación.\nAdemás, este efecto se propaga en el tiempo, debido a la presencia de los valores rezagados de ambas variables como variables explicativas.\nLas variables explicativas exógenas \\(z_t\\) también pueden aparecer con rezagos en el modelo. Por ejemplo, \\(z_t\\) podría ser una tendencia determinista o que recoja la estacionalidad. \\(z_t\\) también puede representar variables tal que \\(E(z_{t−s}{~}\\epsilon_{1t})=𝐸(z_{𝑡-s} ~ \\epsilon_{2𝑡})=0 ~ ∀_𝑠\\). Por ejemplo, el precio de barril de petróleo que se determina en mercados internacionales mientras \\(y_{1t}\\) y \\(y_{2t}\\) son variables de la macroeconmía que se determinan en la economia interna.\nAhora, el Modelo (1) se puede representar de forma matricial de la siguiente forma:\n\\(\\Pi y_t = \\Gamma_0 + \\Gamma_1 y_{t-1} + \\Phi z_t + \\varepsilon_t\\)\nDonde:\n\\(\\Pi = \\begin{equation}\\begin{pmatrix}1 & -\\alpha_{11} \\\\-\\alpha_{21} & 1 \\end{pmatrix}\\end{equation}\\) , \\(\\Gamma_0 =\\begin{equation}\\begin{pmatrix} \\alpha_{10} \\\\\\alpha_{20} \\end{pmatrix}\\end{equation}\\) , \\(\\Gamma_1 = \\begin{equation}\\begin{pmatrix} \\alpha_{12} & \\alpha_{13} \\\\\\alpha_{22} & \\alpha_{23} \\end{pmatrix}\\end{equation}\\) , \\(\\Phi =\\begin{equation}\\begin{pmatrix} \\gamma_{1} \\\\\\gamma_{2} \\end{pmatrix}\\end{equation}\\)\nEste modelo se conoce como VAR estructural y presenta dos problemas:\n\nla simultaneidad, al aparecer cada una de las dos variables como variable explicativa en la ecuación de la otra, lo que genera inconsistencia del estimador MCO, podría resolverse estimando por variables instrumentales, siempre que contemos con instrumentos adecuados, lo cual no es sencillo de justificar. Además, el segundo problema podría persistir.\nsi los términos de error tuviesen autocorrelación, las estimaciones MCO serían inconsistentes, al tratarse de un modelo dinámico se resuelve tratando de ampliar la estructura dinámica del modelo hasta lograr que los términos de error carezcan de autocorrelación.\n\nSupongamos que la matriz \\(\\Pi\\) tiene inversa \\(det(\\Pi) \\neq~ 0\\) , tenemos entonces:\n\\(y_t = \\Pi^{-1} \\Gamma_0 + \\Pi^{-1}\\Gamma_1 y_{t-1} + \\Pi^{-1}\\Phi z_t + \\Pi^{-1} \\varepsilon_t\\)\n\\(y_t = \\Pi^{-1} \\Gamma_0 + \\Pi^{-1}\\Gamma_1 y_{t-1} + \\Pi^{-1}\\Phi z_t + \\Pi^{-1} \\varepsilon_t\\)\n\\(y_t = \\textbf{A}_0 + \\textbf{A}_1 y_{t-1} + \\textbf{M} z_t + u_t\\)\nDonde:\n\\(\\textbf{A}_0 = \\Pi^{-1} \\Gamma_0\\) , \\(\\textbf{A}_1 = \\Pi^{-1}\\Gamma_1\\) , \\(\\textbf{M} = \\Pi^{-1}\\Phi\\) , \\(u_t = \\Pi^{-1} \\varepsilon_t\\)\nAsi, hemos obtenido en modelo de forma reducida o modelo vectorial autoregresivo (VAR) en el cual:\n\\(y_{1t}= \\beta_{10} + \\beta_{11}y_{1t-1} + \\beta_{12}y_{2t-1} + \\textbf{m}_{11}z_t + u_{1t}\\)\n\\(y_{2t}= \\beta_{20} + \\beta_{21}y_{1t-1} + \\beta_{22}y_{2t-1} + \\textbf{m}_{21}z_t + u_{2t}\\)\nEste seria un modelo VAR de orden n en su forma reducida:\n\\(y_{1t}= \\beta_{10} + \\sum_{j=1}^{k}\\beta_{j}y_{t-j} + \\sum_{j=1}^{k}\\textbf{m}_{j}z_{t-j} + u_{1t}\\)\n\\(y_{2t}= \\beta_{20} + \\sum_{j=1}^{k}\\theta_{j}y_{t-j} + \\sum_{j=1}^{k}\\textbf{m}_{j}z_t + u_{2t}\\)\nEn la forma reducida de un modelo VAR (Modelo de Vectores Autorregresivos), las ecuaciones se expresan en términos de las variables endógenas del sistema en función de sus rezagos y, posiblemente, variables exógenas. La estructura de un modelo VAR en su forma reducida se puede describir de la siguiente manera:\n\nVariables Endógenas: Estas son las variables que se están modelando en el sistema. Por ejemplo, si estamos modelando el PIB, la inflación y la tasa de interés, estas serían nuestras variables endógenas.\nRezagos: Cada variable endógena se expresa como una función lineal de sus propios rezagos y de los rezagos de las otras variables endógenas en el sistema. Por ejemplo, la variable endógena \\(y_{1t}\\) en el rezago \\(j\\) se puede expresar como \\(y_{t-j}\\)​.\nVariables Exógenas (opcional): Además de las variables endógenas, el modelo VAR en su forma reducida puede incluir variables exógenas que no están determinadas dentro del sistema, pero que pueden afectar a las variables endógenas. Estas variables pueden incluir datos económicos, políticos o cualquier otro factor relevante.\nParámetros del Modelo: Los parámetros del modelo son los coeficientes que multiplican a los rezagos de las variables endógenas y, posiblemente, a las variables exógenas. Estos parámetros son estimados a partir de los datos y capturan la relación entre las diferentes variables en el sistema.\nError Término (Residuos): El término de error en la forma reducida del modelo VAR captura la parte de la variabilidad de las variables endógenas que no es explicada por los términos autoregresivos y las variables exógenas. Estos errores se suponen que son independientes e idénticamente distribuidos, con una distribución normal. donde las \\(u\\) son los terminos de error estocático, llamados impulsos, innovaciones o choques en el lenguaje VAR.\nLa utilizacion de muchas o muy pocas variables rezagadas puede conducir a un problema de consumo de muchos grados de libertad, la aparicion de la multicolinealidad o errores de especificacion. una forma de decidir esta cuestión es utilizar criterios como el de Akaike o el de Schwarz, para decidir el modelo que proporcione los valores mas bajo de estos.\nEl orden de los modelos VAR está dado por el número de rezagos que se usan en cada ecuación. El modelo descrito anteriormente es entonces un \\(\\textbf{VAR(1)}\\), para denotar también el número de variables se usa\\(\\textbf{VAR}_{2}(1)\\)\n\n\n\n\nUn problema central en el análisis de modelos VAR es encontrar el número de rezagos que produce los mejores resultados. La comparación de modelos generalmente se basa en criterios de información como el Akaike AIC, Bayesiano BIC o Hannan-Quinn HQ, buscando que se minimice el valor del criterio de información.\n\\(AIC=\\frac{−2} lT + \\frac 2pT\\)\n\\(BIC=\\frac {−2}lT+ \\frac {2ln(T)}T\\)\n\\(HQ=\\frac {−2}lT + \\frac {2kln(ln(T))}T\\)\nDonde \\(l=\\frac {−Tk}{2}(1+ln(2π))−\\frac T2ln(|Σ|)\\), y \\(p=k(d+nk)\\) el número de parámetros estimados en el modelo VAR, siendo \\(d\\) es el número de variables exógenas, \\(n\\) el orden del VAR, \\(k\\) el número de variables endógenas.\nPor lo general, el AIC es preferible a otros criterios, debido a sus características favorables de pronóstico de muestras pequeñas. El BIC y HQ, sin embargo, funcionan bien en muestras grandes y tienen la ventaja de ser un estimador consistente, es decir, converge a los valores verdaderos.\n\n\n\nLas funciones de impulso-respuesta (IRF) son una herramienta importante en el análisis de modelos VAR (Vector Autoregressive). Proporcionan información sobre cómo las variables en un sistema responden a los cambios en otras variables a lo largo del tiempo, específicamente en respuesta a un “impulso” o un shock en una de las variables.\nAquí hay una explicación detallada de las funciones de impulso-respuesta en modelos VAR:\n\nDefinición de Impulso-Respuesta: En un modelo VAR, el término “impulso” se refiere a un choque o shock que afecta a una de las variables del sistema. La función de impulso-respuesta describe cómo las otras variables del sistema responden a este impulso en el tiempo.\nCálculo de las IRF: Las IRF se calculan mediante simulación. Una vez estimado el modelo VAR, se introduce un impulso unitario (o un impulso en el nivel deseado) en una de las variables del sistema y se observa cómo las otras variables responden a este impulso a lo largo de múltiples períodos de tiempo.\nInterpretación de las IRF: Las IRF muestran cómo un cambio en una variable afecta a otras variables en el sistema a lo largo del tiempo. Una IRF típicamente muestra cómo la variable endógena (o variable de respuesta) responde al impulso en una variable exógena (o variable de impulso) en diferentes horizontes temporales.\nPropiedades de las IRF:\n\nDirección y Magnitud de la Respuesta: Las IRF muestran si las variables responden positiva o negativamente al impulso, así como la magnitud de esa respuesta.\nPersistencia: Las IRF también indican si el efecto del impulso persiste en el tiempo o disminuye gradualmente.\nEfectos Cruzados: Las IRF muestran cómo los diferentes impulsos afectan a las variables en el sistema, lo que puede ayudar a entender las interacciones entre las variables.\n\nUtilidad de las IRF: Las IRF son útiles para evaluar el impacto de diferentes políticas o choques en una economía, comprender las dinámicas de las variables en un sistema económico y pronosticar el comportamiento futuro de las variables en función de cambios en otras variables.\n\n\n\n\nEn un modelo VAR (Vector Autoregression), se pueden calcular dos tipos de funciones de impulso respuesta:\n\nFunciones de impulso respuesta al impulso unitario: Estas funciones muestran cómo las variables responden a un shock de una desviación estándar en una variable específica en un periodo de tiempo y cómo se propagan esos efectos a lo largo de los periodos siguientes. Es decir, muestran el impacto de un shock de una magnitud específica en una variable sobre las demás variables en el modelo.\nFunciones de impulso respuesta acumuladas: Estas funciones muestran la respuesta acumulada de las variables a lo largo del tiempo después de un shock en una variable específica. Muestran cómo se acumulan los efectos de un shock en una variable sobre las demás variables en el modelo a lo largo de varios periodos.\n\nAmbos tipos de funciones de impulso respuesta son útiles para analizar cómo se propagan los efectos de un shock en una variable a lo largo del tiempo y cómo afecta a las demás variables en el modelo VAR. Esto permite comprender mejor las interacciones entre las variables y predecir cómo se comportarán en respuesta a cambios en una de ellas.\n\n\n\nLa causalidad de Granger es un concepto importante en el análisis de series temporales que se utiliza para determinar si una serie temporal proporciona información útil para predecir otra serie temporal. Es una herramienta comúnmente utilizada en el contexto de los modelos VAR (Vector Autoregressive).\nAquí está una explicación detallada de la causalidad de Granger en el contexto de los modelos VAR:\n\nDefinición: La causalidad de Granger establece que una serie temporal \\(y_{1t}\\) “Granger-causa” a otra serie temporal \\(y_{2t}\\) si la información pasada de \\(y_{1t}\\) ayuda a predecir \\(y_{2t}\\) mejor que solo utilizando la información pasada de \\(y_{2t}\\).\nPrincipio: Si la serie \\(y_{1t}\\) Granger-causa a la serie \\(y_{2t}\\), entonces los rezagos de \\(y_{1t}\\) se incluirán como predictores en el modelo para predecir \\(y_{2t}\\). En otras palabras, los rezagos de \\(y_{1t}\\) tienen un poder predictivo significativo para \\(y_{2t}\\).\nPrueba de Causalidad de Granger: La causalidad de Granger se evalúa mediante una prueba estadística. En el contexto de los modelos VAR, esta prueba implica ajustar dos modelos:\n\nModelo restringido: Un modelo VAR que solo incluye rezagos de la serie \\(y_{2t}\\) como predictores para predecir \\(y_{2t}\\).\nModelo no restringido: Un modelo VAR que incluye rezagos tanto de la serie \\(y_{2t}\\) como de la serie \\(y_{1t}\\)como predictores para predecir \\(y_{2t}\\).\n\nComparación de Modelos: Después de ajustar ambos modelos, se utiliza una prueba estadística (se utiliza la estadisticaF) para comparar su ajuste. Si el modelo no restringido (que incluye rezagos de \\(y_{1t}\\) ) se ajusta significativamente mejor que el modelo restringido (que no incluye los rezagos de \\(y_{1t}\\)), entonces se concluye que la serie \\(y_{1t}\\) Granger-causa a la serie \\(y_{2t}\\).\nInterpretación: Si se establece que la serie \\(y_{1t}\\) Granger-causa a la serie \\(y_{2t}\\), significa que la información pasada de \\(y_{1t}\\) contiene información adicional que ayuda a predecir \\(y_{2t}\\), más allá de lo que ya se puede predecir con la información pasada de \\(y_{2t}\\).\n\nCausalidad de Granger: \\(y_{1t}\\) granger causa \\(y_{2t}\\) si un modelo que usa valores actuales \\(y_{2t}\\) pasados de \\(y_{1t}\\) y valores actuales y pasados de \\(y_{2t}\\) para predecir valores futuros de \\(y_{2t}\\) tiene un error de pronóstico menor que un modelo que solo usa valores actuales y pasados de \\(y_{2t}\\) para predecir \\(y_{2t}\\). En otras palabras, la causalidad de Granger responde a la siguiente pregunta: ¿ayuda el pasado de la variable \\(y_{1t}\\) a mejorar la predicción de los valores futuros de \\(y_{2t}\\)?\nCausalidad instantánea: \\(y_{1t}\\) causa \\(y_{2t}\\) (en el sentido de Granger instantáneo) si un modelo que usa valores actuales, pasados y futuros de \\(y_{1t}\\) y valores actuales y pasados de \\(y_{2t}\\) para predecir \\(y_{2t}\\) tiene un error de pronóstico menor que un modelo que solo usa valores actuales y pasados de \\(y_{1t}\\) y valores actuales y valores pasados de \\(y_{2t}\\). En otras palabras, la causalidad instantánea de Granger responde a la pregunta: ¿conocer el futuro de \\(y_{1t}\\) me ayuda a predecir mejor el futuro de \\(y_{2t}\\)? Si sé que va a hacer \\(y_{1t}\\), ¿me ayuda a saber lo que va a saber \\(y_{2t}\\)?"
  },
  {
    "objectID": "Proyectos/Modelos VAR.html#introducción-a-los-modelos-var",
    "href": "Proyectos/Modelos VAR.html#introducción-a-los-modelos-var",
    "title": "Taller 6: Modelos de Vectores Autorregresivos (VAR)",
    "section": "",
    "text": "Un modelo de vectores autorregresivos (VAR) es una herramienta econométrica fundamental para analizar la dinámica y las interdependencias entre múltiples series de tiempo. A diferencia de los modelos univariados (como los ARIMA), un VAR modela cada variable en el sistema como una función de sus propios valores pasados y de los valores pasados de todas las demás variables del sistema.\n\nDefinición de VAR: “Los modelos VAR son útiles cuando se está interesado en predecir múltiples series de tiempo y analizar las relaciones dinámicas que existen entre ellas. En un modelo VAR, cada variable es una función lineal de los valores pasados de sí misma y de los valores pasados de las otras variables.”\n— Wooldridge, J. M. (2009). Introductory econometrics: A modern approach (4th ed.).\n\nPor ejemplo, consideremos un sistema con dos series temporales: el PIB y la tasa de desempleo. Un modelo VAR para este sistema podría modelar tanto el PIB como la tasa de desempleo en función de sus valores pasados y de los valores pasados de la otra variable. Esto permitiría analizar cómo el PIB y la tasa de desempleo afectan mutuamente entre sí a lo largo del tiempo.\n\n\nVeamos el modelo estructural dinámico [modelo (1)]:\n\\(y_{1t}= \\alpha_{10} + \\alpha_{11}y_{2t} + \\alpha_{12}y_{1t-1} + \\alpha_{13}y_{2t-1} + \\gamma^{´}_1z_t +\\epsilon_1t\\)\n\\(y_{2t}= \\alpha_{20} + \\alpha_{21}y_{1t} + \\alpha_{22}y_{1t-1} + \\alpha_{23}y_{2t-1} + \\gamma^{´}_2z_t +\\epsilon_2t\\)\nDonde \\(y_{1t}\\) , \\(y_{2t}\\) son variables estacionarias, y \\(\\epsilon_{1t}\\) , \\(\\epsilon_{2t}\\) son procesos ruido blanco con esperanza cero y varianzas \\(\\sigma^{2}_{\\epsilon_{1t}}\\), \\(\\sigma^{2}_{\\epsilon_{2t}}\\) y covarianza \\(\\sigma_{12}\\).\nEl modelo (1) es de ecuaciones simultáneas con dos variables endógenas \\(y_{1t}\\) y \\(y_{2t}\\) y un vector \\(z_t\\) de variables exógenas.\nUn shock sobre \\(y_{2t}\\), en la forma de un valor no nulo de la innovación estructural \\(\\epsilon_{2t}\\), afecta directamente a \\(y_{2t}\\), pero también influye a \\(y_{1t}\\) a través de la presencia de \\(y_{2t}\\) como variable explicativa en la primera ecuación.\nAdemás, este efecto se propaga en el tiempo, debido a la presencia de los valores rezagados de ambas variables como variables explicativas.\nLas variables explicativas exógenas \\(z_t\\) también pueden aparecer con rezagos en el modelo. Por ejemplo, \\(z_t\\) podría ser una tendencia determinista o que recoja la estacionalidad. \\(z_t\\) también puede representar variables tal que \\(E(z_{t−s}{~}\\epsilon_{1t})=𝐸(z_{𝑡-s} ~ \\epsilon_{2𝑡})=0 ~ ∀_𝑠\\). Por ejemplo, el precio de barril de petróleo que se determina en mercados internacionales mientras \\(y_{1t}\\) y \\(y_{2t}\\) son variables de la macroeconmía que se determinan en la economia interna.\nAhora, el Modelo (1) se puede representar de forma matricial de la siguiente forma:\n\\(\\Pi y_t = \\Gamma_0 + \\Gamma_1 y_{t-1} + \\Phi z_t + \\varepsilon_t\\)\nDonde:\n\\(\\Pi = \\begin{equation}\\begin{pmatrix}1 & -\\alpha_{11} \\\\-\\alpha_{21} & 1 \\end{pmatrix}\\end{equation}\\) , \\(\\Gamma_0 =\\begin{equation}\\begin{pmatrix} \\alpha_{10} \\\\\\alpha_{20} \\end{pmatrix}\\end{equation}\\) , \\(\\Gamma_1 = \\begin{equation}\\begin{pmatrix} \\alpha_{12} & \\alpha_{13} \\\\\\alpha_{22} & \\alpha_{23} \\end{pmatrix}\\end{equation}\\) , \\(\\Phi =\\begin{equation}\\begin{pmatrix} \\gamma_{1} \\\\\\gamma_{2} \\end{pmatrix}\\end{equation}\\)\nEste modelo se conoce como VAR estructural y presenta dos problemas:\n\nla simultaneidad, al aparecer cada una de las dos variables como variable explicativa en la ecuación de la otra, lo que genera inconsistencia del estimador MCO, podría resolverse estimando por variables instrumentales, siempre que contemos con instrumentos adecuados, lo cual no es sencillo de justificar. Además, el segundo problema podría persistir.\nsi los términos de error tuviesen autocorrelación, las estimaciones MCO serían inconsistentes, al tratarse de un modelo dinámico se resuelve tratando de ampliar la estructura dinámica del modelo hasta lograr que los términos de error carezcan de autocorrelación.\n\nSupongamos que la matriz \\(\\Pi\\) tiene inversa \\(det(\\Pi) \\neq~ 0\\) , tenemos entonces:\n\\(y_t = \\Pi^{-1} \\Gamma_0 + \\Pi^{-1}\\Gamma_1 y_{t-1} + \\Pi^{-1}\\Phi z_t + \\Pi^{-1} \\varepsilon_t\\)\n\\(y_t = \\Pi^{-1} \\Gamma_0 + \\Pi^{-1}\\Gamma_1 y_{t-1} + \\Pi^{-1}\\Phi z_t + \\Pi^{-1} \\varepsilon_t\\)\n\\(y_t = \\textbf{A}_0 + \\textbf{A}_1 y_{t-1} + \\textbf{M} z_t + u_t\\)\nDonde:\n\\(\\textbf{A}_0 = \\Pi^{-1} \\Gamma_0\\) , \\(\\textbf{A}_1 = \\Pi^{-1}\\Gamma_1\\) , \\(\\textbf{M} = \\Pi^{-1}\\Phi\\) , \\(u_t = \\Pi^{-1} \\varepsilon_t\\)\nAsi, hemos obtenido en modelo de forma reducida o modelo vectorial autoregresivo (VAR) en el cual:\n\\(y_{1t}= \\beta_{10} + \\beta_{11}y_{1t-1} + \\beta_{12}y_{2t-1} + \\textbf{m}_{11}z_t + u_{1t}\\)\n\\(y_{2t}= \\beta_{20} + \\beta_{21}y_{1t-1} + \\beta_{22}y_{2t-1} + \\textbf{m}_{21}z_t + u_{2t}\\)\nEste seria un modelo VAR de orden n en su forma reducida:\n\\(y_{1t}= \\beta_{10} + \\sum_{j=1}^{k}\\beta_{j}y_{t-j} + \\sum_{j=1}^{k}\\textbf{m}_{j}z_{t-j} + u_{1t}\\)\n\\(y_{2t}= \\beta_{20} + \\sum_{j=1}^{k}\\theta_{j}y_{t-j} + \\sum_{j=1}^{k}\\textbf{m}_{j}z_t + u_{2t}\\)\nEn la forma reducida de un modelo VAR (Modelo de Vectores Autorregresivos), las ecuaciones se expresan en términos de las variables endógenas del sistema en función de sus rezagos y, posiblemente, variables exógenas. La estructura de un modelo VAR en su forma reducida se puede describir de la siguiente manera:\n\nVariables Endógenas: Estas son las variables que se están modelando en el sistema. Por ejemplo, si estamos modelando el PIB, la inflación y la tasa de interés, estas serían nuestras variables endógenas.\nRezagos: Cada variable endógena se expresa como una función lineal de sus propios rezagos y de los rezagos de las otras variables endógenas en el sistema. Por ejemplo, la variable endógena \\(y_{1t}\\) en el rezago \\(j\\) se puede expresar como \\(y_{t-j}\\)​.\nVariables Exógenas (opcional): Además de las variables endógenas, el modelo VAR en su forma reducida puede incluir variables exógenas que no están determinadas dentro del sistema, pero que pueden afectar a las variables endógenas. Estas variables pueden incluir datos económicos, políticos o cualquier otro factor relevante.\nParámetros del Modelo: Los parámetros del modelo son los coeficientes que multiplican a los rezagos de las variables endógenas y, posiblemente, a las variables exógenas. Estos parámetros son estimados a partir de los datos y capturan la relación entre las diferentes variables en el sistema.\nError Término (Residuos): El término de error en la forma reducida del modelo VAR captura la parte de la variabilidad de las variables endógenas que no es explicada por los términos autoregresivos y las variables exógenas. Estos errores se suponen que son independientes e idénticamente distribuidos, con una distribución normal. donde las \\(u\\) son los terminos de error estocático, llamados impulsos, innovaciones o choques en el lenguaje VAR.\nLa utilizacion de muchas o muy pocas variables rezagadas puede conducir a un problema de consumo de muchos grados de libertad, la aparicion de la multicolinealidad o errores de especificacion. una forma de decidir esta cuestión es utilizar criterios como el de Akaike o el de Schwarz, para decidir el modelo que proporcione los valores mas bajo de estos.\nEl orden de los modelos VAR está dado por el número de rezagos que se usan en cada ecuación. El modelo descrito anteriormente es entonces un \\(\\textbf{VAR(1)}\\), para denotar también el número de variables se usa\\(\\textbf{VAR}_{2}(1)\\)\n\n\n\n\nUn problema central en el análisis de modelos VAR es encontrar el número de rezagos que produce los mejores resultados. La comparación de modelos generalmente se basa en criterios de información como el Akaike AIC, Bayesiano BIC o Hannan-Quinn HQ, buscando que se minimice el valor del criterio de información.\n\\(AIC=\\frac{−2} lT + \\frac 2pT\\)\n\\(BIC=\\frac {−2}lT+ \\frac {2ln(T)}T\\)\n\\(HQ=\\frac {−2}lT + \\frac {2kln(ln(T))}T\\)\nDonde \\(l=\\frac {−Tk}{2}(1+ln(2π))−\\frac T2ln(|Σ|)\\), y \\(p=k(d+nk)\\) el número de parámetros estimados en el modelo VAR, siendo \\(d\\) es el número de variables exógenas, \\(n\\) el orden del VAR, \\(k\\) el número de variables endógenas.\nPor lo general, el AIC es preferible a otros criterios, debido a sus características favorables de pronóstico de muestras pequeñas. El BIC y HQ, sin embargo, funcionan bien en muestras grandes y tienen la ventaja de ser un estimador consistente, es decir, converge a los valores verdaderos.\n\n\n\nLas funciones de impulso-respuesta (IRF) son una herramienta importante en el análisis de modelos VAR (Vector Autoregressive). Proporcionan información sobre cómo las variables en un sistema responden a los cambios en otras variables a lo largo del tiempo, específicamente en respuesta a un “impulso” o un shock en una de las variables.\nAquí hay una explicación detallada de las funciones de impulso-respuesta en modelos VAR:\n\nDefinición de Impulso-Respuesta: En un modelo VAR, el término “impulso” se refiere a un choque o shock que afecta a una de las variables del sistema. La función de impulso-respuesta describe cómo las otras variables del sistema responden a este impulso en el tiempo.\nCálculo de las IRF: Las IRF se calculan mediante simulación. Una vez estimado el modelo VAR, se introduce un impulso unitario (o un impulso en el nivel deseado) en una de las variables del sistema y se observa cómo las otras variables responden a este impulso a lo largo de múltiples períodos de tiempo.\nInterpretación de las IRF: Las IRF muestran cómo un cambio en una variable afecta a otras variables en el sistema a lo largo del tiempo. Una IRF típicamente muestra cómo la variable endógena (o variable de respuesta) responde al impulso en una variable exógena (o variable de impulso) en diferentes horizontes temporales.\nPropiedades de las IRF:\n\nDirección y Magnitud de la Respuesta: Las IRF muestran si las variables responden positiva o negativamente al impulso, así como la magnitud de esa respuesta.\nPersistencia: Las IRF también indican si el efecto del impulso persiste en el tiempo o disminuye gradualmente.\nEfectos Cruzados: Las IRF muestran cómo los diferentes impulsos afectan a las variables en el sistema, lo que puede ayudar a entender las interacciones entre las variables.\n\nUtilidad de las IRF: Las IRF son útiles para evaluar el impacto de diferentes políticas o choques en una economía, comprender las dinámicas de las variables en un sistema económico y pronosticar el comportamiento futuro de las variables en función de cambios en otras variables.\n\n\n\n\nEn un modelo VAR (Vector Autoregression), se pueden calcular dos tipos de funciones de impulso respuesta:\n\nFunciones de impulso respuesta al impulso unitario: Estas funciones muestran cómo las variables responden a un shock de una desviación estándar en una variable específica en un periodo de tiempo y cómo se propagan esos efectos a lo largo de los periodos siguientes. Es decir, muestran el impacto de un shock de una magnitud específica en una variable sobre las demás variables en el modelo.\nFunciones de impulso respuesta acumuladas: Estas funciones muestran la respuesta acumulada de las variables a lo largo del tiempo después de un shock en una variable específica. Muestran cómo se acumulan los efectos de un shock en una variable sobre las demás variables en el modelo a lo largo de varios periodos.\n\nAmbos tipos de funciones de impulso respuesta son útiles para analizar cómo se propagan los efectos de un shock en una variable a lo largo del tiempo y cómo afecta a las demás variables en el modelo VAR. Esto permite comprender mejor las interacciones entre las variables y predecir cómo se comportarán en respuesta a cambios en una de ellas.\n\n\n\nLa causalidad de Granger es un concepto importante en el análisis de series temporales que se utiliza para determinar si una serie temporal proporciona información útil para predecir otra serie temporal. Es una herramienta comúnmente utilizada en el contexto de los modelos VAR (Vector Autoregressive).\nAquí está una explicación detallada de la causalidad de Granger en el contexto de los modelos VAR:\n\nDefinición: La causalidad de Granger establece que una serie temporal \\(y_{1t}\\) “Granger-causa” a otra serie temporal \\(y_{2t}\\) si la información pasada de \\(y_{1t}\\) ayuda a predecir \\(y_{2t}\\) mejor que solo utilizando la información pasada de \\(y_{2t}\\).\nPrincipio: Si la serie \\(y_{1t}\\) Granger-causa a la serie \\(y_{2t}\\), entonces los rezagos de \\(y_{1t}\\) se incluirán como predictores en el modelo para predecir \\(y_{2t}\\). En otras palabras, los rezagos de \\(y_{1t}\\) tienen un poder predictivo significativo para \\(y_{2t}\\).\nPrueba de Causalidad de Granger: La causalidad de Granger se evalúa mediante una prueba estadística. En el contexto de los modelos VAR, esta prueba implica ajustar dos modelos:\n\nModelo restringido: Un modelo VAR que solo incluye rezagos de la serie \\(y_{2t}\\) como predictores para predecir \\(y_{2t}\\).\nModelo no restringido: Un modelo VAR que incluye rezagos tanto de la serie \\(y_{2t}\\) como de la serie \\(y_{1t}\\)como predictores para predecir \\(y_{2t}\\).\n\nComparación de Modelos: Después de ajustar ambos modelos, se utiliza una prueba estadística (se utiliza la estadisticaF) para comparar su ajuste. Si el modelo no restringido (que incluye rezagos de \\(y_{1t}\\) ) se ajusta significativamente mejor que el modelo restringido (que no incluye los rezagos de \\(y_{1t}\\)), entonces se concluye que la serie \\(y_{1t}\\) Granger-causa a la serie \\(y_{2t}\\).\nInterpretación: Si se establece que la serie \\(y_{1t}\\) Granger-causa a la serie \\(y_{2t}\\), significa que la información pasada de \\(y_{1t}\\) contiene información adicional que ayuda a predecir \\(y_{2t}\\), más allá de lo que ya se puede predecir con la información pasada de \\(y_{2t}\\).\n\nCausalidad de Granger: \\(y_{1t}\\) granger causa \\(y_{2t}\\) si un modelo que usa valores actuales \\(y_{2t}\\) pasados de \\(y_{1t}\\) y valores actuales y pasados de \\(y_{2t}\\) para predecir valores futuros de \\(y_{2t}\\) tiene un error de pronóstico menor que un modelo que solo usa valores actuales y pasados de \\(y_{2t}\\) para predecir \\(y_{2t}\\). En otras palabras, la causalidad de Granger responde a la siguiente pregunta: ¿ayuda el pasado de la variable \\(y_{1t}\\) a mejorar la predicción de los valores futuros de \\(y_{2t}\\)?\nCausalidad instantánea: \\(y_{1t}\\) causa \\(y_{2t}\\) (en el sentido de Granger instantáneo) si un modelo que usa valores actuales, pasados y futuros de \\(y_{1t}\\) y valores actuales y pasados de \\(y_{2t}\\) para predecir \\(y_{2t}\\) tiene un error de pronóstico menor que un modelo que solo usa valores actuales y pasados de \\(y_{1t}\\) y valores actuales y valores pasados de \\(y_{2t}\\). En otras palabras, la causalidad instantánea de Granger responde a la pregunta: ¿conocer el futuro de \\(y_{1t}\\) me ayuda a predecir mejor el futuro de \\(y_{2t}\\)? Si sé que va a hacer \\(y_{1t}\\), ¿me ayuda a saber lo que va a saber \\(y_{2t}\\)?"
  },
  {
    "objectID": "Proyectos/Modelos VAR.html#ejemplo-análisis-de-un-sistema-bivariado-simulado",
    "href": "Proyectos/Modelos VAR.html#ejemplo-análisis-de-un-sistema-bivariado-simulado",
    "title": "Taller 6: Modelos de Vectores Autorregresivos (VAR)",
    "section": "2 Ejemplo: Análisis de un Sistema Bivariado Simulado",
    "text": "2 Ejemplo: Análisis de un Sistema Bivariado Simulado\nPara entender la metodología VAR, utilizaremos datos simulados. Esto nos permite conocer el verdadero proceso generador de datos y evaluar qué tan bien las herramientas econométricas lo recuperan.\n\n2.1 1. Simulación de un Proceso VAR(2)\nGeneramos un sistema de dos variables (V1 y V2) que siguen un proceso VAR(2) estacionario.\n\n\nCode\n# --- Simulación del Modelo ---\nset.seed(123) # Misma semilla para tener los mismos resultados\n\n# Generamos muestra\nt &lt;- 200 # tamaño de la serie\nk &lt;- 2 # Número de variables endogenas\np &lt;- 2 # numero de rezagos\n\n# Generamos matriz de coeficientes\nA.1 &lt;- matrix(c(-.3, .6, -.4, .5), k) # Matriz de coeficientes del rezago 1\nA.2 &lt;- matrix(c(-.1, -.2, .1, .05), k) # Matriz de coeficientes del rezago 2\nA &lt;- cbind(A.1, A.2) # Forma compuesta\n\n# Generamos las series\nseries_matrix &lt;- matrix(0, k, t + 2*p) # Inicio serie con ceros\nfor (i in (p + 1):(t + 2*p)){ # Generamos los errores e ~ N(0,0.5)  \n  series_matrix[, i] &lt;- A.1%*%series_matrix[, i-1] + A.2%*%series_matrix[, i-2] + rnorm(k, 0, .5)\n}\n\nseries &lt;- ts(t(series_matrix[, -(1:p)])) # Convertimos a formato ts\ncolnames(series) &lt;- c(\"V1\", \"V2\") # Renombrar variables\n\n# Convertir a un data frame largo para ggplot2\nplot_data &lt;- as.data.frame(series) %&gt;%\n  mutate(Tiempo = 1:nrow(.)) %&gt;%\n  pivot_longer(cols = c(\"V1\", \"V2\"), names_to = \"Variable\", values_to = \"Valor\")\n\n# Graficos de la serie con ggplot2\nggplot(plot_data, aes(x = Tiempo, y = Valor, color = Variable)) +\n  geom_line() +\n  facet_wrap(~Variable, scales = \"free_y\", ncol = 1) +\n  labs(title = \"Series Simuladas de un Proceso VAR(2)\", x = \"Tiempo\", y = \"Valor\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nInterpretación Gráfica: Las series simuladas parecen fluctuar alrededor de una media de cero, lo que es consistente con un proceso estacionario, como fue diseñado.\n\n\n2.2 2. Pruebas de Raíz Unitaria\nAunque sabemos que las series son estacionarias por construcción, en un caso real verificaríamos esto formalmente.\n\n\nCode\n# Aplicar la prueba de Phillips-Perron a cada columna\napply(series, 2, function(s){\n  ur.pp(s, type = \"Z-tau\", model = \"constant\", lags = \"short\")@teststat\n}) %&gt;%\n  t() %&gt;%\n  kable(caption = \"Estadísticos de Prueba de Phillips-Perron para las Series Simuladas\")\n\n\n\nEstadísticos de Prueba de Phillips-Perron para las Series Simuladas\n\n\nV1\nV2\n\n\n\n\n-17.5378\n-12.42686\n\n\n\n\n\nInterpretación: Los estadísticos de prueba son mucho más negativos que los valores críticos, lo que nos llevaría a rechazar la hipótesis nula de raíz unitaria y confirmar que las series son estacionarias.\n\n\n2.3 3. Selección del Número de Rezagos (p)\nAhora, usamos los criterios de información para ver si podemos recuperar el verdadero orden del proceso, que sabemos es \\(p=2\\).\n\n\nCode\n# Selección de rezagos usando VARselect\nsel &lt;- VARselect(series, lag.max = 6, type = \"none\")\n\n# Presentar los resultados en una tabla\nt(as.matrix(sel$criteria)) %&gt;% \n  kable(caption = \"Criterios de Selección de Rezagos\")\n\n\n\nCriterios de Selección de Rezagos\n\n\nAIC(n)\nHQ(n)\nSC(n)\nFPE(n)\n\n\n\n\n-2.837988\n-2.810904\n-2.771088\n0.0585434\n\n\n-2.922849\n-2.868680\n-2.789049\n0.0537808\n\n\n-2.910680\n-2.829427\n-2.709979\n0.0544408\n\n\n-2.876234\n-2.767896\n-2.608633\n0.0563517\n\n\n-2.843625\n-2.708202\n-2.509123\n0.0582246\n\n\n-2.809024\n-2.646517\n-2.407622\n0.0602823\n\n\n\n\n\nInterpretación: Todos los criterios de información (AIC, HQ, SC y FPE) seleccionan correctamente un orden de \\(p=2\\). Esto demuestra la efectividad de estos criterios cuando el modelo está bien especificado.\n\n\n2.4 4. Estimación y Diagnósticos del VAR\nEstimamos el modelo VAR(2) y realizamos las pruebas de diagnóstico.\n\n\nCode\n# Estimar el modelo VAR(2)\nmodVar &lt;- VAR(series, p = 2, type = \"none\")\n\n\n\n2.4.1 4.1 Prueba de Estabilidad\n\n\nCode\n# Prueba de estabilidad\nroots_modVar &lt;- roots(modVar)\nprint(paste(\"Todas las raíces son menores a 1:\", all(roots_modVar &lt; 1)))\n\n\n[1] \"Todas las raíces son menores a 1: TRUE\"\n\n\nInterpretación: Todas las raíces tienen un módulo menor a 1, por lo que el modelo VAR(2) estimado es estable, como se esperaba.\n\n\n2.4.2 4.2 Diagnósticos sobre los Residuos\n\n\nCode\n# Pruebas de diagnóstico\nserial_test &lt;- serial.test(modVar, lags.bg = 12, type = \"BG\")\narch_test &lt;- arch.test(modVar, lags.multi = 5)\nnormality_test &lt;- normality.test(modVar)\n\n# Calcular el estadístico y p-value de Jarque-Bera combinado\njb_stat &lt;- normality_test$jb.mul$Skewness$statistic + normality_test$jb.mul$Kurtosis$statistic\njb_df &lt;- normality_test$jb.mul$Skewness$parameter + normality_test$jb.mul$Kurtosis$parameter\njb_pval &lt;- 1 - pchisq(jb_stat, df = jb_df)\n\n# Presentar resultados en una tabla\ntibble(\n  Prueba = c(\"Breusch-Godfrey (Autocorrelación)\", \"ARCH-LM (Heterocedasticidad)\", \"Jarque-Bera (Normalidad)\"),\n  `Estadístico` = c(serial_test$serial$statistic, arch_test$arch.mul$statistic, jb_stat),\n  `p-value` = c(serial_test$serial$p.value, arch_test$arch.mul$p.value, jb_pval)\n) %&gt;% kable(caption = \"Pruebas de Diagnóstico Multivariantes sobre los Residuos\")\n\n\n\nPruebas de Diagnóstico Multivariantes sobre los Residuos\n\n\nPrueba\nEstadístico\np-value\n\n\n\n\nBreusch-Godfrey (Autocorrelación)\n43.628478\n0.6524127\n\n\nARCH-LM (Heterocedasticidad)\n40.459685\n0.6644916\n\n\nJarque-Bera (Normalidad)\n4.586017\n0.3324696\n\n\n\n\n\nInterpretación de los Diagnósticos: Todos los p-values son altos (mayores a 0.05), por lo que no rechazamos las hipótesis nulas. Esto indica que los residuos del modelo se comportan como un ruido blanco: no tienen autocorrelación, son homocedásticos y se distribuyen normalmente. El modelo está bien especificado.\n\n\n\n2.5 5. Funciones de Impulso-Respuesta (IRF)\nAnalizamos cómo responde una variable a un shock en la otra. Usamos la descomposición de Cholesky, asumiendo que V1 es contemporáneamente más exógena que V2.\n\n\nCode\n# Calcular y graficar la IRF\nir_v1_v2 &lt;- irf(modVar, \n                impulse = \"V1\", \n                response = \"V2\", \n                ortho = TRUE, # Usar descomposición de Cholesky\n                boot = TRUE, runs = 1000, ci = 0.95)\n\nplot(ir_v1_v2)\n\n\n\n\n\nRespuesta de V2 a un shock en V1.\n\n\n\n\nInterpretación: El gráfico muestra la respuesta de V2 a un shock positivo de una desviación estándar en V1. Se observa un efecto positivo y significativo en V2 durante los primeros periodos, que luego se disipa y converge a cero, como es de esperar en un sistema estacionario.\n\n\n2.6 6. Causalidad de Granger\nFinalmente, evaluamos si los rezagos de una variable ayudan a predecir a la otra.\n\n\nCode\n# Realizar pruebas de causalidad para cada variable\ncausality_results &lt;- list()\nfor(v in colnames(series)){\n  causality_results[[v]] &lt;- causality(modVar, cause = v)\n}\n\n# Presentar un resumen\ntibble(\n  Causa = names(causality_results),\n  `p-value Granger` = map_dbl(causality_results, ~ .x$Granger$p.value),\n  `p-value Instantánea` = map_dbl(causality_results, ~ .x$Instant$p.value)\n) %&gt;% kable(caption = \"Resultados de las Pruebas de Causalidad de Granger\")\n\n\n\nResultados de las Pruebas de Causalidad de Granger\n\n\nCausa\np-value Granger\np-value Instantánea\n\n\n\n\nV1\n0.00e+00\n0.0503066\n\n\nV2\n1.03e-05\n0.0503066\n\n\n\n\n\nInterpretación: Ambos p-values para la causalidad de Granger son muy bajos (menores a 0.05). Esto significa que rechazamos la hipótesis nula en ambos casos. Concluimos que los rezagos de V1 ayudan a predecir V2, y los rezagos de V2 ayudan a predecir V1. Existe una causalidad bidireccional, lo cual es consistente con la forma en que construimos nuestras matrices de coeficientes."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html",
    "href": "Proyectos/Repaso Tercer Parcial.html",
    "title": "Repaso Tercer Parcial",
    "section": "",
    "text": "Una serie de tiempo es una secuencia de datos recogidos y ordenados en intervalos de tiempo regulares. Estos datos pueden representar cualquier tipo de variable económica, como precios de acciones, producto interno bruto (PIB), tasas de desempleo, entre otros.\n\\[Y_t, Y_ {t-1}, Y_{t-2}, … Y_{t-k}, Y_{t+1}, Y_{t+2}, … Y_{t+h}\\]\n\nSerie estocástica: una parte conocida (sistemática) susceptible de predecir y de una parte totalmente desconocida (aleatoria)\nSerie determinística: el futuro se puede predecir sin error Es una variable que está determinada o fija y que no cambia de una muestra a otra\n\n\n\nLas series de tiempo pueden descomponerse en varios componentes fundamentales:\n\nTendencia (T): Movimiento general y a largo plazo en los datos.\n\n\n\n\nSerie con tendencia\n\n\n\nEstacionalidad (S): Patrones repetitivos y predecibles en los datos dentro de un año o cualquier período fijo.\n\n\n\n\nSerie con Estacionalidad\n\n\n\nCiclo (C): Fluctuaciones que ocurren en el largo plazo, relacionadas con el ciclo económico.\n\n\n\n\nSerie con tendencia\n\n\n\nComponentes Aleatorios (E): Variaciones irregulares y no predecibles en los datos.\n\n\n\n\nSerie con aleatoria\n\n\n\n\n\n\nSi analizamos el PIB de un país durante 20 años, podríamos observar una tendencia ascendente debido al crecimiento económico sostenido.\n\n\n\nLas ventas de ropa pueden mostrar estacionalidad, con picos en invierno y verano, y bajas en primavera y otoño.\n\n\n\nLa economía puede experimentar ciclos de auge y recesión que duran varios años, reflejando períodos de expansión y contracción.\n\n\n\nUn evento inesperado, como un desastre natural, puede introducir fluctuaciones aleatorias en una serie de tiempo de precios de productos agrícolas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponente\nDescripción\nEjemplo\n\n\n\n\nTendencia (T)\nMovimiento general a largo plazo\nCrecimiento del PIB a lo largo de las décadas\n\n\nEstacionalidad (S)\nPatrones repetitivos en períodos fijos\nVentas de helados más altas en verano\n\n\nCiclo (C)\nFluctuaciones de largo plazo\nCiclos económicos de expansión y recesión\n\n\nAleatorio (E)\nVariaciones irregulares y no predecibles\nImpacto de un terremoto en la producción agrícola"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#serie-de-tiempo",
    "href": "Proyectos/Repaso Tercer Parcial.html#serie-de-tiempo",
    "title": "Repaso Tercer Parcial",
    "section": "",
    "text": "Una serie de tiempo es una secuencia de datos recogidos y ordenados en intervalos de tiempo regulares. Estos datos pueden representar cualquier tipo de variable económica, como precios de acciones, producto interno bruto (PIB), tasas de desempleo, entre otros.\n\\[Y_t, Y_ {t-1}, Y_{t-2}, … Y_{t-k}, Y_{t+1}, Y_{t+2}, … Y_{t+h}\\]\n\nSerie estocástica: una parte conocida (sistemática) susceptible de predecir y de una parte totalmente desconocida (aleatoria)\nSerie determinística: el futuro se puede predecir sin error Es una variable que está determinada o fija y que no cambia de una muestra a otra\n\n\n\nLas series de tiempo pueden descomponerse en varios componentes fundamentales:\n\nTendencia (T): Movimiento general y a largo plazo en los datos.\n\n\n\n\nSerie con tendencia\n\n\n\nEstacionalidad (S): Patrones repetitivos y predecibles en los datos dentro de un año o cualquier período fijo.\n\n\n\n\nSerie con Estacionalidad\n\n\n\nCiclo (C): Fluctuaciones que ocurren en el largo plazo, relacionadas con el ciclo económico.\n\n\n\n\nSerie con tendencia\n\n\n\nComponentes Aleatorios (E): Variaciones irregulares y no predecibles en los datos.\n\n\n\n\nSerie con aleatoria\n\n\n\n\n\n\nSi analizamos el PIB de un país durante 20 años, podríamos observar una tendencia ascendente debido al crecimiento económico sostenido.\n\n\n\nLas ventas de ropa pueden mostrar estacionalidad, con picos en invierno y verano, y bajas en primavera y otoño.\n\n\n\nLa economía puede experimentar ciclos de auge y recesión que duran varios años, reflejando períodos de expansión y contracción.\n\n\n\nUn evento inesperado, como un desastre natural, puede introducir fluctuaciones aleatorias en una serie de tiempo de precios de productos agrícolas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponente\nDescripción\nEjemplo\n\n\n\n\nTendencia (T)\nMovimiento general a largo plazo\nCrecimiento del PIB a lo largo de las décadas\n\n\nEstacionalidad (S)\nPatrones repetitivos en períodos fijos\nVentas de helados más altas en verano\n\n\nCiclo (C)\nFluctuaciones de largo plazo\nCiclos económicos de expansión y recesión\n\n\nAleatorio (E)\nVariaciones irregulares y no predecibles\nImpacto de un terremoto en la producción agrícola"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#modelos-de-regresión-para-series-de-tiempo",
    "href": "Proyectos/Repaso Tercer Parcial.html#modelos-de-regresión-para-series-de-tiempo",
    "title": "Repaso Tercer Parcial",
    "section": "Modelos de Regresión para Series de Tiempo",
    "text": "Modelos de Regresión para Series de Tiempo\n\nConceptos Clave\n\nAutocorrelación\n\nDefinición: La autocorrelación es la correlación de una serie de tiempo con sus propios valores rezagados. Esto significa que los valores anteriores de la serie pueden influir en los valores futuros.\n\nEjemplo: Si el precio de una acción hoy depende del precio de la acción en días anteriores, estamos viendo autocorrelación.\n\n\n\nLa correlación indica dos aspectos:\n\nEl valor indica la magnitud de la asociación (-1 y 1)\nEl signo indica la dirección de la relación\n\nNegativa: cuando los valores de \\(t\\) aumentan los de \\(t+k\\) disminuyen\nCero: No hay relación armónica en como los valores de \\(t\\) y \\(t+k\\) cambian\nPositiva: cuando los valores de \\(t\\) aumentan, los valores de \\(t+k\\) también aumentan\n\\[r_{k}= \\frac{\\sum_{t=1}^{n-k}(Y_{t}-\\bar{Y})(Y_{t+k}-\\bar{Y})}{\\sum_{t=1}^{n}(Y_{t}-\\bar{Y})^{2}}= \\frac{Cov_{k}}{S^{2}}\\]\n\nModelo Autorregresivo (AR)\n\nDefinición: Un modelo autorregresivo usa los valores pasados de la variable dependiente para predecir su valor futuro. Se denota como AR(p), donde p es el número de rezagos.\nEcuación: \\[Y_t = \\phi_0 + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + ... + \\phi_p Y_{t-p} + \\epsilon_t\\]​\nEjemplo: Un AR(1) para el PIB puede ser \\(PIB_t = \\phi_0 + \\phi_1 PIB_{t-1} + \\epsilon_t\\).\n\nModelo de Media Móvil (MA)\n\nDefinición: Un modelo de media móvil utiliza los errores pasados para predecir el valor futuro de la variable dependiente. Se denota como MA(q), donde q es el número de rezagos de los errores.\nEcuación: \\[Y_t = \\mu + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ... + \\theta_q \\epsilon_{t-q} + \\epsilon_t\\]​​\nEjemplo: Un MA(1) para las tasas de inflación puede ser \\[\\text{Inflación}_t = \\mu + \\theta_1 \\epsilon_{t-1} + \\epsilon_t\\].\n\nModelo Autorregresivo de Media Móvil (ARMA)\n\nDefinición: Combina los modelos AR y MA para capturar tanto la autocorrelación como las relaciones de media móvil en una serie de tiempo. Se denota como ARMA(p,q).\nEcuación: \\[Y_t = \\phi_0 + \\phi_1 Y_{t-1} + ... + \\phi_p Y_{t-p} + \\theta_1 \\epsilon_{t-1} + ... + \\theta_q \\epsilon_{t-q} + \\epsilon_t\\]\n\n\n\n\nEjemplos\n\nEjemplo 1: Modelo AR(2)\nSupongamos que estamos modelando el consumo de energía en función de sus valores pasados: \\[\\text{Energía}_{t} = 0.5 + 0.6 \\text{Energía}_{t-1} + 0.3 \\text{Energía}_{t-2} + \\epsilon_t\\]​\n\n\nEjemplo 2: Modelo MA(1)\nSi queremos modelar el índice de precios al consumidor (IPC) teniendo en cuenta el error del mes pasado: \\[\\text{IPC}_{t} = 2 + 0.8 \\epsilon_{t-1} + \\epsilon_{t}\\]\n\n\nEjemplo 3: Modelo ARMA(1,1)\nPara modelar el PIB considerando tanto el valor pasado del PIB como el error del período anterior: \\[PIB_t = 3 + 0.7 PIB_{t-1} + 0.5 \\epsilon_{t-1} + \\epsilon_t\\]​\n\n\n\nCuadro Comparativo de Modelos\n\n\n\n\n\n\n\n\n\nModelo\nEcuación\nCaracterísticas\nUso Principal\n\n\n\n\nAR(p)\n\\[Y_t = \\phi_0 + \\sum_{i=1}^p \\phi_i Y_{t-i} + \\epsilon_t\\]\nCaptura autocorrelación\nSeries con fuerte dependencia temporal\n\n\nMA(q)\n\\[Y_t = \\mu + \\sum_{i=1}^q \\theta_i \\epsilon_{t-i} + \\epsilon_t\\]\nCaptura dependencia en errores\nSeries con fluctuaciones irregulares\n\n\nARMA(p,q)\n\\[Y_t = \\phi_0 + \\sum_{i=1}^p \\phi_i Y_{t-i} + \\sum_{j=1}^q \\theta_j \\epsilon_{t-j} + \\epsilon_t\\]\nCombina AR y MA\nSeries con patrones complejos"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#conceptos-ampliados",
    "href": "Proyectos/Repaso Tercer Parcial.html#conceptos-ampliados",
    "title": "Repaso Tercer Parcial",
    "section": "Conceptos Ampliados",
    "text": "Conceptos Ampliados\n\nAutocorrelación y Función de Autocorrelación (ACF)\n\nAutocorrelación: La correlación de una serie de tiempo con sus propios valores rezagados.\nFunción de Autocorrelación (ACF): Mide la autocorrelación de la serie de tiempo en diferentes rezagos. Ayuda a identificar patrones y a determinar la estructura del modelo ARMA.\n\n\n\nFunción de Autocorrelación Parcial (PACF)\n\nPACF: Mide la correlación entre la serie de tiempo y sus rezagos, eliminando el efecto de los rezagos intermedios. Es útil para identificar el orden p de un modelo AR(p).\n\n\n\n\nACF Y ACFP\n\n\n\n\nIdentificación del Modelo\n\nACF y PACF: Usadas para determinar los órdenes p y q de los modelos AR(p), MA(q) y ARMA(p,q).\n\nAR(p): La PACF muestra un corte brusco después del rezago p.\nMA(q): La ACF muestra un corte brusco después del rezago q.\nARMA(p,q): Tanto la ACF como la PACF decaen exponencialmente.\n\n\n\n\nEstimación de Parámetros\n\nMétodos: Máxima Verosimilitud y Mínimos Cuadrados.\nDesafíos: Complejidad debido a la dependencia temporal y problemas de no estacionariedad.\n\n\n\nDiagnóstico del Modelo\n\nResiduales: Deben comportarse como ruido blanco después del ajuste del modelo.\nPruebas: Ljung-Box para verificar si los residuales son ruido blanco.\n\n\n\nCriterios de Selección del Modelo\n\nAIC (Akaike Information Criterion) y BIC (Bayesian Information Criterion): Usados para seleccionar el modelo que mejor se ajusta a los datos, penalizando la complejidad del modelo.\n\n\n\nPronósticos\n\nMetodología: Uso de modelos ajustados para predecir valores futuros.\nEvaluación del Pronóstico: Métricas como MSE (Mean Squared Error) y MAE (Mean Absolute Error) para evaluar la precisión de los pronósticos.\n\n\n\nResumen en Tabla\n\n\n\n\n\n\n\n\nConcepto\nDefinición/Descripción\nUso Principal\n\n\n\n\nAutocorrelación\nCorrelación de una serie de tiempo con sus propios valores rezagados.\nIdentificación de patrones temporales\n\n\nFunción de Autocorrelación (ACF)\nMide la autocorrelación en diferentes rezagos.\nDeterminación de la estructura del modelo\n\n\nFunción de Autocorrelación Parcial (PACF)\nMide la correlación entre la serie y sus rezagos, eliminando efectos intermedios.\nIdentificación del orden p en AR(p)\n\n\nIdentificación del Modelo\nUso de ACF y PACF para determinar los órdenes p y q de AR(p), MA(q) y ARMA(p,q).\nSelección de modelo adecuado\n\n\nEstimación de Parámetros\nMétodos de Máxima Verosimilitud y Mínimos Cuadrados.\nAjuste del modelo\n\n\nDiagnóstico del Modelo\nAnálisis de residuales; uso de pruebas como Ljung-Box.\nVerificación de la adecuación del modelo\n\n\nCriterios de Selección del Modelo\nAIC y BIC para seleccionar el modelo que mejor se ajusta, penalizando la complejidad.\nOptimización del modelo\n\n\nPronósticos\nUso de modelos ajustados para predecir valores futuros; evaluación con MSE y MAE.\nPredicción y evaluación de precisión"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#estacionariedad-y-pruebas-de-raíz-unitaria",
    "href": "Proyectos/Repaso Tercer Parcial.html#estacionariedad-y-pruebas-de-raíz-unitaria",
    "title": "Repaso Tercer Parcial",
    "section": "Estacionariedad y Pruebas de Raíz Unitaria",
    "text": "Estacionariedad y Pruebas de Raíz Unitaria\n\nDefinición de Estacionariedad\n\nSerie Estacionaria: Una serie de tiempo es estacionaria si sus propiedades estadísticas, como la media, la varianza y la autocorrelación, son constantes a lo largo del tiempo. Esto significa que los patrones de comportamiento de la serie no dependen del tiempo en que se observan.\nSerie No Estacionaria: Una serie cuya media, varianza o autocorrelación cambian con el tiempo. Estas series pueden tener tendencias, estacionalidades o varianzas que evolucionan con el tiempo.\n\n\n\nTipos de Estacionariedad\n\nEstacionariedad Estricta: Una serie de tiempo es estrictamente estacionaria si la distribución conjunta de cualquier conjunto de valores de la serie es la misma independientemente del tiempo en que se observe.\nEstacionariedad Débil (o Covarianza Estacionaria): Una serie es débilmente estacionaria si la media, la varianza y la autocorrelación son constantes a lo largo del tiempo.\n\n\n\nImportancia de la Estacionariedad\n\nModelado: Los modelos ARMA (AutoRegressive Moving Average) requieren que las series sean estacionarias. Si la serie no es estacionaria, se deben aplicar transformaciones como la diferenciación.\nPronósticos: Las series estacionarias son más predecibles, ya que sus propiedades no cambian con el tiempo.\n\n\n\nPruebas de Raíz Unitaria\n\nDefinición de Raíz Unitaria\n\nUna serie de tiempo tiene una raíz unitaria si uno de los coeficientes del polinomio característico es igual a uno, lo que indica que la serie es no estacionaria. Esto implica que los efectos de un shock en la serie no se disipan con el tiempo y la serie sigue una caminata aleatoria.\n\n\n\nPruebas de Raíz Unitaria\n\nPrueba de Dickey-Fuller (DF)\n\nObjetivo: Determinar si una serie de tiempo tiene una raíz unitaria.\nHipótesis:\n\n\\(H_0\\)​: La serie tiene una raíz unitaria (no estacionaria).\n\\(H_1\\)​: La serie no tiene una raíz unitaria (estacionaria).\n\nProceso: Ajuste del modelo \\[ y_t = \\alpha y_{t-1} + \\epsilon_t\\]. Si \\(\\alpha = 0\\), hay una raíz unitaria.\n\nPrueba de Dickey-Fuller Aumentada (ADF)\n\nExtensión: Incluye términos rezagados adicionales para corregir la autocorrelación en los errores.\nModelo: \\[\\Delta y_t = \\alpha y_{t-1} + \\sum_{i=1}^p \\beta_i \\Delta y_{t-i} + \\epsilon_t\\].\nImportancia: Es más robusta que la prueba DF básica porque maneja la autocorrelación en los residuales, lo cual es crucial para obtener resultados confiables.\n\nPrueba de Phillips-Perron (PP)\n\nAlternativa: Similar a ADF, pero corrige la heterocedasticidad y autocorrelación en los errores sin agregar términos rezagados.\nMétodo: Utiliza estimadores no paramétricos para ajustar las varianzas asintóticas.\nVentaja: Es útil en casos donde se sospecha de heterocedasticidad en los errores y es más flexible que ADF en ciertos contextos.\n\n\n\n\n\nConceptos Clave\n\nTransformaciones para Estacionariedad\n\nDiferenciación: Aplicar la diferencia de primer orden (\\(\\Delta y_t = y_t - y_{t-1}\\)) o de orden superior para eliminar tendencias. La diferenciación puede convertir una serie no estacionaria en estacionaria.\nTransformación Logarítmica: Usada para estabilizar la varianza en series que exhiben varianza no constante.\n\n\n\nComponentes de una Serie No Estacionaria\n\nTendencia: Componente determinista o estocástico que aumenta o disminuye con el tiempo. Puede ser eliminada mediante diferenciación o detrending.\nEstacionalidad: Componentes que se repiten a intervalos regulares, como anualmente, trimestralmente, etc. Puede ser ajustada utilizando técnicas como desestacionalización.\nCiclo: Fluctuaciones a largo plazo alrededor de una tendencia. Identificar los ciclos ayuda a entender los movimientos a largo plazo de la serie.\n\n\n\nImportancia del Diagnóstico\n\nAnálisis de Residuales: Después de ajustar un modelo ARIMA, es crucial examinar los residuales para asegurarse de que se comportan como ruido blanco. Si no es así, el modelo puede no ser adecuado.\nPruebas de Diagnóstico: Pruebas como Ljung-Box se utilizan para verificar la autocorrelación en los residuales.\n\n\n\n\nResumen en Tabla\n\n\n\n\n\n\n\n\nConcepto\nDefinición/Descripción\nImportancia/Aplicación\n\n\n\n\nSerie Estacionaria\nSerie con propiedades estadísticas constantes a lo largo del tiempo.\nModelado y pronósticos precisos.\n\n\nSerie No Estacionaria\nSerie con media, varianza o autocorrelación cambiantes con el tiempo.\nRequiere transformaciones.\n\n\nEstacionariedad Estricta\nDistribución conjunta de valores de la serie es constante en el tiempo.\nAnálisis teórico.\n\n\nEstacionariedad Débil\nMedia, varianza y autocorrelación son constantes en el tiempo.\nModelos ARMA.\n\n\nRaíz Unitaria\nUn coeficiente del polinomio característico es igual a uno; indica no estacionariedad.\nIdentificación de no estacionariedad.\n\n\nPrueba de Dickey-Fuller (DF)\nPrueba para detectar raíz unitaria.\nIdentificación de no estacionariedad.\n\n\nPrueba de ADF\nExtensión de DF con términos rezagados para autocorrelación.\nMás robusta que DF.\n\n\nPrueba de PP\nCorrige heterocedasticidad y autocorrelación sin términos rezagados.\nAlternativa a ADF.\n\n\nDiferenciación\nTransformación para eliminar tendencias.\nEstacionarización de la serie.\n\n\nTransformación Logarítmica\nUsada para estabilizar la varianza.\nPreprocesamiento de datos.\n\n\nTendencia\nComponente de la serie que aumenta o disminuye con el tiempo.\nIdentificación de patrones.\n\n\nEstacionalidad\nComponentes que se repiten a intervalos regulares.\nAnálisis de patrones recurrentes.\n\n\nCiclo\nFluctuaciones a largo plazo alrededor de una tendencia.\nAnálisis de fluctuaciones de largo plazo.\n\n\nAnálisis de Residuales\nVerificación de que los residuales de un modelo se comportan como ruido blanco.\nValidación del modelo.\n\n\nPruebas de Diagnóstico\nPruebas como Ljung-Box para verificar la autocorrelación en los residuales.\nVerificación de la idoneidad del modelo."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#metodología-box-jenkins-para-pronosticar-series-de-tiempo",
    "href": "Proyectos/Repaso Tercer Parcial.html#metodología-box-jenkins-para-pronosticar-series-de-tiempo",
    "title": "Repaso Tercer Parcial",
    "section": "Metodología Box-Jenkins para Pronosticar Series de Tiempo",
    "text": "Metodología Box-Jenkins para Pronosticar Series de Tiempo\nLa metodología Box-Jenkins es un enfoque sistemático para identificar, estimar y verificar modelos ARIMA (AutoRegressive Integrated Moving Average) aplicables a series temporales estacionarias o no estacionarias. Este método, desarrollado por George Box y Gwilym Jenkins, sigue varios pasos: identificación, estimación, diagnóstico y pronóstico.\n\nPasos de la Metodología Box-Jenkins:\n\nIdentificación:\n\nDeterminar si la serie temporal es estacionaria o no.\nUsar gráficos (como el correlograma) y pruebas estadísticas (como la prueba de Dickey-Fuller) para verificar la estacionariedad.\nIdentificar el orden del modelo ARIMA (p, d, q) utilizando el correlograma y el parciorregresograma.\n\nEstación de Modelos:\n\nSeleccionar los valores iniciales de p, d y q basados en los gráficos de autocorrelación (ACF) y autocorrelación parcial (PACF).\nUtilizar métodos de estimación como Máxima Verosimilitud para ajustar los parámetros del modelo.\n\nDiagnóstico:\n\nEvaluar la adecuación del modelo ajustado mediante el análisis de los residuales.\nUtilizar pruebas estadísticas como la prueba de Ljung-Box para verificar la ausencia de autocorrelación en los residuales.\nAjustar y refinar el modelo si los residuales no se comportan como ruido blanco.\n\nPronóstico:\n\nUsar el modelo ajustado para realizar pronósticos futuros.\nEvaluar la precisión de los pronósticos y ajustar el modelo si es necesario.\n\n\n\n\nTeoría Matemática:\n\nModelo ARIMA(p,d,q): Este modelo combina las partes AR (AutoRegresiva), I (Integrada) y MA (Media Móvil).\n\\[Y_t = \\phi_0 + \\sum_{i=1}^{p} \\phi_i Y_{t-i} + \\sum_{i=1}^{q} \\theta_i \\epsilon_{t-i} + \\epsilon_t\\]\nDonde:\n\n\\(Y_t\\)​ es la serie temporal en el tiempo t.\n\\(\\phi_i\\)​ son los coeficientes del modelo AR.\n\\(\\theta_i\\)​ son los coeficientes del modelo MA.\n\\(\\epsilon_t\\) es el término de error en el tiempo t.\n\\(p\\) es el orden del modelo AR.\n\\(q\\) es el orden del modelo MA.\n\\(d\\) es el número de diferenciaciones necesarias para hacer la serie estacionaria.\n\n\n\n\nTabla Resumen:\n\n\n\n\n\n\n\n\nPaso\nDescripción\nHerramientas/Pruebas\n\n\n\n\nIdentificación\nDeterminar estacionariedad y seleccionar orden inicial del modelo ARIMA.\nCorrelograma, Prueba Dickey-Fuller\n\n\nEstimación\nAjustar los parámetros del modelo utilizando métodos estadísticos.\nMáxima Verosimilitud\n\n\nDiagnóstico\nVerificar la adecuación del modelo evaluando los residuales.\nPrueba Ljung-Box, Análisis de Residuales\n\n\nPronóstico\nUtilizar el modelo ajustado para realizar predicciones futuras y evaluar su precisión.\nForecasting, Evaluación de Pronósticos"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#prueba-de-cointegración-para-series-de-tiempo",
    "href": "Proyectos/Repaso Tercer Parcial.html#prueba-de-cointegración-para-series-de-tiempo",
    "title": "Repaso Tercer Parcial",
    "section": "Prueba de Cointegración para Series de Tiempo",
    "text": "Prueba de Cointegración para Series de Tiempo\nLa prueba de cointegración es utilizada para determinar si existe una relación de equilibrio a largo plazo entre dos o más series temporales no estacionarias. Conceptualmente, si dos o más series temporales están cointegradas, significa que aunque individualmente pueden ser no estacionarias, alguna combinación lineal de ellas es estacionaria. Esta relación de equilibrio a largo plazo es de interés económico y financiero.\n\nConceptual:\n\nCointegración: Si dos series temporales \\(X_t\\)​ e \\(Y_t\\)​ son integradas de orden 1 (I(1)), y existe un coeficiente \\(\\beta\\) tal que la combinación lineal \\(X_t - \\beta Y_t\\) es estacionaria (I(0)), entonces \\(X_t\\)​ e \\(Y_t\\)​ están cointegradas.\n\n\n\nMatemáticamente:\nPara dos series temporales \\(X_t\\)​ e \\(Y_t\\)​:\n\nSi \\(Xt​∼I(1)\\) y \\(Y_t \\sim I(1)\\).\nExiste \\(\\beta\\) tal que \\(Z_t = X_t - \\beta Y_t \\sim I(0)\\).\n\n\n\nMétodos de Prueba de Cointegración\n\n1. Método de Engle y Granger:\nEl método de Engle y Granger se utiliza para probar la cointegración entre dos series temporales.\n\nPaso 1: Realizar una regresión de \\(Y_t\\)​ sobre \\(X_t\\)​:\n\\[Y_t = \\alpha + \\beta X_t + \\epsilon_t\\]\nPaso 2: Obtener los residuos \\(\\epsilon_t\\)​ de la regresión.\nPaso 3: Aplicar una prueba de raíz unitaria (como la prueba de Dickey-Fuller) a los residuos \\(\\epsilon_t\\)​.\n\nSi los residuos \\(\\epsilon_t\\)​ son estacionarios, \\(Y_t\\)​ y \\(X_t\\)​ están cointegrados.\n\n\n\n\n2. Prueba de Johansen:\nLa prueba de Johansen se utiliza para determinar el número de vectores de cointegración en un sistema de múltiples series temporales.\n\nModelo VAR: Formar un modelo Vector Autoregresivo (VAR).\nModelo VECM: Transformar el modelo VAR en un Modelo de Corrección de Errores Vectorial (VECM):\n\\[\\Delta Y_t = \\Pi Y_{t-1} + \\sum_{i=1}^{k-1} \\Gamma_i \\Delta Y_{t-i} + \\epsilon_t\\]\nDonde \\(\\Pi\\) y \\(\\Gamma_i\\) son matrices de parámetros.\nTest de Rango de la Matriz \\(\\Pi\\): Evaluar el rango de la matriz \\(\\Pi\\) para determinar el número de vectores de cointegración.\n\nPrueba Trace: Evalúa el rango de \\(\\Pi\\) mediante la traza.\nPrueba Máximo Eigenvalor: Evalúa el mayor eigenvalor de \\(\\Pi\\).\n\n\n\n\n3. Prueba de Phillips-Ouliaris:\nEl método de Phillips-Ouliaris es una prueba de cointegración residual similar al método de Engle y Granger, pero con ajustes para la no estacionariedad y la autocorrelación en los residuos.\n\nRegresión de Cointegración:\n\nRealizar una regresión entre las series temporales.\n\nResiduos Ajustados:\n\nObtener los residuos y ajustar por autocorrelación.\n\nPrueba de Raíz Unitaria:\n\nAplicar la prueba de raíz unitaria ajustada a los residuos.\n\n\n\n\n\nTabla Comparativa de Métodos de Cointegración\n\n\n\n\n\n\n\n\n\nCaracterística\nMétodo de Engle y Granger\nPrueba de Johansen\nPrueba de Phillips-Ouliaris\n\n\n\n\nSeries Temporales\nBivariada (dos series)\nMultivariada (más de dos series)\nBivariada (dos series)\n\n\nEnfoque\nResidual\nVector Autoregresivo (VAR/VECM)\nResidual\n\n\nPasos Principales\nRegresión, Residuos, Prueba de Raíz Unitaria\nModelo VAR, Transformación VECM, Prueba de Rango\nRegresión, Residuos Ajustados, Prueba de Raíz Unitaria\n\n\nTipo de Prueba\nPrueba de Dickey-Fuller sobre residuos\nPrueba Trace y Máximo Eigenvalor\nPrueba de raíz unitaria ajustada\n\n\nConsidera Autocorrelación\nNo explícitamente\nSí\nSí\n\n\nComplejidad\nRelativamente Simple\nMás Complejo\nIntermedio\n\n\nAplicabilidad\nDos series, análisis sencillo\nMúltiples series, análisis avanzado\nDos series, robusto a autocorrelación"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#modelo-de-correción-de-errores-mce",
    "href": "Proyectos/Repaso Tercer Parcial.html#modelo-de-correción-de-errores-mce",
    "title": "Repaso Tercer Parcial",
    "section": "Modelo de Correción de Errores MCE",
    "text": "Modelo de Correción de Errores MCE\nEl modelo de corrección de errores me permite probar el equilibrio en el corto plazo sabiendo que existe un equilibrio en el largo plazo (demostrado a traves de una prueba de cointegracion). Por supuesto en el corto plazo, puede haber desequilibrio. En consecuencia puede tratarse el termino de error, como error de eqilibrio. Y se puede utilizar este termino de error para atar el comportamiento de corto plazo con el de largo plazo. El ECM corrige el desequilibrio. Engle y Granger (1987) establece una equivalencia entre los conceptos de cointegración y modelo ECM, en cuanto cointegración implica un modelo de ECM y a la vez un modelo de ECM implica cointegracion (teorema de representación). para entenderlo consideremos dos series de tiempo escalares \\(Y_t\\) y \\(X_t\\), ambas \\(L(1)\\). Un modelo autorregresivo distribuidos de rezagos (ADL) de orden (1, 1) de estas series es:\n\\[Y_t = \\delta + \\theta Y_{t-1} + \\delta_0X_t + \\delta_1X_{t-1} + \\epsilon_t\\]\ndonde:\n\\[\\epsilon_t \\overset{iid}{\\sim} N(0,1)\\]\n\\[\\delta + \\theta Y_{t-1}\\] = autoregresivo\n\\[\\delta_0X_t + \\delta_1X_{t-1}\\] = rezago distribuido\n\\[\\epsilon_t\\] = termino de error\nSe tiene que cumplir estas condiciones para encontrar la relación de largo plazo\n\\[X \\sim I(1)\\]\n\\[Y \\sim I(1)\\]\n\\[\\epsilon \\sim I(0)\\]\nPero pese a que \\(\\epsilon_t\\) sea estacionario, aún puede presentar autocorrelación. Una posible solución es incluir términos rezagados para obtener residuos que no tengan autocorrelación. Por ejemplo:\nIntroducimos en el ARDL\n\\[Y_t = Y_{t-1} = Y\\]\n\\[X_t = X_{t-1} = X\\]\n\\[\\epsilon_t = 0\\]\n\nReparametrización de la Ecuación ADL\nPuedo reescribir el ARDL (1, 1)\n\\[Y = \\delta + \\theta_1 Y+ \\delta_0X + \\delta_1X\\]\n\\[(1 - \\theta_1) Y = \\delta + (\\delta_0 + \\delta_1)X\\]\n\\[Y = \\frac{\\delta}{(1 - \\theta_1)} + \\frac{(\\delta_0 + \\delta_1)}{(1 - \\theta_1)}X\\]\n\\[Y = \\beta_1 + \\beta_2X\\]\nDonde:\n\\[\\beta_1 = \\frac{\\delta}{(1 - \\theta_1)}\\]\n\\[\\beta_2 = \\frac{(\\delta_0 + \\delta_1)}{(1 - \\theta_1)}\\]\nLa primera diferencia es un cambio del corto plazo\n\\[\\Delta Y_t = Y_t - Y_{t-1}\\]\n\\[\\Delta X_t = X_t - X_{t-1}\\]\nRestar a ambos lados \\(Y_{t-1}\\) y restando \\(\\delta_0 X_{t-1}\\) (manipulación algebraica)\n\\[Y_t = \\delta + \\theta Y_{t-1} + \\delta_0X_t + \\delta_1X_{t-1} + \\epsilon_t\\]\n\\[Y_t - Y_{t-1} = \\delta + \\theta_1 Y_{t-1} - Y_{t-1} + \\delta_0X_t + \\delta_0 X_{t-1} -\\delta_0 X_{t-1} + \\theta_1X_{t-1} + \\epsilon_t\\]\n\\[\\Delta Y_t = \\delta + (\\theta -1)Y_{t-1} + \\delta_0(X_t - X_{t-1}) + (\\delta_0 + \\delta_1)X_{t-1} + \\epsilon_t\\]\n\\[\\Delta Y_t = \\delta + (\\theta -1)Y_{t-1} + \\delta_0 \\Delta X_t + (\\delta_0 + \\delta_1)X_{t-1} + \\epsilon_t\\]\n\\[\\Delta Y_t = [\\delta + (\\theta -1)Y_{t-1} + (\\delta_0 + \\delta_1)X_{t-1}] + \\delta_0 \\Delta X_t + \\epsilon_t\\]\nMultiplicamos y dividimos por \\((\\theta -1)\\) la expresion \\([\\delta + (\\theta -1)Y_{t-1} + (\\delta_0 + \\delta_1)X_{t-1}]\\), asi:\n\\[\\Delta Y_t = (\\theta -1)[\\frac{\\delta}{\\theta -1)} + \\frac{\\theta -1}{\\theta -1}Y_{t-1} + \\frac{\\delta_0 + \\delta_1}{\\theta-1}X_{t-1}] + \\delta_0 \\Delta X_t + \\epsilon_t\\]\nAhora \\(\\theta-1 = -(1-\\theta)\\)\n\\[\\Delta Y_t = -(1-\\theta)[\\frac{\\delta}{-(1-\\theta)} + Y_{t-1} + \\frac{\\delta_0 + \\delta_1}{-(1-\\theta)}X_{t-1}] + \\delta_0 \\Delta X_t + \\epsilon_t\\]\nDonde:\n\\[\\frac{\\delta}{(1-\\theta)} = -\\beta_1\\]\n\\[\\frac{\\delta_0 + \\delta_1}{(1-\\theta)} = -\\beta_2\\]\n\\[\\Delta Y_t = -(1-\\theta)[-\\beta_1 + Y_{t-1} - \\beta_2X_{t-1}] + \\delta_0 \\Delta X_t + \\epsilon_t\\]\n\\[\\Delta Y_t = -(1-\\theta)[Y_{t-1}-\\beta_1 -\\beta_2X_{t-1}] + \\delta_0 \\Delta X_t + \\epsilon_t\\]\nRecordemos que \\(Y_t = Y_{t-1} = Y\\) , donde \\(\\hat{Y} = \\beta_1 + \\beta_2X\\)\n\\(\\Delta Y_t = -(1-\\theta)[Y_{t-1}-\\hat Y_{t-1}] + \\delta_0 \\Delta X_t + \\epsilon_t\\) Se puede ver que: \\(Y_{t-1} - \\hat{Y_{t-1}} = \\epsilon_{t-1}\\) , quedando la ecuacion de correción de errores\n\\[\\Delta Y_t = -(1-\\theta)\\epsilon_{t-1} + \\delta_0 \\Delta X_t + \\epsilon_t\\]\n\\[\\Delta Y_t = -\\alpha \\epsilon_{t-1} + \\delta_0 \\Delta X_t + \\epsilon_t\\]\nDonde:\n\\[(1-\\theta_1) = \\delta_0\\]\n\\[-(1-\\theta) = -\\alpha\\]\n\\[\\frac{\\delta}{(1-\\theta)} = -\\beta_1\\]\n\\[\\frac{\\delta_0 + \\delta_1}{(1-\\theta)} = -\\beta_2\\]\n\nSe debe indicar que \\(\\delta_0 \\Delta X_t\\) recoge la relacion de corto plazo donde, \\((1-\\theta_1) = \\delta_0\\) es un coeficiente de impacto, el cual mide los efectos de corto plazo de un cambio de \\(X_t\\)\n\\(-\\alpha \\epsilon_{t-1}\\), me recoge la relacion de largo plazo donde, \\(-(1-\\theta) = -\\alpha\\) muestra la correcion de \\(\\Delta Y_t\\) respecto al error (desviación del largo plazo) en \\({t-1}\\) (coeficiente de ajuste).\nSi se da un \\(\\epsilon_{t-1}\\) positivo es porque \\(Y_{t-1}&gt;\\beta_1+\\beta_2 X_{t-1}\\), entonces \\(Y_t\\) debe caer y \\(\\Delta Y_{t-1}\\) sera negativo (y viceversa).\nsi hay una relacion de cointegracion, siempre se daran ajustes que corrigen el error.\nempiricamente debemos encontrar que \\(\\delta_0 = 1- \\theta_1 &gt; 0\\) , \\(\\theta_1&lt;0\\) , ya que se demostraria que los cambios son hacia el equilibrio (modelovalido).\nEl término \\[-\\alpha (Y_{t-1} - \\beta_1 + \\beta_2X_{t-1})\\]\n\ntambién se puede representar como \\(-\\alpha(\\epsilon_{t-1})\\)\nse conoce como el mecanismo de corrección de errores.\n\nRecuerda que \\(Y_{t} = \\beta_1 + \\beta_2X_{t}\\) representa el equilibrio de largo plazo:\n\nsi \\(\\epsilon_t = Y_t - \\beta_1 + \\beta_2X_{t}~&gt;~0, Y_t\\) está sobre el punto de equilibrio en \\(t−1\\).\nsi \\(\\epsilon_t = Y_t - \\beta_1 + \\beta_2X_{t}~&lt;~0, Y_t\\)está debajo el punto de equilibrio en \\(t−1\\).\n\n\nNota: El mecanismo de corrección de errores se refiere a la forma en que un sistema o proceso ajusta su comportamiento para volver a un estado de equilibrio. En el contexto de las ecuaciones presentadas, se hace referencia a cómo la variable \\(Y_t\\) se ajusta hacia el equilibrio representado por \\(\\beta_1 + \\beta_2X_{t}\\) cuando la diferencia entre \\(Y_t ~y~ \\beta_1 + \\beta_2X_{t}\\) es positiva \\((\\epsilon_t &gt; 0 )\\) o negativa \\((\\epsilon_t &lt; 0)\\). Si \\((\\epsilon_t &gt; 0)\\), significa que \\(Y_t\\) está por encima del punto de equilibrio en \\(t-1\\), mientras que si \\((\\epsilon_t &lt; 0)\\), significa que \\(Y_t\\) está por debajo del punto de equilibrio en \\(t-1\\). En ambos casos, el sistema tiende a corregir el error y volver al equilibrio de largo plazo.\n\nInterpretación de los Coeficientes\n\n\\(\\phi\\) : Coeficiente de impacto a corto plazo, mide el efecto inmediato de un cambio en XXX.\n\\(\\psi\\) : Coeficiente de ajuste, indica cómo se corrige el desequilibrio a largo plazo.\n\\(Y_{t-1} - \\theta X_{t-1}\\)​: Error de equilibrio que el MCE corrige.\n\n\n\n\nTabla Comparativa\n\n\n\n\n\n\n\n\nCaracterística\nModelo ADL\nModelo de Corrección de Errores (MCE)\n\n\n\n\nEcuación Base\n\\[Y_t = \\alpha + \\beta X_t + \\gamma Y_{t-1} + \\delta X_{t-1} + \\epsilon_t\\]\n\\[\\Delta Y_t = \\phi \\Delta X_t + \\psi (Y_{t-1} - \\theta X_{t-1}) + \\epsilon_t\\]\n\n\nObjetivo\nRelación entre variables con rezagos\nCorrección de desequilibrio a corto y largo plazo\n\n\nCointegración\nNo directamente considerado\nImplícita, basada en el teorema de representación\n\n\nDesequilibrio\nNo explícitamente tratado\nCorregido mediante el término de error\n\n\nCoeficiente de Impacto\n\\[\\beta\\]\n\\[\\phi\\]\n\n\nCoeficiente de Ajuste\nNo explícito\n\\[\\psi\\]\n\n\nRelación de Largo Plazo\nNo considerado\n\\[Y_{t-1} - \\theta X_{t-1}\\]​\n\n\nPrimera Diferencia\nNo aplicado\nSí, \\(\\Delta Y_t\\) y \\(\\Delta X_t\\)\n\n\nEstacionariedad\nPuede tener autocorrelación\nSe ajusta para lograr residuales no autocorrelacionados"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#modelos-de-ecuaciones-simultaneas-mes",
    "href": "Proyectos/Repaso Tercer Parcial.html#modelos-de-ecuaciones-simultaneas-mes",
    "title": "Repaso Tercer Parcial",
    "section": "Modelos de Ecuaciones Simultaneas (MES)",
    "text": "Modelos de Ecuaciones Simultaneas (MES)\nLa estimación en modelos de ecuaciones simultáneas (MES) es esencial en econometría para modelar sistemas donde las variables endógenas influyen mutuamente. Este enfoque es vital para entender interacciones complejas en economía y otras ciencias sociales. Los modelos MES permiten la estimación de parámetros estructurales que reflejan relaciones causales directas entre las variables endógenas, diferenciándolos de los parámetros de la forma reducida que solo reflejan correlaciones observadas.\n\nIdentificación\nLa identificación asegura que los parámetros estructurales del modelo puedan estimarse de manera única. Esto es crucial porque, sin identificación, no es posible distinguir entre diferentes conjuntos de parámetros que generan los mismos resultados observables.\n\nCondición de Orden:\n\nDefinición 1: En un modelo con MMM ecuaciones, una ecuación está identificada si se excluyen al menos M−1M-1M−1 variables (endógenas y predeterminadas) del modelo.\nDefinición 2: La ecuación está identificada si el número de variables predeterminadas excluidas (K−kK-kK−k) es mayor o igual al número de variables endógenas incluidas menos uno (m−1m-1m−1).\nInterpretación:\n\nSi K−k=m−1K-k = m-1K−k=m−1, la ecuación está exactamente identificada.\nSi K−k&gt;m−1K-k &gt; m-1K−k&gt;m−1, está sobreidentificada.\nSi K−k&lt;m−1K-k &lt; m-1K−k&lt;m−1, no está identificada.\n\n\nCondición de Rango:\n\nUna ecuación está identificada si se puede construir al menos un determinante no nulo de orden (M−1)(M-1)(M−1) a partir de los coeficientes de variables excluidas de esa ecuación, pero incluidas en otras ecuaciones del modelo.\nEsto garantiza que las variables excluidas aportan suficiente información para identificar los parámetros estructurales de la ecuación en cuestión.\n\n\n\n\nMétodos de Estimación\nExisten varios métodos para estimar los parámetros en modelos de ecuaciones simultáneas, dependiendo de si las ecuaciones están exactamente identificadas o sobreidentificadas.\n\nMétodo de Mínimos Cuadrados Indirectos (MCI):\n\nSe aplica cuando el modelo estructural está exactamente identificado.\nPasos:\n\nForma Reducida: Convertir el sistema estructural en su forma reducida, donde cada variable endógena se expresa en función de todas las variables predeterminadas y los errores.\nEstimación por MCO: Aplicar mínimos cuadrados ordinarios (MCO) a las ecuaciones de la forma reducida para obtener estimaciones de los parámetros de la forma reducida.\nDerivación de Parámetros Estructurales: Utilizar las relaciones entre los parámetros estructurales y los parámetros de la forma reducida para estimar los primeros a partir de los segundos.\n\n\nMétodo de Mínimos Cuadrados en Dos Etapas (MC2E):\n\nSe utiliza cuando una ecuación está sobreidentificada.\nPasos:\n\nPrimera Etapa: Regresión de las variables endógenas sobre todas las variables predeterminadas para obtener estimaciones libres de error de las variables endógenas.\nSegunda Etapa: Sustituir las variables endógenas por sus valores estimados en la primera etapa y aplicar MCO a las ecuaciones modificadas para estimar los parámetros estructurales.\n\n\n\n\nEjemplo Matemático\n\n\nIdentificación\nConsideremos el siguiente sistema de ecuaciones:\n\nDemanda: \\[Q_{t}^{d} = \\alpha_0 + \\alpha_1 P_t + \\alpha_2 I_t + u_{t}^{d}\\]\nOferta: \\[Q_t^s = \\beta_0 + \\beta_1 P_t + \\beta_2 P_{t-1} + u_t^s\\]\nCondición de Orden:\n\n\nDemanda:\n\nNúmero de variables excluidas (\\(K-k\\)): 1 (\\(P_{t-1}\\)​).\nNúmero de variables endógenas menos uno (\\(m-1\\)): 1 (\\(P_t\\)​).\nConclusión: Exactamente identificada (\\(K-k = m-1\\)).\n\nOferta:\n\nNúmero de variables excluidas (\\(K-k\\)): 1 (\\(I_t\\)​).\nNúmero de variables endógenas menos uno (\\(m-1\\)): 1 (\\(P_t\\)​).\nConclusión: Exactamente identificada (\\(K-k = m-1\\)).\n\n\nCondición de Rango:\n\nMatriz de coeficientes de variables excluidas debe tener un determinante no nulo.\nPara la ecuación de demanda, se excluye \\(P_{t-1}\\)​, cuya matriz de coeficientes no debe ser cero en la ecuación de oferta.\nConclusión: Si \\(\\det(\\beta_2) &gt; 0\\), la condición de rango se satisface.\n\n\n\nEstimación\nMétodo de MCI:\n\nForma Reducida:\n\nDemanda: \\(P_t = \\Pi_0 + \\Pi_1 I_t + \\Pi_2 P_{t-1} + v_t\\)\nOferta: \\(Q_t = \\Pi_3 + \\Pi_4 I_t + \\Pi_5 P_{t-1} + v_t\\)​\n\nAplicación de MCO:\n\nEstimación de parámetros de forma reducida (\\(\\Pi\\)) usando MCO.\n\nCoeficientes Estructurales:\n\nDerivación de los coeficientes estructurales a partir de las estimaciones de la forma reducida:\n\n\\(\\Pi_0 = (\\beta_0 - \\alpha_0) / (\\alpha_1 - \\beta_1)\\)\n\\(\\Pi_1 = -\\alpha_2 / (\\alpha_1 - \\beta_1)\\)\n\\(\\Pi_2 = \\beta_2 / (\\alpha_1 - \\beta_1)\\)\n\n\n\nMétodo de MC2E:\n\nPrimera Etapa:\n\nRegresión de \\(Y_1\\)​ (variable endógena) sobre todas las variables predeterminadas (\\(X_1, X_2, X_3, X_4\\)) para obtener \\(\\hat{Y}_1\\)​:\n\n\\(Y_1 = \\hat{\\Pi}_0 + \\hat{\\Pi}_1 X_1 + \\hat{\\Pi}_2 X_2 + \\hat{\\Pi}_3 X_3 + \\hat{\\Pi}_4 X_4 + \\hat{u}_1\\)​\n\n\nSegunda Etapa:\n\nSustitución de \\(Y_1\\)​ por \\(\\hat{Y}_1\\)​ y aplicar MCO:\n\n\\(Y_1 = \\beta_0 + \\beta_{12} \\hat{Y}_2 + \\gamma_{11} X_1 + \\gamma_{12} X_2 + u^*_1\\)​\n\n\n\n\n\n\nTabla de Resumen\n\n\n\n\n\n\n\n\nConcepto\nDescripción\nFórmulas Importantes\n\n\n\n\nIdentificación\nAsegura la estimabilidad única de parámetros estructurales.\nCondicioˊn de Orden: \\(K−k≥m−1\\)\n\n\n\n\nCondición de Rango: \\(det(\\text{matriz de excluidas})\\neq0\\)\n\n\nMétodo de MCI\nSe usa para modelos exactamente identificados.\nPaso 1: Forma reducida: \\[P_t = \\Pi_0 + \\Pi_1 I_t + \\Pi_2 P_{t-1} + v_t\\]\n\n\n\n\nPaso 2: MCO en forma reducida: \\[\\beta = (X^T X)^{-1} X^T Y\\]\n\n\n\n\nPaso 3: Coeficientes estructurales: \\[\\Pi = f(\\alpha, \\beta)\\]\n\n\nMétodo de MC2E\nSe usa para ecuaciones sobreidentificadas.\nPaso 1: \\[Y_1 = \\hat{\\Pi}_0 + \\hat{\\Pi}_1 X_1 + \\hat{\\Pi}_2 X_2 + \\hat{\\Pi}_3 X_3 + \\hat{\\Pi}_4 X_4 + \\hat{u}_1\\]​\n\n\n\n\nPaso 2: \\[Y_1 = \\beta_0 + \\beta_{12} \\hat{Y}_2 + \\gamma_{11} X_1 + \\gamma_{12} X_2 + u^*_1\\]"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#modelo-de-vectores-autorregresivos-var",
    "href": "Proyectos/Repaso Tercer Parcial.html#modelo-de-vectores-autorregresivos-var",
    "title": "Repaso Tercer Parcial",
    "section": "Modelo de Vectores Autorregresivos (VAR)",
    "text": "Modelo de Vectores Autorregresivos (VAR)\nUn modelo de vectores autorregresivos (VAR) es un tipo de modelo estadístico utilizado para analizar la relación entre múltiples series temporales. En lugar de modelar cada serie temporal por separado, como se haría en un modelo univariado, un VAR modela simultáneamente todas las series temporales en un sistema.\nLa idea fundamental detrás de un VAR es que cada variable en el sistema se regresa a sí misma en función de sus valores pasados, así como de los valores pasados de las otras variables en el sistema. Esto significa que cada variable en el sistema se modela como una función lineal de sus propios rezagos (valores pasados) y de los rezagos de todas las demás variables.\nPor ejemplo, consideremos un sistema con dos series temporales: el PIB y la tasa de desempleo. Un modelo VAR para este sistema podría modelar tanto el PIB como la tasa de desempleo en función de sus valores pasados y de los valores pasados de la otra variable. Esto permitiría analizar cómo el PIB y la tasa de desempleo afectan mutuamente entre sí a lo largo del tiempo.\nVeamos el modelo estructural dinámico [modelo (1)]:\n\\[y_{1t}= \\alpha_{10} + \\alpha_{11}y_{2t} + \\alpha_{12}y_{1t-1} + \\alpha_{13}y_{2t-1} + \\gamma^{´}_1z_t +\\epsilon_1t\\]\n\\[y_{2t}= \\alpha_{20} + \\alpha_{21}y_{1t} + \\alpha_{22}y_{1t-1} + \\alpha_{23}y_{2t-1} + \\gamma^{´}_2z_t +\\epsilon_2t\\]\nDonde \\(y_{1t}\\) , \\(y_{2t}\\) son variables estacionarias, y \\(\\epsilon_{1t}\\) , \\(\\epsilon_{2t}\\) son procesos ruido blanco con esperanza cero y varianzas \\(\\sigma^{2}_{\\epsilon_{1t}}\\), \\(\\sigma^{2}_{\\epsilon_{2t}}\\) y covarianza \\(\\sigma_{12}\\).\nEl modelo (1) es de ecuaciones simultáneas con dos variables endógenas \\(y_{1t}\\) y \\(y_{2t}\\) y un vector \\(z_t\\) de variables exógenas.\nUn shock sobre \\(y_{2t}\\), en la forma de un valor no nulo de la innovación estructural \\(\\epsilon_{2t}\\), afecta directamente a \\(y_{2t}\\), pero también influye a \\(y_{1t}\\) a través de la presencia de \\(y_{2t}\\) como variable explicativa en la primera ecuación.\nAdemás, este efecto se propaga en el tiempo, debido a la presencia de los valores rezagados de ambas variables como variables explicativas.\nLas variables explicativas exógenas \\(z_t\\) también pueden aparecer con rezagos en el modelo. Por ejemplo, \\(z_t\\) podría ser una tendencia determinista o que recoja la estacionalidad. \\(z_t\\) también puede representar variables tal que \\(E(z_{t−s}{~}\\epsilon_{1t})=𝐸(z_{𝑡-s} ~ \\epsilon_{2𝑡})=0 ~ ∀_𝑠\\). Por ejemplo, el precio de barril de petróleo que se determina en mercados internacionales mientras \\(y_{1t}\\) y \\(y_{2t}\\) son variables de la macroeconmía que se determinan en la economia interna.\nAhora, el Modelo (1) se puede representar de forma matricial de la siguiente forma:\n\\[\\Pi y_t = \\Gamma_0 + \\Gamma_1 y_{t-1} + \\Phi z_t + \\varepsilon_t\\]\nDonde:\n\\[\\Pi = \\begin{equation}\\begin{pmatrix}1 & -\\alpha_{11} \\\\-\\alpha_{21} & 1 \\end{pmatrix}\\end{equation}\\] , \\(\\Gamma_0 =\\begin{equation}\\begin{pmatrix} \\alpha_{10} \\\\\\alpha_{20} \\end{pmatrix}\\end{equation}\\) , \\(\\Gamma_1 = \\begin{equation}\\begin{pmatrix} \\alpha_{12} & \\alpha_{13} \\\\\\alpha_{22} & \\alpha_{23} \\end{pmatrix}\\end{equation}\\) , \\(\\Phi =\\begin{equation}\\begin{pmatrix} \\gamma_{1} \\\\\\gamma_{2} \\end{pmatrix}\\end{equation}\\)\nEste modelo se conoce como VAR estructural y presenta dos problemas:\n\nla simultaneidad, al aparecer cada una de las dos variables como variable explicativa en la ecuación de la otra, lo que genera inconsistencia del estimador MCO, podría resolverse estimando por variables instrumentales, siempre que contemos con instrumentos adecuados, lo cual no es sencillo de justificar. Además, el segundo problema podría persistir.\nsi los términos de error tuviesen autocorrelación, las estimaciones MCO serían inconsistentes, al tratarse de un modelo dinámico se resuelve tratando de ampliar la estructura dinámica del modelo hasta lograr que los términos de error carezcan de autocorrelación.\n\nSupongamos que la matriz \\(\\Pi\\) tiene inversa \\(det(\\Pi) \\neq~ 0\\) , tenemos entonces:\n\\[y_t = \\Pi^{-1} \\Gamma_0 + \\Pi^{-1}\\Gamma_1 y_{t-1} + \\Pi^{-1}\\Phi z_t + \\Pi^{-1} \\varepsilon_t\\]\n\\[y_t = \\Pi^{-1} \\Gamma_0 + \\Pi^{-1}\\Gamma_1 y_{t-1} + \\Pi^{-1}\\Phi z_t + \\Pi^{-1} \\varepsilon_t\\]\n\\[y_t = \\textbf{A}_0 + \\textbf{A}_1 y_{t-1} + \\textbf{M} z_t + u_t\\]\nDonde:\n\\[\\textbf{A}_0 = \\Pi^{-1} \\Gamma_0\\] , \\[\\textbf{A}_1 = \\Pi^{-1}\\Gamma_1\\] , \\[\\textbf{M} = \\Pi^{-1}\\Phi\\] , \\[u_t = \\Pi^{-1} \\varepsilon_t\\]\nAsi, hemos obtenido en modelo de forma reducida o modelo vectorial autoregresivo (VAR) en el cual:\n\\(y_{1t}= \\beta_{10} + \\beta_{11}y_{1t-1} + \\beta_{12}y_{2t-1} + \\textbf{m}_{11}z_t + u_{1t}\\)\n\\(y_{2t}= \\beta_{20} + \\beta_{21}y_{1t-1} + \\beta_{22}y_{2t-1} + \\textbf{m}_{21}z_t + u_{2t}\\)\nEste seria un modelo VAR de orden n en su forma reducida:\n\\[y_{1t}= \\beta_{10} + \\sum_{j=1}^{k}\\beta_{j}y_{t-j} + \\sum_{j=1}^{k}\\textbf{m}_{j}z_{t-j} + u_{1t}\\]\n\\[y_{2t}= \\beta_{20} + \\sum_{j=1}^{k}\\theta_{j}y_{t-j} + \\sum_{j=1}^{k}\\textbf{m}_{j}z_t + u_{2t}\\]\nEn la forma reducida de un modelo VAR (Modelo de Vectores Autorregresivos), las ecuaciones se expresan en términos de las variables endógenas del sistema en función de sus rezagos y, posiblemente, variables exógenas. La estructura de un modelo VAR en su forma reducida se puede describir de la siguiente manera:\n\nVariables Endógenas: Estas son las variables que se están modelando en el sistema. Por ejemplo, si estamos modelando el PIB, la inflación y la tasa de interés, estas serían nuestras variables endógenas.\nRezagos: Cada variable endógena se expresa como una función lineal de sus propios rezagos y de los rezagos de las otras variables endógenas en el sistema. Por ejemplo, la variable endógena \\(y_{1t}\\) en el rezago \\(j\\) se puede expresar como \\(y_{t-j}\\)​.\nVariables Exógenas (opcional): Además de las variables endógenas, el modelo VAR en su forma reducida puede incluir variables exógenas que no están determinadas dentro del sistema, pero que pueden afectar a las variables endógenas. Estas variables pueden incluir datos económicos, políticos o cualquier otro factor relevante.\nParámetros del Modelo: Los parámetros del modelo son los coeficientes que multiplican a los rezagos de las variables endógenas y, posiblemente, a las variables exógenas. Estos parámetros son estimados a partir de los datos y capturan la relación entre las diferentes variables en el sistema.\nError Término (Residuos): El término de error en la forma reducida del modelo VAR captura la parte de la variabilidad de las variables endógenas que no es explicada por los términos autoregresivos y las variables exógenas. Estos errores se suponen que son independientes e idénticamente distribuidos, con una distribución normal. donde las \\(u\\) son los terminos de error estocático, llamados impulsos, innovaciones o choques en el lenguaje VAR.\nLa utilizacion de muchas o muy pocas variables rezagadas puede conducir a un problema de consumo de muchos grados de libertad, la aparicion de la multicolinealidad o errores de especificacion. una forma de decidir esta cuestión es utilizar criterios como el de Akaike o el de Schwarz, para decidir el modelo que proporcione los valores mas bajo de estos.\nElorden de los modelos VAR está dado por el número de rezagosque se usa en cada ecuación. El modelo descrito anteriormente es entonces un \\(\\textbf{VAR(1)}\\), para denotar también el número de variables se usa\\(\\textbf{VAR}_{2}(1)\\)\n\n\nCriterios de Información\nUn problema central en el análisis de modelos VAR es encontrar el número de rezagos que produce los mejores resultados. La comparación de modelos generalmente se basa en criterios de información como el Akaike AIC, Bayesiano BIC o Hannan-Quinn HQ, buscando que se minimice el valor del criterio de información.\n\\[AIC=\\frac{−2} lT + \\frac 2pT\\]\n\\[BIC=\\frac {−2}lT+ \\frac {2ln(T)}T\\]\n\\[HQ=\\frac {−2}lT + \\frac {2kln(ln(T))}T\\]\nDonde \\(l=\\frac {−Tk}{2}(1+ln(2π))−\\frac T2ln(|Σ|)\\), y \\(p=k(d+nk)\\) el número de parámetros estimados en el modelo VAR, siendo \\(d\\) es el número de variables exógenas, \\(n\\) el orden del VAR, \\(k\\) el número de variables endógenas.\nPor lo general, el AIC es preferible a otros criterios, debido a sus características favorables de pronóstico de muestras pequeñas. El BIC y HQ, sin embargo, funcionan bien en muestras grandes y tienen la ventaja de ser un estimador consistente, es decir, converge a los valores verdaderos.\n\n\nFunciones de Impulso Respuesta\nLas funciones de impulso-respuesta (IRF) son una herramienta importante en el análisis de modelos VAR (Vector Autoregressive). Proporcionan información sobre cómo las variables en un sistema responden a los cambios en otras variables a lo largo del tiempo, específicamente en respuesta a un “impulso” o un shock en una de las variables.\nAquí hay una explicación detallada de las funciones de impulso-respuesta en modelos VAR:\n\nDefinición de Impulso-Respuesta: En un modelo VAR, el término “impulso” se refiere a un choque o shock que afecta a una de las variables del sistema. La función de impulso-respuesta describe cómo las otras variables del sistema responden a este impulso en el tiempo.\nCálculo de las IRF: Las IRF se calculan mediante simulación. Una vez estimado el modelo VAR, se introduce un impulso unitario (o un impulso en el nivel deseado) en una de las variables del sistema y se observa cómo las otras variables responden a este impulso a lo largo de múltiples períodos de tiempo.\nInterpretación de las IRF: Las IRF muestran cómo un cambio en una variable afecta a otras variables en el sistema a lo largo del tiempo. Una IRF típicamente muestra cómo la variable endógena (o variable de respuesta) responde al impulso en una variable exógena (o variable de impulso) en diferentes horizontes temporales.\nPropiedades de las IRF:\n\nDirección y Magnitud de la Respuesta: Las IRF muestran si las variables responden positiva o negativamente al impulso, así como la magnitud de esa respuesta.\nPersistencia: Las IRF también indican si el efecto del impulso persiste en el tiempo o disminuye gradualmente.\nEfectos Cruzados: Las IRF muestran cómo los diferentes impulsos afectan a las variables en el sistema, lo que puede ayudar a entender las interacciones entre las variables.\n\nUtilidad de las IRF: Las IRF son útiles para evaluar el impacto de diferentes políticas o choques en una economía, comprender las dinámicas de las variables en un sistema económico y pronosticar el comportamiento futuro de las variables en función de cambios en otras variables.\n\n\n\nTipos de Funcion de Impulso Respuesta\nEn un modelo VAR (Vector Autoregression), se pueden calcular dos tipos de funciones de impulso respuesta:\n\nFunciones de impulso respuesta al impulso unitario: Estas funciones muestran cómo las variables responden a un shock de una desviación estándar en una variable específica en un periodo de tiempo y cómo se propagan esos efectos a lo largo de los periodos siguientes. Es decir, muestran el impacto de un shock de una magnitud específica en una variable sobre las demás variables en el modelo.\nFunciones de impulso respuesta acumuladas: Estas funciones muestran la respuesta acumulada de las variables a lo largo del tiempo después de un shock en una variable específica. Muestran cómo se acumulan los efectos de un shock en una variable sobre las demás variables en el modelo a lo largo de varios periodos.\n\nAmbos tipos de funciones de impulso respuesta son útiles para analizar cómo se propagan los efectos de un shock en una variable a lo largo del tiempo y cómo afecta a las demás variables en el modelo VAR. Esto permite comprender mejor las interacciones entre las variables y predecir cómo se comportarán en respuesta a cambios en una de ellas.\n\n\nCausalidad de Granger\nLa causalidad de Granger es un concepto importante en el análisis de series temporales que se utiliza para determinar si una serie temporal proporciona información útil para predecir otra serie temporal. Es una herramienta comúnmente utilizada en el contexto de los modelos VAR (Vector Autoregressive).\nAquí está una explicación detallada de la causalidad de Granger en el contexto de los modelos VAR:\n\nDefinición: La causalidad de Granger establece que una serie temporal \\(y_{1t}\\) “Granger-causa” a otra serie temporal \\(y_{2t}\\) si la información pasada de \\(y_{1t}\\) ayuda a predecir \\(y_{2t}\\) mejor que solo utilizando la información pasada de \\(y_{2t}\\).\nPrincipio: Si la serie \\(y_{1t}\\) Granger-causa a la serie \\(y_{2t}\\), entonces los rezagos de \\(y_{1t}\\) se incluirán como predictores en el modelo para predecir \\(y_{2t}\\). En otras palabras, los rezagos de \\(y_{1t}\\) tienen un poder predictivo significativo para \\(y_{2t}\\).\nPrueba de Causalidad de Granger: La causalidad de Granger se evalúa mediante una prueba estadística. En el contexto de los modelos VAR, esta prueba implica ajustar dos modelos:\n\nModelo restringido: Un modelo VAR que solo incluye rezagos de la serie \\(y_{2t}\\) como predictores para predecir \\(y_{2t}\\).\nModelo no restringido: Un modelo VAR que incluye rezagos tanto de la serie \\(y_{2t}\\) como de la serie \\(y_{1t}\\)como predictores para predecir \\(y_{2t}\\).\n\nComparación de Modelos: Después de ajustar ambos modelos, se utiliza una prueba estadística (se utiliza la estadisticaF) para comparar su ajuste. Si el modelo no restringido (que incluye rezagos de \\(y_{1t}\\) ) se ajusta significativamente mejor que el modelo restringido (que no incluye los rezagos de \\(y_{1t}\\)), entonces se concluye que la serie \\(y_{1t}\\) Granger-causa a la serie \\(y_{2t}\\).\nInterpretación: Si se establece que la serie \\(y_{1t}\\) Granger-causa a la serie \\(y_{2t}\\), significa que la información pasada de \\(y_{1t}\\) contiene información adicional que ayuda a predecir \\(y_{2t}\\), más allá de lo que ya se puede predecir con la información pasada de \\(y_{2t}\\).\n\nCausalidad de Granger: \\(y_{1t}\\) granger causa \\(y_{2t}\\) si un modelo que usa valores actuales \\(y_{2t}\\) pasados de \\(y_{1t}\\) y valores actuales y pasados de \\(y_{2t}\\) para predecir valores futuros de \\(y_{2t}\\) tiene un error de pronóstico menor que un modelo que solo usa valores actuales y pasados de \\(y_{2t}\\) para predecir \\(y_{2t}\\). En otras palabras, la causalidad de Granger responde a la siguiente pregunta: ¿ayuda el pasado de la variable \\(y_{1t}\\) a mejorar la predicción de los valores futuros de \\(y_{2t}\\)?\nCausalidad instantánea: \\(y_{1t}\\) causa \\(y_{2t}\\) (en el sentido de Granger instantáneo) si un modelo que usa valores actuales, pasados y futuros de \\(y_{1t}\\) y valores actuales y pasados de \\(y_{2t}\\) para predecir \\(y_{2t}\\) tiene un error de pronóstico menor que un modelo que solo usa valores actuales y pasados de \\(y_{1t}\\) y valores actuales y valores pasados de \\(y_{2t}\\). En otras palabras, la causalidad instantánea de Granger responde a la pregunta: ¿conocer el futuro de \\(y_{1t}\\) me ayuda a predecir mejor el futuro de \\(y_{2t}\\)? Si sé que va a hacer \\(y_{1t}\\), ¿me ayuda a saber lo que va a saber \\(y_{2t}\\)?\n\n\nTabla Resumen\n\n\n\n\n\n\n\nConcepto\nDescripción\n\n\n\n\nModelo VAR\nModelo que analiza la relación entre múltiples series temporales simultáneamente, modelando cada variable en función de sus valores pasados y los de otras variables en el sistema.\n\n\nEstructura Matricial\n\\[y_t = \\Gamma_0 + \\Gamma_1 y_{t-1} + \\Phi z_t + \\varepsilon_t\\]\n\n\nMatrices del Modelo\n\\(\\Gamma_0, \\Gamma_1, \\Phi\\)​ derivadas de las ecuaciones individuales.\n\n\nForma Reducida del VAR\n\\[y_t = \\textbf{A}_0 + \\textbf{A}_1 y_{t-1} + \\textbf{M} z_t + u_t\\]\n\n\nProblemas en VAR\nSimultaneidad (inconsistencia del MCO), autocorrelación (inconsistencia de estimaciones MCO).\n\n\nCriterios de Información\nAIC: Mejor para muestras pequeñas. BIC y HQ: Mejores para muestras grandes. Fórmulas específicas para cada criterio.\n\n\nFunciones de Impulso-Respuesta\nMuestran cómo las variables responden a choques en otras variables a lo largo del tiempo.\n\n\nCausalidad de Granger\nDetermina si una serie temporal proporciona información útil para predecir otra. Prueba de Causalidad: Comparación entre modelos restringidos y no restringidos.\n\n\nErrores y Soluciones\nSimultaneidad resuelta con variables instrumentales. Autocorrelación resuelta ampliando la estructura dinámica."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#modelo-heterocedástico-condicional-autorregresivo-arch",
    "href": "Proyectos/Repaso Tercer Parcial.html#modelo-heterocedástico-condicional-autorregresivo-arch",
    "title": "Repaso Tercer Parcial",
    "section": "Modelo Heterocedástico Condicional Autorregresivo (ARCH)",
    "text": "Modelo Heterocedástico Condicional Autorregresivo (ARCH)\nel Modelo Heterocedástico Condicional Autorregresivo (ARCH) es un modelo utilizado en econometría y finanzas para modelar la volatilidad de series temporales. Fue introducido por Robert F. Engle en 1982 y es particularmente útil para capturar la heterocedasticidad condicional en los datos financieros, donde la variabilidad puede cambiar con el tiempo.\n\nConceptos Clave del Modelo ARCH\n\nHeterocedasticidad Condicional: La varianza de los errores no es constante a lo largo del tiempo, sino que depende de los errores pasados. En un contexto financiero, esto significa que los periodos de alta volatilidad tienden a ser seguidos por periodos de alta volatilidad, y lo mismo ocurre para los periodos de baja volatilidad.\nAutorregresividad de la Varianza: La varianza condicional de los errores en un momento dado se modela como una función lineal de los errores pasados.\n\n\n\nEspecificación del Modelo ARCH(p)\nEl modelo ARCH(p) (donde \\(p\\) denota el número de retardos incluidos) se especifica de la siguiente manera:\n\nModelo de Media: Primero, se especifica el modelo para la serie temporal \\(y_t\\)​. Esto puede ser un modelo simple de media o un modelo ARMA, por ejemplo:\n\\(y_{t} = \\mu + \\epsilon_{t}\\)\ndonde \\(\\epsilon_{t}\\) son los errores (residuos) del modelo de media, y \\(\\mu\\) es la media del proceso.\nModelo de Varianza Condicional: La varianza condicional de los errores \\(\\epsilon_{t}\\) se modela como una función de los errores pasados:\n\\(\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2} + \\alpha_{2}\\epsilon_{t-2}^{2} + ... + \\alpha_{p}\\epsilon_{t-p}^{2}\\) ​\ndonde:\n\n\\(\\sigma_{t}^{2}\\) es la varianza condicional en el tiempo \\(t\\).\n\\(\\alpha_{0} &gt; 0\\) es una constante.\n\\(\\alpha_{i} ≥ 0\\) son los coeficientes que miden el impacto de los errores pasados en la varianza condicional.\n\nErrores: Los errores \\(\\epsilon_{t}\\) se suponen que son ruido blanco con media cero y varianza condicional \\(\\sigma_{t}^{2}\\) :\n\\(\\epsilon_{t} \\sim \\textbf{N}(0, \\sigma_{t}^{2})\\)\n\n\n\nEstimación del Modelo ARCH\nLa estimación del modelo ARCH generalmente implica los siguientes pasos:\n\nEstimar el Modelo de Media: Ajustar el modelo de media (por ejemplo, un modelo ARMA) y obtener los residuos \\(\\epsilon_{t}\\)​.\nEstimación de la Varianza Condicional: Ajustar el modelo de varianza condicional ARCH(p) a los residuos al cuadrado \\(\\epsilon_{t}^{2}\\) ​ utilizando técnicas de máxima verosimilitud.\nEvaluación del Modelo: Verificar si los residuos del modelo ARCH muestran las características esperadas (por ejemplo, verificar que los residuos estandarizados son ruido blanco).\n\n\n\nEjemplo de Aplicación\nSupongamos que queremos modelar la volatilidad de los retornos diarios de una acción. Podríamos especificar un modelo ARCH(1) de la siguiente manera:\n\nModelo de Media: \\(y_{t} = \\mu + \\epsilon_{t}\\)\nModelo ARCH(1): \\(\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2}\\)\n\nAquí, la varianza condicional \\(\\sigma_{t}^{2}\\)​ en el tiempo \\(t\\) depende linealmente del cuadrado del error en el tiempo \\(t-1\\). Si \\(\\alpha_{1}\\)​ es significativo, indica que hay dependencia en la volatilidad de los retornos.\n\n\nLimitaciones del Modelo ARCH\n\nSimplicidad: Los modelos ARCH pueden ser demasiado simples para capturar todas las características de la volatilidad en series temporales financieras.\nNúmero de Parámetros: A medida que \\(p\\) aumenta, el número de parámetros a estimar también aumenta, lo que puede complicar la estimación y la interpretación.\n\n\n\nExtensiones\nPara superar algunas de las limitaciones del modelo ARCH, se han desarrollado extensiones como el modelo GARCH (Generalización ARCH), que incluye tanto retardos de los errores como retardos de las varianzas pasadas.\n\n\nResumen\nEl modelo ARCH es una herramienta poderosa para modelar la heterocedasticidad condicional en series temporales, permitiendo capturar la naturaleza cambiante de la volatilidad en los datos financieros. Específicamente, la varianza de los errores se modela como una función de los errores pasados, proporcionando una forma de capturar la agrupación de volatilidad observada en muchas series temporales financieras."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#modelo-heterocedástico-condicional-autorregresivo-generalizado-garch",
    "href": "Proyectos/Repaso Tercer Parcial.html#modelo-heterocedástico-condicional-autorregresivo-generalizado-garch",
    "title": "Repaso Tercer Parcial",
    "section": "Modelo Heterocedástico Condicional Autorregresivo Generalizado (GARCH)",
    "text": "Modelo Heterocedástico Condicional Autorregresivo Generalizado (GARCH)\nEl Modelo Heterocedástico Condicional Autorregresivo Generalizado (GARCH) es una extensión del modelo ARCH que captura la variabilidad temporal en la volatilidad de una serie temporal. Fue introducido por Tim Bollerslev en 1986. El modelo GARCH es ampliamente utilizado en finanzas y econometría para modelar series temporales financieras, donde la volatilidad puede cambiar con el tiempo.\n\nConceptos Clave del Modelo GARCH\n\nHeterocedasticidad Condicional: La varianza de los errores en una serie temporal no es constante a lo largo del tiempo, sino que depende de los errores y varianzas pasadas.\nAutorregresividad en la Varianza: La varianza condicional de los errores depende de los errores pasados y de las varianzas condicionales pasadas, proporcionando una estructura más completa y flexible que el modelo ARCH.\n\n\n\nEspecificación del Modelo GARCH(p, q)\nEl modelo GARCH(p, q) se especifica de la siguiente manera:\n\nModelo de Media: Primero, se especifica el modelo para la serie temporal \\(y_{t}\\):\n\\(y_{t} = \\mu + \\epsilon_{t}\\)\ndonde \\(\\epsilon_{t}\\)​ son los errores (residuos) del modelo de media, y \\(\\mu\\) es la media del proceso.\nModelo de Varianza Condicional: La varianza condicional \\(\\sigma_{t}^{2}\\)​ se modela como una función lineal de los errores pasados y de las varianzas condicionales pasadas:\n\\(\\sigma_{t}^{2} = \\alpha_{0} + \\sum_{i=1}^{q} \\alpha_{i}\\epsilon_{t-i}^{2} + \\sum_{j=1}^{p} \\beta_{j}\\sigma_{t-j}^{2}\\)\ndonde:\n\n\\(\\sigma_{t}^{2}\\)​ es la varianza condicional en el tiempo \\(t\\).\n\\(\\alpha_{0} &gt; 0\\) es una constante.\n\\(\\alpha_{i} ≥ 0\\) son los coeficientes que miden el impacto de los errores pasados al cuadrado.\n\\(\\beta_{j} ≥ 0\\) son los coeficientes que miden el impacto de las varianzas condicionales pasadas.\n\n\n\n\nEstimación del Modelo GARCH\nLa estimación del modelo GARCH generalmente implica los siguientes pasos:\n\nEstimar el Modelo de Media: Ajustar el modelo de media (por ejemplo, un modelo ARMA) y obtener los residuos \\(\\epsilon_{t}\\)​.\nEstimación de la Varianza Condicional: Ajustar el modelo de varianza condicional GARCH(p, q) a los residuos al cuadrado \\(\\epsilon_{t}^{2}\\)​ utilizando técnicas de máxima verosimilitud.\nEvaluación del Modelo: Verificar si los residuos del modelo GARCH muestran las características esperadas (por ejemplo, verificar que los residuos estandarizados son ruido blanco).\n\n\n\nEjemplo de Aplicación\nSupongamos que queremos modelar la volatilidad de los retornos diarios de una acción. Podríamos especificar un modelo GARCH(1, 1) de la siguiente manera:\n\nModelo de Media: \\(y_{t} = \\mu + \\epsilon_{t}\\)​\nModelo GARCH(1, 1): \\(\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2} + \\beta_{1}\\sigma_{t-1}^{2}\\)\n\nAquí, la varianza condicional \\(\\sigma_{t}^{2}\\) en el tiempo \\(t\\) depende linealmente del cuadrado del error en el tiempo \\(t−1\\) y de la varianza condicional en el tiempo \\(t−1\\). Este modelo captura tanto la memoria corta (a través de \\(\\epsilon_{t-1}^{2}\\) ) como la memoria larga (a través de \\(\\sigma_{t-1}^{2}\\)) de la volatilidad.\n\n\nVentajas del Modelo GARCH\n\nFlexibilidad: Al incluir tanto errores pasados como varianzas pasadas, el modelo GARCH puede capturar de manera más efectiva la dinámica de la volatilidad en los datos financieros.\nAplicabilidad: Los modelos GARCH son aplicables a una amplia variedad de series temporales financieras, incluyendo precios de acciones, tasas de cambio y tasas de interés.\n\n\n\nLimitaciones del Modelo GARCH\n\nComplejidad: La estimación de modelos GARCH puede ser compleja y computacionalmente intensiva, especialmente para valores altos de \\(p\\) y \\(q\\).\nSuposición de Normalidad: La suposición de que los errores estandarizados son normales puede no ser realista en todos los casos. Existen extensiones como GARCH-T que permiten distribuciones alternativas para los errores.\n\n\n\nResumen\nEl modelo GARCH es una herramienta poderosa para modelar la volatilidad condicional en series temporales financieras. Extiende el modelo ARCH al incluir tanto errores pasados como varianzas pasadas, lo que permite capturar la naturaleza persistente de la volatilidad en los datos. Es ampliamente utilizado en econometría y finanzas para prever la volatilidad y gestionar el riesgo.\n\n\nPruebas, Estimaciones y Pronósticos\nLa prueba del multiplicador de Lagrange (LM) es una herramienta estadística utilizada para detectar la presencia de efectos ARCH en una serie temporal. Los efectos ARCH se refieren a la situación en la que la varianza de los errores no es constante a lo largo del tiempo, sino que depende de los errores pasados.\nA continuación, se describe el procedimiento para llevar a cabo una prueba LM para efectos ARCH:\n\n\nProcedimiento de la Prueba LM para Efectos ARCH\n\nEstimar un Modelo de Regresión Básico: Primero, estima el modelo de regresión básico, que podría ser un modelo ARIMA o cualquier otro modelo de serie temporal adecuado. Obtén los residuos de este modelo.\n\\(y_{t}= \\beta X_{t} + \\epsilon_{t}\\)\nAquí, \\(y_{t}\\)​ es la variable dependiente, \\(X_{t}\\) son las variables independientes, \\(\\beta\\) son los coeficientes del modelo, y \\(\\epsilon_{t}\\)​ son los residuos.\nCuadrar los Residuos: Calcula los residuos al cuadrado \\((e_{t}^{2})\\) . Estos valores se usarán para detectar la heterocedasticidad.\nRegresión Auxiliar: Realiza una regresión auxiliar donde los residuos cuadrados se regresan sobre \\(q\\) retardos de los mismos residuos cuadrados.\n\\(e_{t}^{2}=\\alpha_{0} + \\alpha_{1}e_{t-1}^{2}+\\alpha_{2}e_{t-2}^{2} + ... + \\alpha_{q}e_{t-q}^{2} + v_{t}\\)\nAquí, \\(\\alpha_{0}\\) es el intercepto, \\(\\alpha_{i}\\) son los coeficientes de los residuos retardados, y \\(v_{t}\\) es el término de error de esta regresión auxiliar.\nCalcular el Estadístico de Prueba: El estadístico de prueba LM se calcula como \\(\\textbf{T*R}^{2}\\), donde \\(\\textbf{T}\\)es el número de observaciones y \\(\\textbf{R}^{2}\\) es el coeficiente de determinación de la regresión auxiliar.\n\\(\\textbf{LM}=\\textbf{T*R}^{2}\\)\nDistribución Asintótica: Bajo la hipótesis nula de que no hay efectos ARCH (es decir, todos los \\(\\alpha_{i}=0\\), el estadístico LM sigue una distribución chi-cuadrado \\(\\textbf{X}^{2}\\) con \\(q\\) grados de libertad.\nDecisión: Compara el estadístico LM con el valor crítico de la distribución \\(\\textbf{X}^{2}\\) con \\(q\\) grados de libertad. Si el estadístico LM es mayor que el valor crítico, se rechaza la hipótesis nula, indicando la presencia de efectos ARCH. De lo contrario, no se rechaza la hipótesis nula.\n\n\n\nResumen\nLa prueba del multiplicador de Lagrange (LM) para efectos ARCH sigue estos pasos principales:\n\nEstimar el modelo básico y obtener los residuos.\nCalcular los residuos al cuadrado.\nRealizar una regresión auxiliar de los residuos cuadrados sobre \\(q\\) retardos de los mismos.\nCalcular el estadístico \\(\\textbf{T*R}^{2}\\).\nComparar el estadístico con la distribución chi-cuadrado para tomar la decisión.\n\nEsta prueba es una herramienta efectiva para detectar heterocedasticidad condicional en las series temporales, y es ampliamente utilizada en econometría y finanzas para modelar y prever la volatilidad."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#que-son-los-errores-estandarizados",
    "href": "Proyectos/Repaso Tercer Parcial.html#que-son-los-errores-estandarizados",
    "title": "Repaso Tercer Parcial",
    "section": "¿Que son los Errores Estandarizados?",
    "text": "¿Que son los Errores Estandarizados?\nLos errores estandarizados son una forma de normalizar los residuos de un modelo para hacerlos comparables a lo largo del tiempo y evaluar su comportamiento de una manera más uniforme. En el contexto de modelos de heterocedasticidad condicional como GARCH, los errores estandarizados son particularmente útiles para verificar la adecuación del modelo y la presencia de heterocedasticidad residual.\n\nDefinición de Errores Estandarizados\nPara una serie temporal \\(y_{t}\\)​ modelada con un modelo de media y un modelo de varianza condicional (como un GARCH), los errores estandarizados se calculan de la siguiente manera:\n\nErrores del Modelo \\((\\epsilon_{t})\\)​: Estos son los residuos del modelo, que se obtienen como la diferencia entre los valores observados y los valores ajustados por el modelo de media: \\(\\epsilon_{t} = y_{t} -  \\hat y_{t}\\) ​ donde \\(\\hat y_{t}\\) es el valor ajustado.\nVarianza Condicional \\((\\sigma_{t}^{2})\\): Esta es la varianza condicional estimada en cada punto en el tiempo, que se obtiene del modelo de varianza condicional (por ejemplo, GARCH): \\((\\sigma_{t}^{2})\\)​\nErrores Estandarizados \\((e_{t})\\) : Los errores estandarizados se obtienen dividiendo los errores del modelo por la raíz cuadrada de la varianza condicional: \\(e_{t} = \\frac{\\epsilon_{t}} {\\sigma_{t}}\\) ​​ donde \\(\\sigma_{t} = \\sqrt{\\sigma_{t}^{2}}\\)​ es la desviación estándar condicional.\n\n\n\nInterpretación y Uso\nLos errores estandarizados \\(e_{t}\\)​ tienen una media esperada de cero y una varianza esperada de uno, siempre y cuando el modelo esté correctamente especificado. Esto permite comparar los errores a lo largo del tiempo y verificar si hay patrones que indiquen una mala especificación del modelo.\n\n\nEvaluación del Modelo con Errores Estandarizados\n\nAnálisis de Ruido Blanco: Los errores estandarizados deben comportarse como ruido blanco, es decir, deben ser independientes e idénticamente distribuidos con media cero y varianza uno. Si no se comportan de esta manera, podría indicar que el modelo no ha capturado adecuadamente la dinámica de la serie temporal.\nGráficos de Errores Estandarizados: Los gráficos de los errores estandarizados pueden ayudar a identificar patrones o anomalías en los residuos que no fueron capturados por el modelo. Por ejemplo, si los errores estandarizados muestran agrupaciones de valores altos o bajos, puede ser un signo de que hay heterocedasticidad residual no modelada.\nPruebas Estadísticas: Pruebas estadísticas, como la prueba de Ljung-Box, pueden ser aplicadas a los errores estandarizados para verificar la presencia de autocorrelación. También se pueden usar pruebas para la normalidad de los errores estandarizados, como la prueba de Jarque-Bera.\n\n\n\nEjemplo Práctico\nSupongamos que hemos ajustado un modelo GARCH(1,1) a una serie temporal de retornos financieros:\n\nEstimación del Modelo: Ajustamos el modelo y obtenemos los residuos \\(\\epsilon_{t}\\).\nCálculo de la Varianza Condicional: Estimamos la varianza condicional \\(\\sigma_{t}^{2}\\) utilizando los parámetros del modelo GARCH.\nCálculo de los Errores Estandarizados: Dividimos cada residuo \\(\\epsilon_{t}\\) por la desviación estándar condicional \\(\\sigma_{t}\\)​: \\(e_{t} = \\frac{\\epsilon_{t}} {\\sigma_{t}}\\)\nEvaluación: Analizamos los errores estandarizados \\(e_{t}\\)​ para verificar si son ruido blanco y si siguen una distribución normal.\n\n\n\nResumen\nLos errores estandarizados son una herramienta clave para evaluar la adecuación de los modelos de series temporales con heterocedasticidad condicional. Al normalizar los errores, permiten una evaluación uniforme de los residuos a lo largo del tiempo y ayudan a identificar posibles problemas de especificación del modelo."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#identificación-2",
    "href": "Proyectos/Repaso Tercer Parcial.html#identificación-2",
    "title": "Repaso Tercer Parcial",
    "section": "Identificación",
    "text": "Identificación\nLa mejor herramienta de identificación puede ser un gráfico de la serie de tiempo. Por lo general, es fácil detectar periodos de mayor variación esparcidos a lo largo de la serie.\nPuede resultar útil examinar el ACF y el PACF de \\(\\epsilon_{t}\\) y \\(\\epsilon_{t}^{2}\\). Por ejemplo,\n\nsi \\(\\epsilon_{t}\\) parece ser ruido blanco y parece ser \\(𝐴𝑅(1)\\), se sugiere un modelo \\(ARCH(1)\\) para la varianza.\nSi el PACF de sugiere \\(AR(p)\\), entonces \\(ARCH(m)\\) puede funcionar.\n\nLos modelos \\(GARCH\\) pueden ser sugeridos por una estructura de tipo ARMA en el ACF y PACF de \\(\\epsilon_{t}^{2}\\).\nEn la práctica, es posible que tengas que experimentar con varias estructuras ARCH y GARCH después de detectar que es necesario aborar el problema con este tipo de modelos."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#características-y-ventajas-de-los-datos-del-panel",
    "href": "Proyectos/Repaso Tercer Parcial.html#características-y-ventajas-de-los-datos-del-panel",
    "title": "Repaso Tercer Parcial",
    "section": "Características y Ventajas de los Datos del Panel",
    "text": "Características y Ventajas de los Datos del Panel\nDimensión Temporal y Transversal : Los datos de panel permiten analizar el comportamiento de los individuos a lo largo del tiempo, lo que proporciona información tanto sobre diferencias entre individuos (dimensión transversal) como sobre cambios dentro de los individuos a lo largo del tiempo (dimensión temporal).\nControl de Heterogeneidad : Permiten controlar la heterogeneidad no observada, es decir, las características no observables que pueden influir en las variables de interés y que no cambian con el tiempo para cada individuo.\nMejora de la Eficiencia : Incrementan la eficiencia de las estimaciones econométricas al proporcionar más información y reducir la colinealidad entre variables.\nal estudiar la sección transversal repetida de observaciones, los datos de panel resulta mas adecuado para estudiar la dinamica de cambio.\nCon datos de panel tenemos N individuos observados en varios periodos consecutivos, asi:\n\\[Y_{it} = \\beta_{1} + \\beta_{2}X_{2it} + \\beta_{3}X_{3it} + \\epsilon_{t}\\]\nDonde:\n\n\\(Y_{it}\\) = valor de \\(Y\\) para el individuo \\(i\\) en el periodo \\(t\\).\n\\(i\\) = es la i-ésima unidad transversal\n\\(t\\) = es el tiempo\n\\(X\\) = son la variables independientes, que en un principios no se suponen estocasticas\n\nEl termino de error cumple con las suposiciones clasicas \\(\\epsilon_{t} \\sim \\textbf{N}(0, \\sigma_{t}^{2})\\)\nNota 1: se supone que hay un maximo de N unidades transversales u observaciones, y un maximo de T periodos.\nNota 2: si cada unidad tranversal tiene el mismo el mismo número de observaciones de series de tiempo, entonces dicho panel (de datos) se llama panel balanceado . Si el número de observaciones difiere entre los miembros del panel se dice que es un panel desbalanceado\nNota 3: existen otras fomas de clasificación de los datos de panel.\nmacropaneles: los individuso son paises, sectores, regiones. N es pequeños respecto a T. (N &gt; T) o (T &gt; N)\nmicropaneles: los individuos son personal u hogares. N es mucho mas grande que T"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#características-del-modelo-de-panel-de-datos-agrupado",
    "href": "Proyectos/Repaso Tercer Parcial.html#características-del-modelo-de-panel-de-datos-agrupado",
    "title": "Repaso Tercer Parcial",
    "section": "Características del Modelo de Panel de Datos Agrupado:",
    "text": "Características del Modelo de Panel de Datos Agrupado:\n\nIntercepción Común:\nAsume que todas las entidades en el panel tienen el mismo intercepto. No se controla por diferencias específicas entre entidades.\n\n\nHomogeneidad:\nSe considera que todas las entidades son homogéneas en cuanto a las variables explicativas y su relación con la variable dependiente.\n\n\nSimplificación:\nLa especificación del modelo es más simple en comparación con los modelos de efectos fijos y aleatorios, ya que no incorpora interceptos o efectos específicos de cada entidad.\n\n\nFormulación del Modelo:\nEl modelo de panel de datos agrupado se puede representar de la siguiente manera:\n\\[Y_{it}=\\alpha + \\beta X_{it} + \\epsilon_{it}\\]​\nDonde:\n\n\\(Y_{it}\\) es la variable dependiente para la entidad \\(i\\) en el tiempo \\(t\\).\n\\(\\alpha\\) es el intercepto común para todas las entidades.\n\\(\\beta\\) es un vector de coeficientes que mide el efecto de las variables explicativas.\n\\(X_{it}\\)​ es un vector fila de variables explicativas para la entidad \\(i\\) en el tiempo \\(t\\).\n\\(\\epsilon_{it}\\) es el término de error.\n\nNota: los parametrso estimados del modelo son validos para todos los individuos en todos los peridos de tiempo."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#supuestos-del-modelo-de-panel-de-datos-agrupados",
    "href": "Proyectos/Repaso Tercer Parcial.html#supuestos-del-modelo-de-panel-de-datos-agrupados",
    "title": "Repaso Tercer Parcial",
    "section": "Supuestos del Modelo de Panel de Datos Agrupados:",
    "text": "Supuestos del Modelo de Panel de Datos Agrupados:\n\nHomogeneidad de los Interceptos:\nSupuesto: Se asume que todas las entidades tienen el mismo intercepto \\((\\alpha)\\).\nImplicación: No se consideran diferencias específicas entre entidades que podrían influir en la variable dependiente.\n\n\nLinealidad:\nSupuesto: La relación entre las variables independientes \\((X_{it}​\\)) y la variable dependiente \\((Y_{it})\\) es lineal.\nImplicación: Se puede modelar mediante una combinación lineal de las variables explicativas.\n\n\nExogeneidad:\nSupuesto: Las variables explicativas \\((X_{it}​)\\) no están correlacionadas con el término de error \\((\\epsilon_{it}​)\\).\nImplicación: \\(\\mathbb{E}(\\epsilon_{it} | X_{it}) = 0\\). Esto asegura que las variables independientes no están influenciadas por factores inobservables que afectan a la variable dependiente.\n\n\nNo Autocorrelación:\nSupuesto: Los errores \\((\\epsilon_{it}​)\\) no están correlacionados a lo largo del tiempo para una misma entidad.\nImplicación: \\(\\mathbb{E}(\\epsilon_{it} \\epsilon_{is}) = 0\\) para \\(t \\neq s\\). Esto significa que no hay relación entre los errores en diferentes períodos de tiempo para una misma entidad.\n\n\nNo Correlación entre Entidades:\nSupuesto: Los errores \\((\\epsilon_{it})\\) de diferentes entidades no están correlacionados.\nImplicación: \\(\\mathbb{E}(\\epsilon_{it} \\epsilon_{jt}) = 0\\) para \\(i \\neq j\\). Esto asegura que no hay dependencia entre los errores de diferentes entidades.\n\n\nHomoscedasticidad:\nSupuesto: La varianza de los errores \\((\\epsilon_{it}​\\)) es constante para todas las observaciones.\nImplicación: \\(\\text{Var}(\\epsilon_{it}) = \\sigma^{2}\\). Esto significa que la dispersión de los errores es constante a lo largo del tiempo y entre las entidades.\n\n\nIndependencia de las Variables Explicativas:\nSupuesto: Las variables explicativas \\((X_{it}​)\\) son independientes entre sí.\nImplicación: No hay multicolinealidad perfecta entre las variables explicativas.\n\n\nParametro Estimado del Modelo Agrupado\nPara estimar el parámetro \\(\\beta\\), se utiliza la fórmula de Mínimos Cuadrados Ordinarios (MCO), que se aplica de la misma manera que en un modelo de regresión lineal estándar. La fórmula para el estimador de \\(\\beta\\) es:\n\\[\\hat{\\beta} = (X´ X)^{-1}X´Y\\]\nDonde:\n\n\\(X\\) es la matriz de las variables explicativas, incluyendo todas las observaciones de todas las entidades y períodos de tiempo.\n\\(X´\\) es la transpuesta de la matriz \\(X\\).\n\\(Y\\) es el vector de las observaciones de la variable dependiente.\n\\(\\hat{\\beta}\\)​ es el vector de coeficientes estimados.\n\nNota: en este caso el modelo de homogeneidad total (modelo agrupado) tendria la siguiente representación de la ecuación de parametrso estimados\n\\[\\hat{\\beta}_{agrupado} = (\\sum_{i=1}^{N}\\sum_{t=1}^{T}x´_{it} x_{it})^{-1} \\sum_{i=1}^{N}\\sum_{t=1}^{T}x´_{it} y_{it}\\]"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#ventajas-y-desventajas",
    "href": "Proyectos/Repaso Tercer Parcial.html#ventajas-y-desventajas",
    "title": "Repaso Tercer Parcial",
    "section": "Ventajas y Desventajas:",
    "text": "Ventajas y Desventajas:\n\nVentajas:\n\nSimplicidad:\nEs más fácil de estimar y entender debido a su simplicidad en comparación con los modelos de efectos fijos o aleatorios.\n\n\nMenos Demandante de Datos:\nRequiere menos datos para estimar, ya que no necesita suficientes observaciones para cada entidad como en los modelos de efectos fijos.\n\n\n\nDesventajas:\n\nIgnora Heterogeneidad:\nNo controla por la heterogeneidad no observable entre entidades, lo que puede sesgar las estimaciones si las diferencias entre entidades son significativas.\n\n\nAsunción Fuerte de Homogeneidad:\nAsume que todas las entidades son idénticas en cuanto a su relación con las variables explicativas, lo cual puede no ser realista en muchos casos.\n\n\n\nEjemplos de Aplicación:\nAnálisis de Ventas en Diferentes Tiendas:\nSuponiendo que se tiene un panel de datos de ventas de varias tiendas a lo largo del tiempo y se asume que todas las tiendas responden de la misma manera a las estrategias de marketing.\nEstudio del Impacto de Políticas Económicas:\nAnálisis del impacto de una política económica específica en varios países, asumiendo que la política tiene el mismo efecto en todos los países."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#formulación-matemática-del-modelo-de-efectos-fijos",
    "href": "Proyectos/Repaso Tercer Parcial.html#formulación-matemática-del-modelo-de-efectos-fijos",
    "title": "Repaso Tercer Parcial",
    "section": "Formulación Matemática del Modelo de Efectos Fijos:",
    "text": "Formulación Matemática del Modelo de Efectos Fijos:\nEl modelo de efectos fijos se puede escribir de la siguiente manera:\n\\(y_{it} = \\alpha_{i} + \\beta x_{ it} + \\epsilon_{it}\\)\nDonde:\n\n\\(y_{it}\\) es la variable dependiente para la entidad \\(i\\) en el tiempo \\(t\\).\n\\(\\alpha_{i}\\) ​ es el intercepto específico de la entidad \\(i\\), capturando los efectos fijos.\n\\(\\beta\\) es el vector de coeficientes que mide el efecto de las variables explicativas.\n\\(x_{it}\\)​ es el vector de variables explicativas para la entidad \\(i\\) en el tiempo \\(t\\).\n\\(\\epsilon_{it}\\) es el término de error.\n\n\nEliminación de los Efectos Fijos:\nPara estimar el modelo sin tener que estimar directamente cada \\(\\alpha_{i}\\)​, se utiliza la transformación de “dentro de la entidad”\n\nNota: se quiere eliminar el intercepto específico de la entidad \\(i\\), que captura los efectos fijos. Se debe entender que el concepto efectos fijos hace referencia a habilidades propias de cada individuo o entidad, los cuales son inobservable por ser propios de cada individuo o entidad.\n\n\n\nCalcular la Media Dentro de la Entidad:\n\n\\(\\bar{y}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}y_{it}\\)\n\\(\\bar{x}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}x_{it}\\)​\n\nademas,\n\n\\(\\bar{\\beta}_{0} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}\\beta_{0} = \\frac{1}{T}T\\beta_{0} = \\beta_{0}\\) es una constante si se tuviera un intercepto en comun.\n\\(\\bar{\\alpha}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}\\alpha_{i} = \\frac{1}{T}T\\alpha_{i} = \\alpha_{i}\\)\n\nDonde \\(T_{i}\\)​ es el número de observaciones de la entidad \\(i\\).\n\n\nRestar las Medias:\nRestamos la media dentro de la entidad de cada observación:\n\\[y_{it} - \\bar{y}_{i} = (\\alpha_{i} + \\beta X_{ it} + \\epsilon_{it}) - ( \\alpha_{i} + \\beta \\bar{X}_{i} + \\bar{\\epsilon}_{i})\\]\n\\[y_{it} - \\bar{y}_{i} = (\\alpha_{i} - \\alpha_{i}) + \\beta (X_{ it} - \\bar{X}_{i}) + (\\epsilon_{it} - \\bar{\\epsilon}_{i})\\]\n\nNota: se elimina \\((\\alpha_{i} - \\alpha_{i})\\) y nos queda un modeloque recibe el nombre de transformación de efectos fijo\n\nEsto simplifica a:\n\\[\\tilde{y}_{it} = \\beta \\tilde{x}_{it} + \\tilde{\\epsilon}_{it}\\]​\nDonde:\n\n\\(\\tilde{y}_{it} = y_{it} - \\bar{y}_{i}\\)\n\\(\\tilde{x}_{it} = x_{it} - \\bar{x}_{i}\\)\n\\(\\tilde{\\epsilon}_{it} = \\epsilon_{it} - \\bar{\\epsilon}_{i}\\)​\n\nEstimación por Mínimos Cuadrados Ordinarios (MCO):\nEl modelo transformado se puede estimar usando Mínimos Cuadrados Ordinarios (MCO):\n\\[\\hat{\\beta} = (\\tilde{X´} \\tilde{X})^{-1}\\tilde{X´}\\tilde{Y}\\]\n​Donde:\n\n\\(\\tilde{X}\\) es la matriz de las variables explicativas después de restar las medias dentro de cada entidad.\n\\(\\tilde{Y}\\)​ es el vector de la variable dependiente después de restar las medias dentro de cada entidad.\n\nNota 1: este modelo explora la variabilidad de \\(x\\) y de \\(y\\) dentro de los individuos a traves del tiempo, donde \\(\\tilde{x}\\) y \\(\\tilde{y}\\)​ son las desviaciones de \\(y_{it}\\) y de \\(x_{it}\\) respecto a su media.\nNota 2: un limitación utilizar estimación por efectos fijos esta en el hecho de no permitir variables constantes a traves del tiempo.\nNota 3: otra limitación es el hecho de obtener estimaciones estadisticamente menos significativas con efectos fijos, ya que la varianza muestral de las variables originales \\(y_{it}, ~ x_{it},...,x_{ik}\\) es mucho mayor que las tranformadas.\nAlternativa: Modelo con Dummies\nOtra manera de especificar el modelo de efectos fijos es incluyendo variables indicadoras (dummies) para cada entidad:\n\\[Y_{it} = \\alpha +\\sum_{i=1}^{N-1} \\delta_{i} D_{i} + \\beta X_{it} + \\epsilon_{it}\\]\nDonde:\n\n\\(D_{i}\\) es una dummy que toma el valor 1 si la observación pertenece a la entidad \\(i\\) y 0 en caso contrario.\n\\(\\delta_{i}\\)​ captura el efecto fijo de la entidad \\(i\\).\n\nSin embargo, este enfoque puede ser computacionalmente intensivo si hay muchas entidades."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#ejemplo-práctico-1",
    "href": "Proyectos/Repaso Tercer Parcial.html#ejemplo-práctico-1",
    "title": "Repaso Tercer Parcial",
    "section": "Ejemplo Práctico:",
    "text": "Ejemplo Práctico:\nSupongamos que tenemos un panel de datos con las siguientes variables:\n\\(y_{it}\\): Ingresos de la persona \\(i\\) en el año \\(t\\).\n\\(X_{it}\\)​: Años de educación de la persona \\(i\\) en el año \\(t\\).\nCalcular la Media de Ingresos y Educación para Cada Persona:\n\\(\\bar{y}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}y_{it}\\)\n\\(\\bar{x}_{i} = \\frac{1}{T_{i}}\\sum_{t=1}^{T_{i}}x_{it}\\)​\nRestar las Medias:\n\\(\\tilde{y}_{it} = y_{it} - \\bar{y}_{i}\\)\n\\(\\tilde{X}_{it} = X_{it} - \\bar{X}_{i}\\)​\nEstimar el Modelo Transformado con MCO:\n\\(\\hat{\\beta} = (\\tilde{X´} \\tilde{X})^{-1}\\tilde{X´}\\tilde{Y}\\)​\nEn resumen, el modelo de efectos fijos en panel de datos controla por la heterogeneidad inobservable específica de cada entidad, permitiendo estimar los efectos de las variables explicativas de manera más precisa. La transformación de “dentro de la entidad” elimina los efectos fijos, facilitando la estimación mediante MCO."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#estructura-del-modelo-de-efectos-aleatorios",
    "href": "Proyectos/Repaso Tercer Parcial.html#estructura-del-modelo-de-efectos-aleatorios",
    "title": "Repaso Tercer Parcial",
    "section": "Estructura del Modelo de Efectos Aleatorios",
    "text": "Estructura del Modelo de Efectos Aleatorios\nEl modelo de efectos aleatorios puede ser formulado matemáticamente de la siguiente manera:\n\\[y_{it} = \\alpha + \\beta x_{it} + u_{it}\\]\nDonde:\n\n\\(y_{it}\\)​ es la variable dependiente para la unidad \\(i\\) en el tiempo \\(t\\).\n\\(\\alpha\\) es el intercepto común a todas las unidades.\n\\(x_{it}\\)​ es un vector de variables explicativas para la unidad \\(i\\) en el tiempo \\(t\\).\n\\(\\beta\\) es un vector de coeficientes que mide el impacto de las variables explicativas en \\(y_{it}\\)​.\n\\(u_{it}\\) es el término de error compuesto.\n\nEl término de error compuesto \\(u_{it}\\) se descompone en dos partes:\n\n\\(u_{it} = \\mu_{i} + \\epsilon_{it}\\) ​\n\nDonde:\n\n\\(\\mu_{i}\\)​ es el efecto aleatorio específico de la unidad \\(i\\), que se asume que sigue una distribución normal con media cero y varianza \\(\\sigma_{\\mu}^{2}\\)​. Es decir, \\(\\mu_{i} \\sim \\mathcal{N}(0, \\sigma_{\\mu}^{2})\\).\n\\(\\epsilon_{it}\\)​ es el término de error idiosincrático, que también se asume que sigue una distribución normal con media cero y varianza \\(\\sigma_{\\epsilon}^{2}\\). Es decir, \\(\\epsilon_{it} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})\\)."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#supuestos-del-modelo-de-efectos-aleatorios",
    "href": "Proyectos/Repaso Tercer Parcial.html#supuestos-del-modelo-de-efectos-aleatorios",
    "title": "Repaso Tercer Parcial",
    "section": "Supuestos del Modelo de Efectos Aleatorios",
    "text": "Supuestos del Modelo de Efectos Aleatorios\n\nNo correlación entre efectos individuales y variables explicativas:\n\\[\\mathbb{E}(\\mu_i | x_{it}) = 0\\]\nEsto implica que los efectos aleatorios \\(\\mu_{i}\\)​ no están correlacionados con las variables explicativas \\(x_{it}\\).\nHomocedasticidad:\nLa varianza de los errores idiosincráticos es constante:\n\\[\\mathbb{E}(\\epsilon_{it}^{2}) = \\sigma_{\\epsilon}^{2}\\]\nNo autocorrelación de los errores idiosincráticos:\n\\[\\mathbb{E}(\\epsilon_{it} \\epsilon_{js}) = 0 \\quad \\text{para} \\quad (i \\neq j) \\, \\text{o} \\, (t \\neq s)\\]\nDistribución Normal de los Efectos Aleatorios:\n\\[\\mu_i \\sim \\mathcal{N}(0, \\sigma_{\\mu}^{2})\\]"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#varianza-compuesta-del-error",
    "href": "Proyectos/Repaso Tercer Parcial.html#varianza-compuesta-del-error",
    "title": "Repaso Tercer Parcial",
    "section": "Varianza Compuesta del Error",
    "text": "Varianza Compuesta del Error\nDado que \\(u_{it} = \\mu_i + \\epsilon_{it}\\), la varianza total del error puede ser expresada como:\n\\[\\sigma_{u}^{2} = \\sigma_{\\mu}^{2} + \\sigma_{\\epsilon}^{2}\\]​"
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#estimación-del-modelo-de-efectos-aleatorios",
    "href": "Proyectos/Repaso Tercer Parcial.html#estimación-del-modelo-de-efectos-aleatorios",
    "title": "Repaso Tercer Parcial",
    "section": "Estimación del Modelo de Efectos Aleatorios",
    "text": "Estimación del Modelo de Efectos Aleatorios\nEl modelo de efectos aleatorios se estima comúnmente utilizando el estimador de mínimos cuadrados generalizados (MCG). El estimador MCG considera la estructura de la varianza-covarianza del error para proporcionar estimaciones eficientes de los coeficientes \\(\\beta\\).\nveamos: ¿podemos calcular los parametros por MCG?\n\nReescribamos el modelo como: \\(y_{it} = \\alpha + \\beta x_{it} + u_{it}\\)\n\nDonde:\n\n\\(y_{it}\\)​ es la variable dependiente para la unidad \\(i\\) en el tiempo \\(t\\).\n\\(\\alpha\\) es el intercepto común a todas las unidades.\n\\(x_{it}\\)​ es un vector de variables explicativas para la unidad \\(i\\) en el tiempo \\(t\\).\n\\(\\beta\\) es un vector de coeficientes que mide el impacto de las variables explicativas en \\(y_{it}\\)​.\n\\(u_{it}\\) es el término de error compuesto. (heterogeneidad no observada + el error idiosincrático)\nNota: \\(\\epsilon_{it}\\) es homocedastico y no esta serialmete correlacionado.\n\n\nNo se conoce el comportamiento de \\(u_{it}\\). Es decir, se tiene que saber si \\(u_{it}\\) esta bien comportada. para esto asumimos que :\n\\(VAR(\\mu_{i}) = \\sigma_{\\mu_{i}}^{2}\\) y \\(VAR(\\epsilon_{i}) = \\sigma_{\\epsilon_{i}}^{2}\\)\n¿cúal es la correlación intertemporal entre \\(u_{it}\\) y \\(u_{is}\\) \\((Corr(u_{it}~ u_{is}))\\)\n\\[\\mathbb{Corr}(u_{it}u_{is})= \\frac{Cov(u_{it} u_{is})}{\\sqrt{VAR(u_{it})VAR(u_{is})}} = \\frac{Cov(\\mu_{i}+\\epsilon_{it}, \\mu_{i}+\\epsilon_{is})}{\\sqrt{[VAR(\\mu_{i})+VAR(\\epsilon_{it})]^{2}}}\\]\nNota: se expresa la \\(\\mathbb{Corr}(u_{it}u_{is})\\) por definición de correlación y se hacemos las sustituciones a partir de las identidades ya halladas.\n\\[\\mathbb{Corr}(u_{it}u_{is})= \\frac{Cov(\\mu_i\\mu_i) + Cov(\\mu_{i} \\epsilon_{is}) + Cov(\\epsilon_{it}\\mu_{i}) + Cov(\\epsilon_{it}\\epsilon_{is})}{\\sigma_{\\mu}^{2} + \\sigma_{\\epsilon}^{2}}\\]\n\nDonde: \\(Cov(\\mu_{i} \\epsilon_{is}) = 0\\), \\(Cov(\\epsilon_{it}\\mu_{i})=0\\), \\(Cov(\\epsilon_{it}\\epsilon_{is}) = 0\\)\nA partir de loa naterior se llega a:\n\\(\\mathbb{Corr}(u_{it}u_{is}) = \\frac{\\sigma_{\\mu}^{2}}{\\sigma_{\\mu}^2 + \\sigma_\\epsilon^2}\\) , es decir \\(u_{it}\\) esta serialmente correlacionado\n\nFiltro de transformación\n\nUna transformación comúnmente usada en el modelo de efectos aleatorios es la deglación de los efectos individuales mediante el factor \\(\\theta\\), donde:\n​​\\[\\theta = 1 - \\sqrt{\\frac{\\sigma_\\epsilon^2}{\\sigma_\\mu^2 + \\sigma_\\epsilon^2}}\\]\nNota: efectos aleatorios corrige esa correlacion serial por medio de \\(\\theta\\) .\nAplicando esta transformación:\n\\[y_{it} - \\theta \\bar{y}_{i} = \\alpha(1 - \\theta) + \\beta (x_{it} - \\theta \\bar{x}_{i}) + (u_{it} - \\theta \\bar{u}_{i})\\]\nDonde \\(\\bar{y}_{i}\\) y \\(\\bar{x}_{i}\\) son las medias de \\(y_{it}\\)​ y \\(x_{it}\\)​ a través del tiempo para la unidad \\(i\\).\nNota 1: la presencia de \\(\\alpha_{i}\\) se suaviza por medio de \\((1-\\theta)\\). a medida que \\(\\theta\\) se va caercando a 1, la expresión \\((1-\\theta)\\) se va haciendo cero, por lo que \\(\\mu_{i}\\) se va a desaparecer.\nNota 2: el parametro \\(\\theta\\) nunca es conocido en la practica porque depende de variables poblacionales, pero es facil de estimar. los software estadisticos implementan esta formula de forma inmediata.\n\\[\\hat{\\theta} = 1 - \\{\\frac{1}{[1 +\\frac{T \\hat{\\sigma}_{\\mu}^{2}}{\\hat{\\sigma}_{\\epsilon}^{2}}]}\\}\\]\nPrueba de Breusch-Pagan Multiplicador de Lagrange (LM) para Efectos Aleatorios en Datos de Panel\nLa Prueba de Breusch-Pagan LM se utiliza para determinar la presencia de efectos aleatorios en datos de panel. Se analiza la varianza de los residuos para detectar efectos individuales específicos no observables.\nLa Prueba de Breusch-Pagan LM se utiliza para detectar efectos aleatorios en datos de panel mediante el análisis de la varianza de los residuos. Esto permite determinar si existen efectos individuales específicos no observables.\nPasos Clave:\n\nModelo de Datos de Panel: Relaciona una variable dependiente con variables explicativas y un término de error descompuesto en efectos individuales y errores idiosincráticos.\nHipótesis de la Prueba:\n\nNula: No hay efectos aleatorios (\\(\\sigma_{\\mu}^2 = 0\\)).\nAlternativa: Hay efectos aleatorios (\\(\\sigma_{\\mu}^2 &gt; 0\\)).\n\nProceso de Prueba:\n\nEstimar el modelo de efectos comunes.\nCalcular las sumas de residuos cuadrados agrupados y dentro de los grupos.\nComparar el estadístico LM con la distribución chi-cuadrado para determinar la presencia de efectos aleatorios.\n\n\nInterpretación: Si el estadístico LM es mayor que el valor crítico de la chi-cuadrado, se rechaza la hipótesis nula, indicando la presencia de efectos aleatorios en los datos de panel.\nInterpretación del Resultado\nEl estadístico LM sigue una distribución chi-cuadrado (\\(\\chi^{2}\\)) con grados de libertad igual al número de parámetros en el modelo (excluyendo la constante). Si el valor del estadístico LM es mayor que el valor crítico de la distribución chi-cuadrado para un nivel de significancia dado (por ejemplo, 0.05), se rechaza la hipótesis nula, indicando la presencia de efectos aleatorios en los datos de panel."
  },
  {
    "objectID": "Proyectos/Repaso Tercer Parcial.html#resumen-4",
    "href": "Proyectos/Repaso Tercer Parcial.html#resumen-4",
    "title": "Repaso Tercer Parcial",
    "section": "Resumen",
    "text": "Resumen\nEl modelo de efectos aleatorios es apropiado cuando se puede asumir que las diferencias no observadas entre las unidades del panel son no correlacionadas con las variables explicativas. Este modelo permite una estimación más eficiente al aprovechar la variabilidad tanto entre unidades como dentro de las unidades en el tiempo.\nNota 1: efctos aleatorios es consistente, pero no insesgado para \\(N \\longrightarrow \\infty\\) y \\(T\\) fijo (datos para muchos individuos en un tiempo muy corto)\nNota 2: en el caso donde \\(T \\longrightarrow \\infty\\) y \\(N\\) fijo no se sabe si efectos aleatorios es consistente. (en este caso es mejor pensar en un metodo de series de tiempo).\nNota 3: volviendo al modelo transformado se tiene:\n\\[y_{it} - \\theta \\bar{y}_{i} = \\alpha(1 - \\theta) + \\beta (x_{it} - \\theta \\bar{x}_{i}) + (u_{it} - \\theta \\bar{u}_{i})\\]\n\nsi \\(\\theta = 0\\) se tiene MCO agrupados (no existe una correlación serial)\nsi \\(0 &lt; \\theta &lt; 1\\) se tiene efectos aleatorios (existe correlación serial)\nsi \\(\\theta =1\\) se tiene efectos fijos. (nos da la transformación de efectos fijos)\n\nNota: sabemos que el \\(\\theta\\) se remplaza por \\(\\hat{\\theta}\\) por la imposibilidad de calcular el primero."
  }
]